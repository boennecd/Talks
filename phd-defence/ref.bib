@article{cox72,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2985181},
 abstract = {The analysis of censored failure times is considered. It is assumed that on each individual are available values of one or more explanatory variables. The hazard function (age-specific failure rate) is taken to be a function of the explanatory variables and unknown regression coefficients multiplied by an arbitrary and unknown function of time. A conditional likelihood is obtained, leading to inferences about the unknown regression coefficients. Some generalizations are outlined.},
 author = {D. R. Cox},
 journal = {Journal of the Royal Statistical Society Series B},
 number = {2},
 pages = {187-220},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regression Models and Life-Tables},
 volume = {34},
 year = {1972}}

@article{Dempster77,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2984875},
 abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
 author = {A. P. Dempster and N. M. Laird and D. B. Rubin},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {1--38},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Maximum Likelihood from Incomplete Data via the EM Algorithm},
 volume = {39},
 year = {1977}
}

@Article{Vaupel79,
author="Vaupel, James W.
and Manton, Kenneth G.
and Stallard, Eric",
title="The impact of heterogeneity in individual frailty on the dynamics of mortality",
journal="Demography",
year="1979",
day="01",
volume="16",
number="3",
pages="439--454",
abstract="Life table methods are developed for populations whose members differ in their endowment for longevity. Unlike standard methods, which ignore such heterogeneity, these methods use different calculations to construct cohort, period, and individual life tables. The results imply that standard methods overestimate current life expectancy and potential gains in life expectancy from health and safety interventions, while underestimating rates of individual aging, past progress in reducing mortality, and mortality differentials between pairs of populations. Calculations based on Swedish mortality data suggest that these errors may be important, especially in old age."
}

@article{Harvey79,
author = {Harvey, A. C. and Phillips, G. D. A.},
title = {Maximum Likelihood Estimation of Regression Models with Autoregressive-Moving Average Disturbances},
journal = {Biometrika},
volume = {66},
number = {1},
pages = {49},
year = {1979},
doi = {10.1093/biomet/66.1.49}
}

@article {Shumway82,
author = {Shumway, R. H. and Stoffer, D. S.},
title = {An Approach to Time Series Smoothing and Forecasting Using the EM Algorithm},
journal = {Journal of Time Series Analysis},
volume = {3},
number = {4},
publisher = {Blackwell Publishing Ltd},
issn = {1467-9892},
doi = {10.1111/j.1467-9892.1982.tb00349.x},
pages = {253--264},
keywords = {Missing data, Kalman filter, EM algorithm, forecasting, maximum likelihood},
year = {1982},
}

@article {aalen1989,
author = {Aalen, Odd O.},
title = {A Linear Regression Model for the Analysis of Life Times},
journal = {Statistics in Medicine},
volume = {8},
number = {8},
publisher = {Wiley Subscription Services, Inc., A Wiley Company},
issn = {1097-0258},
doi = {10.1002/sim.4780080803},
pages = {907--925},
keywords = {Survival analysis, Cox model, Linear models, Non-parametrics, Counting processes, Empirical process, Regression},
year = {1989},
}

@article{Leary90,
author = {Dianne P. O’Leary},
title = {Robust Regression Computation Using Iteratively Reweighted Least Squares},
journal = {SIAM Journal on Matrix Analysis and Applications},
volume = {11},
number = {3},
pages = {466-480},
year = {1990},
doi = {10.1137/0611032},

URL = {
        http://dx.doi.org/10.1137/0611032

}}

@Article{Fahrmeir91,
author="Fahrmeir, L. and Kaufmann, H.",
title="On Kalman Filtering, Posterior Mode Estimation and Fisher Scoring in Dynamic Exponential Family Regression",
journal="Metrika",
year="1991",
volume="38",
number="1",
pages="37--60",
abstract="Dynamic exponential family regression provides a framework for nonlinear regression analysis with time dependent parameters$\beta$                0,$\beta$                1, {\ldots},$\beta$                t, {\ldots}, dim$\beta$                t=p. In addition to the familiar conditionally Gaussian model, it covers e.g. models for categorical or counted responses. Parameters can be estimated by extended Kalman filtering and smoothing. In this paper, further algorithms are presented. They are derived from posterior mode estimation of the whole parameter vector ($\beta${\textasciiacutex}0, {\ldots},$\beta${\textasciiacutex}t) by Gauss-Newton resp. Fisher scoring iterations. Factorizing the information matrix into block-bidiagonal matrices, algorithms can be given in a forward-backward recursive form where only inverses of ``small''p{\texttimes}p-matrices occur. Approximate error covariance matrices are obtained by an inversion formula for the information matrix, which is explicit up top{\texttimes}p-matrices.",
issn="1435-926X",
doi="10.1007/BF02613597",
}

@article{Miller92,
 ISSN = {00359254, 14679876},
 URL = {http://www.jstor.org/stable/2347583},
 author = {Alan J. Miller},
 journal = {Journal of the Royal Statistical Society Series C},
 number = {2},
 pages = {458-478},
 publisher = {[Wiley, Royal Statistical Society]},
 title = {Algorithm AS 274: Least Squares Routines to Supplement Those of Gentleman},
 volume = {41},
 year = {1992}
}

@article{Fahrmeir92,
author = {Fahrmeir, L.},
title = {Posterior Mode Estimation by Extended Kalman Filtering for Multivariate Dynamic Generalized Linear Models},
journal = {Journal of the American Statistical Association},
volume = {87},
number = {418},
pages = {501-509},
year = {1992},
doi = {10.1080/01621459.1992.10475232}
}

@article{Fahrmeir94,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2336962},
 abstract = {This paper describes a dynamic or state-space approach for analyzing discrete time or grouped survival data. Simultaneous estimation of baseline hazard functions and of time-varying covariate effects is based on maximization of posterior densities or, equivalently, a penalized likelihood, leading to Kalman-type smoothing algorithms. Data-driven choice of unknown smoothing parameters is possible via an EM-type procedure. The methods are illustrated by applications to real data.},
 author = {Fahrmeir, L.},
 journal = {Biometrika},
 number = {2},
 pages = {317-330},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Dynamic Modelling and Penalized Likelihood Estimation for Discrete Time Survival Data},
 volume = {81},
 year = {1994}
}

@article{Abrams94,
author = { Abrams ,  Donald I.  and  Goldman ,  Anne I.  and  Launer ,  Cynthia  and  Korvick ,  Joyce A.  and  Neaton ,  James D.  and  Crane ,  Lawrence R.  and  Grodesky ,  Michael  and  Wakefield ,  Steven  and  Muth ,  Katherine  and  Kornegay ,  Sandra  and  Cohn ,  David L.  and  Harris ,  Allen  and  Luskin-Hawk ,  Roberta  and  Markowitz ,  Norman  and  Sampson ,  James H.  and  Thompson ,  Melanie  and  Deyton ,  Lawrence  and the Terry Beirn Community Programs for Clinical Research on AIDS},
title = {A Comparative Trial of Didanosine or Zalcitabine after Treatment with Zidovudine in Patients with Human Immunodeficiency Virus Infection},
journal = {New England Journal of Medicine},
volume = {330},
number = {10},
pages = {657-662},
year = {1994},
doi = {10.1056/NEJM199403103301001},
    note ={PMID: 7906384},

URL = {
        http://dx.doi.org/10.1056/NEJM199403103301001

}}

@article{kupiec95,
  title={Techniques for verifying the accuracy of risk measurement models},
  author={Kupiec, Paul},
  year={1995},
  pages = {73--84},
  journal = {The Journal of Derivatives},
  volume = {3}
}

@article{Fahrmeir96,
author = { Ludwig   Fahrmeir  and  Stefan   Wagenpfeil },
title = {Smoothing Hazard Functions and Time-Varying Effects in Discrete Duration and Competing Risks Models},
journal = {Journal of the American Statistical Association},
volume = {91},
number = {436},
pages = {1584-1594},
year = {1996},
doi = {10.1080/01621459.1996.10476726},
}

@article{Lee96,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346105},
 abstract = {We consider hierarchical generalized linear models which allow extra error components in the linear predictors of generalized linear models. The distribution of these components is not restricted to be normal; this allows a broader class of models, which includes generalized linear mixed models. We use a generalization of Henderson's joint likelihood, called a hierarchical or h-likelihood, for inferences from hierarchical generalized linear models. This avoids the integration that is necessary when marginal likelihood is used. Under appropriate conditions maximizing the h-likelihood gives fixed effect estimators that are asymptotically equivalent to those obtained from the use of marginal likelihood; at the same time we obtain the random effect estimates that are asymptotically best unbiased predictors. An adjusted profile h-likelihood is shown to give the required generalization of restricted maximum likelihood for the estimation of dispersion components. A scaled deviance test for the goodness of fit, a model selection criterion for choosing between various dispersion models and a graphical method for checking the distributional assumption of random effects are proposed. The ideas of quasi-likelihood and extended quasi-likelihood are generalized to the new class. We give examples of the Poisson-gamma, binomial-beta and gamma-inverse gamma hierarchical generalized linear models. A resolution is proposed for the apparent difference between population-averaged and subject-specific models. A unified framework is provided for viewing and extending many existing methods.},
 author = {Y. Lee and J. A. Nelder},
 journal = {Journal of the Royal Statistical Society Series B},
 number = {4},
 pages = {619-678},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Hierarchical Generalized Linear Models},
 volume = {58},
 year = {1996}
}

@article{Pitt99,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2670179},
 abstract = {This article analyses the recently suggested particle approach to filtering time series. We suggest that the algorithm is not robust to outliers for two reasons: the design of the simulators and the use of the discrete support to represent the sequentially updating prior distribution. Here we tackle the first of these problems.},
 author = {Michael K. Pitt and Neil Shephard},
 journal = {Journal of the American Statistical Association},
 number = {446},
 pages = {590--599},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Filtering via Simulation: Auxiliary Particle Filters},
 volume = {94},
 year = {1999}
}

@proceedings{Julier97,
author = {Julierm, Simon J. and Uhlmann, Jeffrey K. },
title = {New extension of the Kalman filter to nonlinear systems},
volume = {3068},
year = {1997},
doi = {10.1117/12.280797},
URL = {https://doi.org/10.1117/12.280797},
booktitle = {Signal Processing, Sensor Fusion, and Target Recognition VI},
address = {Orlando, FL, United States}
}

@BOOK{laug,
      AUTHOR = {Anderson, E. and Bai, Z. and Bischof, C. and
                Blackford, S. and Demmel, J. and Dongarra, J. and
                Du Croz, J. and Greenbaum, A. and Hammarling, S. and
                McKenney, A. and Sorensen, D.},
      TITLE = {LAPACK Users' Guide},
      EDITION = {Third},
      PUBLISHER = {Society for Industrial and Applied Mathematics},
      YEAR = {1999},
      ADDRESS = {Philadelphia, PA},
      ISBN = {0-89871-447-8}}

@book{Bates00,
  title={Mixed-effects models in S and S-PLUS},
  author={Jos{\'e} C. Pinheiro and Douglas M. Bates},
  year={2000},
  publisher={Springer-Verlag New York}
}

@article{Koopman00,
author = {Koopman, S. J. and Durbin, J.},
title = {Fast Filtering and Smoothing for Multivariate State Space Models},
journal = {Journal of Time Series Analysis},
volume = {21},
number = {3},
publisher = {Blackwell Publishers Ltd},
issn = {1467-9892},
doi = {10.1111/1467-9892.00186},
pages = {281--296},
keywords = {Diffuse initialization, Kalman filter, multivariate models, smoothing, state space, time series, vector cubic splines},
year = {2000},
}

@Book{survival-book,
  title = {Modeling Survival Data: Extending the {C}ox Model},
  author = {{Terry M. Therneau} and {Patricia M. Grambsch}},
  year = {2000},
  publisher = {Springer-Verlag},
  address = {New York},
  isbn = {0-387-98784-3},
}

@INPROCEEDINGS{Wan00,
author={E. A. Wan and R. Van Der Merwe},
booktitle={Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (Cat. No.00EX373)},
title={The Unscented Kalman Filter for Nonlinear Estimation},
year={2000},
pages={153-158},
keywords={adaptive Kalman filters;computational complexity;covariance analysis;filtering theory;learning (artificial intelligence);nonlinear estimation;nonlinear systems;parameter estimation;state estimation;Gaussian random variable;computational complexity;covariance;deterministic sampling approach;dual estimation problems;extended Kalman filter;machine learning;neural networks training;nonlinear control;nonlinear estimation;nonlinear system;nonlinear system identification;posterior mean;sample points;state distribution;state-estimation;system dynamics;unscented Kalman filter;Computational complexity;Filters;Machine learning;Neural networks;Nonlinear dynamical systems;Nonlinear systems;Performance gain;Random variables;Sampling methods;Taylor series},
doi={10.1109/ASSPCC.2000.882463},
month={},}

@article{Durbin00,
 ISSN = {13697412, 14679868},
 URL = {http://www.jstor.org/stable/2680676},
 abstract = {The analysis of non-Gaussian time series using state space models is considered from both classical and Bayesian perspectives. The treatment in both cases is based on simulation using importance sampling and antithetic variables; Markov chain Monte Carlo methods are not employed. Non-Gaussian disturbances for the state equation as well as for the observation equation are considered. Methods for estimating conditional and posterior means of functions of the state vector given the observations, and the mean-square errors of their estimates, are developed. These methods are extended to cover the estimation of conditional and posterior densities and distribution functions. Choice of importance sampling densities and antithetic variables is discussed. The techniques work well in practice and are computationally efficient. Their use is illustrated by applying them to a univariate discrete time series, a series with outliers and a volatility series.},
 author = {J. Durbin and S. J. Koopman},
 journal = {Journal of the Royal Statistical Society Series B},
 number = {1},
 pages = {3-56},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Time Series Analysis of Non-Gaussian Observations Based on State Space Models from Both Classical and Bayesian Perspectives},
 volume = {62},
 year = {2000}
}

@Article{Lunn00,
author="Lunn, David J.
and Thomas, Andrew
and Best, Nicky
and Spiegelhalter, David",
title="WinBUGS - A Bayesian modelling framework: Concepts, structure, and extensibility",
journal="Statistics and Computing",
year="2000",
month="Oct",
day="01",
volume="10",
number="4",
pages="325--337",
abstract="WinBUGS is a fully extensible modular framework for constructing and analysing Bayesian full probability models. Models may be specified either textually via the BUGS language or pictorially using a graphical interface called DoodleBUGS. WinBUGS processes the model specification and constructs an object-oriented representation of the model. The software offers a user-interface, based on dialogue boxes and menu commands, through which the model may then be analysed using Markov chain Monte Carlo techniques. In this paper we discuss how and why various modern computing concepts, such as object-orientation and run-time linking, feature in the software's design. We also discuss how the framework may be extended. It is possible to write specific applications that form an apparently seamless interface with WinBUGS for users with specialized requirements. It is also possible to interface with WinBUGS at a lower level by incorporating new object types that may be used by WinBUGS without knowledge of the modules in which they are implemented. Neither of these types of extension require access to, or even recompilation of, the WinBUGS source-code.",
issn="1573-1375",
doi="10.1023/A:1008929526011",
url="https://doi.org/10.1023/A:1008929526011"
}

@article{Shumway01,
 abstract = {I argue that hazard models are more appropriate than single‐period models for forecasting bankruptcy. Single‐period models are inconsistent, while hazard models produce consistent estimates. I describe a simple technique for estimating a discrete‐time hazard model. I find that about half of the accounting ratios that have been used in previous models are not statistically significant. Moreover, market size, past stock returns, and idiosyncratic returns variability are all strongly related to bankruptcy. I propose a model that uses both accounting ratios and market‐driven variables to produce out‐of‐sample forecasts that are more accurate than those of alternative models.},
 author = {Tyler Shumway},
 journal = {The Journal of Business},
 number = {1},
 pages = {101--124},
 publisher = {The University of Chicago Press},
 title = {Forecasting Bankruptcy More Accurately: A Simple Hazard Model},
 volume = {74},
 year = {2001}
}

@article{Friedman01,
 abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
 author = {Jerome H. Friedman},
 journal = {The Annals of Statistics},
 number = {5},
 pages = {1189--1232},
 publisher = {Institute of Mathematical Statistics},
 title = {Greedy Function Approximation: A Gradient Boosting Machine},
 volume = {29},
 year = {2001}
}

@inproceedings{jags,
  title = {JAGS: A program for analysis of Bayesian graphical models using Gibbs sampling},
  author= {Plummer, Martyn},
  year = {2003},
  venue = {Vienna, Austria},
  issn = {1609-395X},
  volume = 124,
  booktitle = {Proceedings of the 3rd international workshop on distributed statistical computing},
}

@article{So03,
 ISSN = {10170405, 19968507},
 URL = {http://www.jstor.org/stable/24307107},
 abstract = {In this paper, we develop a posterior mode estimation method for nonlinear and non-Gaussian state space models. By exploiting special structures of the state space models, we derive a modified quadratic hill-climbing procedure which can be implemented efficiently in O(n) operations. The method can be used for estimating the state variable, performing Bayesian inference and carrying out Monte Carlo likelihood inference. Numerical illustrations using simulated and real data demonstrate that our procedure is much more efficient than a common gradient method. It is also evident that our method works very well in a new stochastic volatility model which contains a nonlinear state equation.},
 author = {Mike K. P. So},
 journal = {Statistica Sinica},
 number = {1},
 pages = {255-274},
 publisher = {Institute of Statistical Science, Academia Sinica},
 title = {Posterior Mode Estimation for Nonlinear and Non-Gaussian State Space Models},
 volume = {13},
 year = {2003}
}

@ARTICLE{Klutke03,
author={G. A. Klutke and P. C. Kiessler and M. A. Wortman},
journal={IEEE Transactions on Reliability},
title={A Critical Look at the Bathtub Curve},
year={2003},
volume={52},
number={1},
pages={125-129},
keywords={environmental stress screening;quality control;reliability theory;bathtub curve;burn-in;distributions mixture;environmental-stress-screening;hazard function;infant mortality;quality-control;reliability theory;Distribution functions;Electronic switching systems;Extraterrestrial measurements;Hazards;Industrial engineering;Manufactured products;Probability density function;Reliability theory;Stress;Time measurement},
doi={10.1109/TR.2002.804492},
ISSN={0018-9529},
month={March},}

@ARTICLE{Julier04,
author={S. J. Julier and J. K. Uhlmann},
journal={Proceedings of the IEEE},
title={Unscented Filtering and Nonlinear Estimation},
year={2004},
volume={92},
number={3},
pages={401-422},
keywords={Kalman filters;covariance analysis;filtering theory;nonlinear estimation;nonlinear filters;nonlinear systems;EKF;extended Kalman filter;nonlinear estimation;nonlinear systems;nonlinear transformations;unscented filtering;unscented transformation;Chemical processes;Control systems;Filtering;Kalman filters;Navigation;Nonlinear control systems;Nonlinear systems;Particle tracking;Target tracking;Vehicles},
doi={10.1109/JPROC.2003.823141},
ISSN={0018-9219},
month={Mar},}

@INPROCEEDINGS{Cappe05, 
author={O. {Cappe} and E. {Moulines}}, 
booktitle={IEEE/SP 13th Workshop on Statistical Signal Processing, 2005}, 
title={Recursive computation of the score and observed information matrix in hidden markov models}, 
year={2005}, 
volume={}, 
number={}, 
pages={703-708}, 
keywords={approximation theory;hidden Markov models;matrix algebra;Monte Carlo methods;recursive functions;signal processing;smoothing methods;hidden Markov model;HMM;statistical model;Fisher identity;Louis identity;recursive equation;information matrix;sensitivity equation;recursive smoother;approximation;sequential Monte Carlo method;Hidden Markov models;Equations;Signal processing algorithms;State-space methods;Bioinformatics;Econometrics;Solid modeling;Smoothing methods;Digital communication;Speech recognition}, 
doi={10.1109/SSP.2005.1628685}, 
ISSN={2373-0803}, 
month={July},}

@article{Vassalou04,
author = {Vassalou, Maria and Xing, Yuhang},
title = {Default Risk in Equity Returns},
journal = {The Journal of Finance},
volume = {59},
number = {2},
pages = {831-868},
doi = {10.1111/j.1540-6261.2004.00650.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.2004.00650.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1540-6261.2004.00650.x},
abstract = {ABSTRACT This is the first study that uses Merton's (1974) option pricing model to compute default measures for individual firms and assess the effect of default risk on equity returns. The size effect is a default effect, and this is also largely true for the book-to-market (BM) effect. Both exist only in segments of the market with high default risk. Default risk is systematic risk. The Fama–French (FF) factors SMB and HML contain some default-related information, but this is not the main reason that the FF model can explain the cross section of equity returns.},
year = {2004}
}

@article {Hamilton04,
	author = {Hamilton, David T. and Cantor, Richard},
	title = {Rating Transition and Default Rates Conditioned on Outlooks},
	volume = {14},
	number = {2},
	pages = {54--70},
	year = {2004},
	doi = {10.3905/jfi.2004.439837},
	publisher = {Institutional Investor Journals Umbrella},
	abstract = {Recently downgraded corporate bond issuers have different transition and default risks from recently upgraded issuers with the same ratings. Rating transition and default rates are found to be sensitive to both rating history and outlook and rate review status, when viewed in isolation. Controlling for outlook status, the effects of rating history are diminished and the power of ratings as predictors of default grows substantially. Adjusting credits on review by two notches (up or down) and adjusting ratings for outlook status by one notch (up or down) raises the Moody{\textquoteright}s three-year horizon accuracy ratio (AR) from 0.65 to 0.71. Adjusting for rating history adds only an additional 0.002 to the AR score. The adjusted ratings are more accurate than the bond implied ratings at a three-year horizon.},
	issn = {1059-8596},
	URL = {https://jfi.iijournals.com/content/14/2/54},
	eprint = {https://jfi.iijournals.com/content/14/2/54.full.pdf},
	journal = {The Journal of Fixed Income}
}

@article{Chava04,
author = {Chava, Sudheer and Jarrow, Robert A.},
title = { Bankruptcy Prediction with Industry Effects },
journal = {Review of Finance},
volume = {8},
number = {4},
pages = {537--569},
year = {2004}
}

@article{Bates04,
title = "Linear mixed models and penalized least squares",
journal = "Journal of Multivariate Analysis",
volume = "91",
number = "1",
pages = "1--17",
year = "2004",
author = "Douglas M. Bates and Saikat DebRoy",
keywords = "REML, Gradient, Hessian, EM algorithm, ECME algorithm, Maximum likelihood, Profile likelihood, Multilevel models"
}

@Article{tpr1,
  title = {Temporal Process Regression},
  author = {Jason P. Fine and Jun Yan and Michael R. Kosorok},
  year = {2004},
  journal = {Biometrika},
  volume = {91},
  pages = {683--703},
}

@Article{tpr2,
  title = {Estimating Equations for Association Structures},
  author = {Jun Yan and Jason P. Fine},
  year = {2004},
  journal = {Statistics in Medicine},
  volume = {23},
  pages = {859--880},
}

@techreport{seeger04,
  title={Low Rank Updates for the Cholesky Decomposition},
  author={Seeger, Matthias},
  year={2004}
}

@article{Lin05,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/27590681},
 abstract = {Sequential Monte Carlo methods, especially the particle filter (PF) and its various modifications, have been used effectively in dealing with stochastic dynamic systems. The standard PF samples the current state through the underlying state dynamics, then uses the current observation to evaluate the sample's importance weight. However, there is a set of problems in which the current observation provides significant information about the current state but the state dynamics are weak, and thus sampling using the current observation often produces more efficient samples than sampling using the state dynamics. In this article we propose a new variant of the PF, the independent particle filter (IPF), to deal with these problems. The IPF generates exchangeable samples of the current state from a sampling distribution that is conditionally independent of the previous states, a special case of which uses only the current observation. Each sample can then be matched with multiple samples of the previous states in evaluating the importance weight. We present some theoretical results showing that this strategy improves efficiency of estimation as well as reduces resampling frequency. We also discuss some extensions of the IPF, and use several synthetic examples to demonstrate the effectiveness of the method.},
 author = {Ming T. Lin and Junni L. Zhang and Qiansheng Cheng and Rong Chen},
 journal = {Journal of the American Statistical Association},
 number = {472},
 pages = {1412--1421},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Independent Particle Filters},
 volume = {100},
 year = {2005}
}

@article{Beaver05,
author = {Beaver, W.H. and McNichols, M.F. and Rhie, JW.},
title = { Have Financial Statements Become Less Informative? {E}vidence from the Ability of Financial Ratios to Predict Bankruptcy },
journal = {Review of Accounting Studies},
volume = {10},
number = {1},
pages = {93--122},
year = {2005}
}

@article{Min05,
title = "Bankruptcy prediction using support vector machine with optimal choice of kernel function parameters",
journal = "Expert Systems with Applications",
volume = "28",
number = "4",
pages = "603--614",
year = "2005",
author = "Jae H. Min and Young-Chan Lee",
keywords = "Bankruptcy prediction, Support vector machine, Grid-search, Kernel function, Back-propagation neural networks"
}

@inproceedings{Caruana06,
 author = {Caruana, Rich and Niculescu-Mizil, Alexandru},
 title = {An Empirical Comparison of Supervised Learning Algorithms},
 booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
 year = {2006},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {161--168},
 numpages = {8},
 address = {New York, NY, USA},
} 

@article{Xiong06,
title = "Performance Evaluation of UKF-Based Nonlinear Filtering",
journal = "Automatica ",
volume = "42",
number = "2",
pages = "261 - 270",
year = "2006",
note = "",
issn = "0005-1098",
doi = "http://dx.doi.org/10.1016/j.automatica.2005.10.004",
author = "K. Xiong and H.Y. Zhang and C.W. Chan",
keywords = "Nonlinear systems",
keywords = "Stochastic systems",
keywords = "Extended Kalman filter",
keywords = "Unscented Kalman filter",
keywords = "Stability ",
abstract = "The performance of the modified unscented Kalman filter (UKF) for nonlinear stochastic discrete-time system with linear measurement equation is investigated. It is proved that under certain conditions, the estimation error of the \{UKF\} remains bounded. Furthermore, it is shown that the design of noise covariance matrix plays an important role in improving the stability of the algorithm. Error behavior of the \{UKF\} is then derived in terms of mean square error (MSE), and the Cramér–Rao lower bound (CRLB) is introduced as a performance measure. The modified \{UKF\} is found to approach the \{CRLB\} if the difference between the real noise covariance matrix and the selected one is small enough. These results are verified by using Monte Carlo simulations on two example systems. "
}

@book{martinussen06,
  title={Dynamic Regression Models for Survival Data},
  author={Martinussen, Torben and Scheike, Thomas H},
  year={2006},
  publisher={Springer-Verlag, NY}
}

@article {Berg07,
author = {Berg, Daniel},
title = {Bankruptcy prediction by generalized additive models},
journal = {Applied Stochastic Models in Business and Industry},
volume = {23},
number = {2},
publisher = {John Wiley & Sons, Ltd.},
pages = {129--143},
keywords = {bankruptcy prediction, generalized additive models, default horizon, performance depreciation, multi-year model},
year = {2007},
}

@inproceedings{pinheiro07,
 author = {Pinheiro, Eduardo and Weber, Wolf-Dietrich and Barroso, Luiz Andr{\'e}},
 title = {Failure Trends in a Large Disk Drive Population},
 booktitle = {Proceedings of the 5th USENIX Conference on File and Storage Technologies},
 series = {FAST '07},
 year = {2007},
 location = {San Jose, CA},
 url = {http://dl.acm.org/citation.cfm?id=1267903.1267905},
 acmid = {1267905},
 publisher = {USENIX Association},
 address = {Berkeley, CA, USA},
}

@article{Duffie07,
title = "Multi-period corporate default prediction with stochastic covariates",
journal = "Journal of Financial Economics",
volume = "83",
number = "3",
pages = "635 - 665",
year = "2007",
issn = "0304-405X",
doi = "https://doi.org/10.1016/j.jfineco.2005.10.011",
url = "http://www.sciencedirect.com/science/article/pii/S0304405X06002029",
author = "Darrell Duffie and Leandro Saita and Ke Wang",
keywords = "Default, Bankruptcy, Duration analysis, Doubly stochastic, Distance to default",
abstract = "We provide maximum likelihood estimators of term structures of conditional probabilities of corporate default, incorporating the dynamics of firm-specific and macroeconomic covariates. For US Industrial firms, based on over 390,000 firm-months of data spanning 1980 to 2004, the term structure of conditional future default probabilities depends on a firm's distance to default (a volatility-adjusted measure of leverage), on the firm's trailing stock return, on trailing S&P 500 returns, and on US interest rates. The out-of-sample predictive performance of the model is an improvement over that of other available models."
}

@article{Buhlmann07,
 abstract = {We present a statistical perspective on boosting. Special emphasis is given to estimating potentially complex parametric or nonparametric models, including generalized linear and additive models as well as regression models for survival analysis. Concepts of degrees of freedom and corresponding Akaike or Bayesian information criteria, particularly useful for regularization and variable selection in high-dimensional covariate spaces, are discussed as well. The practical aspects of boosting procedures for fitting statistical models are illustrated by means of the dedicated open-source software package mboost. This package implements functions which can be used for model fitting, prediction and variable selection. It is flexible, allowing for the implementation of new boosting algorithms optimizing user-specified loss functions.},
 author = {Peter Bühlmann and Torsten Hothorn},
 journal = {Statistical Science},
 number = {4},
 pages = {477--505},
 publisher = {Institute of Mathematical Statistics},
 title = {Boosting Algorithms: Regularization, Prediction and Model Fitting},
 volume = {22},
 year = {2007}
}

@article{Schnatter07,
title = "Auxiliary Mixture Sampling with Applications to Logistic Models",
journal = "Computational Statistics \& Data Analysis ",
volume = "51",
number = "7",
pages = "3509 - 3528",
year = "2007",
note = "",
issn = "0167-9473",
doi = "http://dx.doi.org/10.1016/j.csda.2006.10.006",
url = "http://www.sciencedirect.com/science/article/pii/S0167947306003720",
author = "Sylvia Frühwirth-Schnatter and Rudolf Frühwirth",
keywords = "Binary data",
keywords = "Categorical data",
keywords = "Markov chain Monte Carlo",
keywords = "Random-effects models",
keywords = "State space models",
keywords = "Utilities ",
abstract = "A new method of data augmentation for binary and multinomial logit models is described. First, the latent utilities are introduced as auxiliary latent variables, leading to a latent model which is linear in the unknown parameters, but involves errors from the type I extreme value distribution. Second, for each error term the density of this distribution is approximated by a mixture of normal distributions, and the component indicators in these mixtures are introduced as further latent variables. This leads to Markov chain Monte Carlo estimation based on a convenient auxiliary mixture sampler that draws from standard distributions like normal or exponential distributions and, in contrast to more common Metropolis–Hastings approaches, does not require any tuning. It is shown how the auxiliary mixture sampler is implemented for binary or multinomial logit models, and it is demonstrated how to extend the sampler to mixed effect models and time-varying parameter models for binary and categorical data. Finally, an application to Austrian labor market data is discussed. "
}

@article{Das07,
author = {Sanjiv R. Das and Darrell Duffie and Nikunj Kapadia and Leandro Saita},
title = {Common Failings: How Corporate Defaults Are Correlated},
journal = {The Journal of Finance},
volume = {62},
number = {1},
pages = {93--117},
year = {2007},
abstract = {ABSTRACT We test the doubly stochastic assumption under which firms' default times are correlated only as implied by the correlation of factors determining their default intensities. Using data on U.S. corporations from 1979 to 2004, this assumption is violated in the presence of contagion or “frailty” (unobservable explanatory variables that are correlated across firms). Our tests do not depend on the time‐series properties of default intensities. The data do not support the joint hypothesis of well‐specified default intensities and the doubly stochastic assumption. We find some evidence of default clustering exceeding that implied by the doubly stochastic model with the given intensities.}
}

@article{Bharath08,
    author = {Bharath, Sreedhar T. and Shumway, Tyler},
    title = "{Forecasting Default with the Merton Distance to Default Model}",
    journal = {The Review of Financial Studies},
    volume = {21},
    number = {3},
    pages = {1339-1369},
    year = {2008},
    month = {05},
    abstract = "{We examine the accuracy and contribution of the Merton distance to default (DD) model, which is based on Merton's (1974) bond pricing model. We compare the model to a “naïve” alternative, which uses the functional form suggested by the Merton model but does not solve the model for an implied probability of default. We find that the naïve predictor performs slightly better in hazard models and in out-of-sample forecasts than both the Merton DD model and a reduced-form model that uses the same inputs. Several other forecasting variables are also important predictors, and fitted values from an expanded hazard model outperform Merton DD default probabilities out of sample. Implied default probabilities from credit default swaps and corporate bond yield spreads are only weakly correlated with Merton DD probabilities after adjusting for agency ratings and bond characteristics. We conclude that while the Merton DD model does not produce a sufficient statistic for the probability of default, its functional form is useful for forecasting defaults.}",
    issn = {0893-9454},
    doi = {10.1093/rfs/hhn044},
    url = {https://dx.doi.org/10.1093/rfs/hhn044},
    eprint = {http://oup.prod.sis.lan/rfs/article-pdf/21/3/1339/24429422/hhn044.pdf},
}

@article {Campbell08,
author = {Campbell, John Y. And Hilscher, Jens and Szilagyi, Jan},
title = {In Search of Distress Risk},
journal = {The Journal of Finance},
volume = {63},
number = {6},
publisher = {Blackwell Publishing Inc},
pages = {2899--2939},
year = {2008}
}

@article{Alfaro08,
title = "Bankruptcy forecasting: An empirical comparison of AdaBoost and neural networks",
journal = "Decision Support Systems",
volume = "45",
number = "1",
pages = "110--122",
year = "2008",
author = "Alfaro, Esteban and García, Noelia and Gámez, Matías  and Elizondo, David",
keywords = "Corporate Failure Prediction, Neural Network, AdaBoost"
}

@Book{ssfpack,
  author={Siem Jan Koopman and Neil Shephard and Jurgen A. Doornik},
  title={Statistical Algorithms for Models in State Space Form: SsfPack 3.0},  
  year=2008, 
  publisher={Timberlake Consultants Press}  
}

@article {Duffie09,
author = {Duffie, Darrell and Eckner, Andreas and Horel, Guillaume and Saita, Leandro},
title = {Frailty Correlated Default},
journal = {The Journal of Finance},
volume = {64},
number = {5},
publisher = {Blackwell Publishing Inc},
pages = {2089--2123},
year = {2009},
}

@article{lambert09,
	author = "Lambert, P. C. and Royston, P.",
	title = "Further Development of Flexible Parametric Models for Survival Analysis",
	journal = "Stata Journal",
	publisher = "Stata Press",
	address = "College Station, TX",
	volume = "9",
	number = "2",
	year = "2009",
	pages = "265-290(26)",
	url = "http://www.stata-journal.com/article.html?article=st0165"
}

@INPROCEEDINGS{Angelosante09,
author={D. Angelosante and S. I. Roumeliotis and G. B. Giannakis},
booktitle={2009 Conference Record of the Forty-third Asilomar Conference on Signals, Systems and Computers},
title={Lasso-Kalman Smoother for Tracking Sparse Signals},
year={2009},
pages={181-185},
keywords={Gaussian processes;Kalman filters;Markov processes;regression analysis;smoothing methods;tracking;vectors;Lasso-Kalman smoother;alternating direction method;estimation approach;fixed-interval smoothing;least-absolute shrinkage operator;linear Gauss-Markov model;linear regression problem;multipliers;recursive implementation;selection operator;sparse signal tracking;sparse time-varying vector process tracking;sparsity-agnostic KS scheme;sparsity-aware KS scheme;weighted (W)-LKS;Costs;Gaussian processes;Kalman filters;Linear regression;Magnetic resonance imaging;Recursive estimation;Signal processing;Smoothing methods;Target tracking;Vectors},
doi={10.1109/ACSSC.2009.5470133},
ISSN={1058-6393},
month={Nov},}

@article{Adam09,
   author = {Adam Johansen},
   title = {SMCTC: Sequential Monte Carlo in C++},
   journal = {Journal of Statistical Software, Articles},
   volume = {30},
   number = {6},
   year = {2009},
   keywords = {},
   abstract = {Sequential Monte Carlo methods are a very general class of Monte Carlo methods for sampling from sequences of distributions. Simple examples of these algorithms are used very widely in the tracking and signal processing literature. Recent developments illustrate that these techniques have much more general applicability, and can be applied very effectively to statistical inference problems. Unfortunately, these methods are often perceived as being computationally expensive and difficult to implement. This article seeks to address both of these problems. A C++ template class library for the efficient and convenient implementation of very general Sequential Monte Carlo algorithms is presented. Two example applications are provided: a simple particle filter for illustrative purposes and a state-of-the-art algorithm for rare event estimation.},
   issn = {1548-7660},
   pages = {1--41},
   doi = {10.18637/jss.v030.i06}
}

@Article{Briers2009,
author="Briers, Mark
and Doucet, Arnaud
and Maskell, Simon",
title="Smoothing algorithms for state--space models",
journal="Annals of the Institute of Statistical Mathematics",
year="2009",
month="Jun",
day="09",
volume="62",
number="1",
pages="61",
abstract="Two-filter smoothing is a principled approach for performing optimal smoothing in non-linear non-Gaussian state--space models where the smoothing distributions are computed through the combination of `forward' and `backward' time filters. The `forward' filter is the standard Bayesian filter but the `backward' filter, generally referred to as the backward information filter, is not a probability measure on the space of the hidden Markov process. In cases where the backward information filter can be computed in closed form, this technical point is not important. However, for general state--space models where there is no closed form expression, this prohibits the use of flexible numerical techniques such as Sequential Monte Carlo (SMC) to approximate the two-filter smoothing formula. We propose here a generalised two-filter smoothing formula which only requires approximating probability distributions and applies to any state--space model, removing the need to make restrictive assumptions used in previous approaches to this problem. SMC algorithms are developed to implement this generalised recursion and we illustrate their performance on various problems.",
issn="1572-9052",
doi="10.1007/s10463-009-0236-2",
url="https://doi.org/10.1007/s10463-009-0236-2"
}

@unpublished{Zhou10,
title = {Thresholded Lasso for High Dimensional Variable Selection and Statistical Estimation},
author = {Zhou, Shuheng},
year = {2010},
journal={arXiv preprint arXiv:1002.1583},
note    = {Working paper}
}

@article{friedman10,
   author = {Jerome Friedman and Trevor Hastie and Rob Tibshirani},
   title = {Regularization Paths for Generalized Linear Models via Coordinate Descent},
   journal = {Journal of Statistical Software},
   volume = {33},
   number = {1},
   year = {2010},
   keywords = {},
   abstract = {We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multi- nomial regression problems while the penalties include ℓ1 (the lasso), ℓ2 (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.},
   issn = {1548-7660},
   pages = {1--22},
   doi = {10.18637/jss.v033.i01}
}

@article{Fearnhead10,
    author = {Fearnhead, Paul and Wyncoll, David and Tawn, Jonathan},
    title = "{A sequential smoothing algorithm with linear computational cost}",
    journal = {Biometrika},
    volume = {97},
    number = {2},
    pages = {447-464},
    year = {2010},
    month = {06},
    abstract = "{In this paper we propose a new particle smoother that has a computational complexity of O(N), where N is the number of particles. This compares favourably with the O(N2) computational cost of most smoothers. The new method also overcomes some degeneracy problems in existing algorithms. Through simulation studies we show that substantial gains in efficiency are obtained for practical amounts of computational cost. It is shown both through these simulation studies, and by the analysis of an athletics dataset, that our new method also substantially outperforms the simple filter-smoother, the only other smoother with computational cost that is O(N).}",
    issn = {0006-3444},
    doi = {10.1093/biomet/asq013},
    url = {https://dx.doi.org/10.1093/biomet/asq013},
    eprint = {http://oup.prod.sis.lan/biomet/article-pdf/97/2/447/584254/asq013.pdf},
}

@article{Lando10,
title = "Correlation in corporate defaults: Contagion or conditional independence?",
journal = "Journal of Financial Intermediation",
volume = "19",
number = "3",
pages = "355 - 372",
year = "2010",
note = "Risk Transfer Mechanisms and Financial Stability",
issn = "1042-9573",
doi = "https://doi.org/10.1016/j.jfi.2010.03.002",
url = "http://www.sciencedirect.com/science/article/pii/S1042957310000070",
author = "David Lando and Mads Stenbo Nielsen",
keywords = "Default correlation, Intensity estimation, Hawkes process",
abstract = "We revisit a method used by Das et al. (2007) (DDKS) who jointly test and reject a specification of firm default intensities and the doubly stochastic assumption in intensity models of default. The method relies on a time change result for counting processes. With an almost identical set of default histories recorded by Moody’s in the period from 1982 to 2006, but using a different specification of the default intensity, we cannot reject the tests based on time change used in DDKS. We then note that the method proposed by DDKS is mainly a misspecification test in that it has very limited power in detecting violations of the doubly stochastic assumption. For example, it will not detect contagion which spreads through the explanatory variables “covariates” that determine the default intensities of individual firms. Therefore, we perform a different test using a Hawkes process alternative to see if firm-specific variables are affected by occurrences of defaults, but find no evidence of default contagion."
}

@article{Kim10,
  title = {Ensemble with neural networks for bankruptcy prediction},
  journal = {Expert Systems with Applications},
  year = {2010},
  volume = {37},
  number = {4},
  pages = {3373--3379},
  author = {Kim, Myoung-Jong and Kang, Dae-Ki}
}

@article{Goeman10,
  author = {Goeman, Jelle J.},
  title = {L1 Penalized Estimation in the Cox Proportional Hazards Model},
  journal = {Biometrical Journal},
  volume = {52},
  number = {1},
  publisher = {WILEY-VCH Verlag},
  issn = {1521-4036},
  doi = {10.1002/bimj.200900028},
  pages = {70--84},
  keywords = {Gradient ascent, Lasso, Penalty, Survival},
  year = {2010},
}

@Article{scheike11,
    title = {Analyzing Competing Risk Data Using the R timereg
      Package},
    author = {Thomas H. Scheike and Mei-Jie Zhang},
    journal = {Journal of Statistical Software},
    year = {2011},
    volume = {38},
    number = {2},
    pages = {1--15},
    url = {http://www.jstatsoft.org/v38/i02/},
  }

@manual{
  Hartikainen11,
  author = {Jouni Hartikainen and Arno Solin and Simo Särkkä},
  title = {Optimal Filtering with Kalman Filters and Smoothers. a Manual for the MATLAB Toolbox EKF/UKF},
  year = {2011}, 
  url = {http://becs.aalto.fi/en/research/bayes/ekfukf/},
}

@article{Ying11,
   author = {Jyh-Ying Peng and John Aston},
   title = {The State Space Models Toolbox for MATLAB},
   journal = {Journal of Statistical Software, Articles},
   volume = {41},
   number = {6},
   year = {2011},
   keywords = {},
   abstract = {State Space Models (SSM) is a MATLAB toolbox for time series analysis by state space methods. The software features fully interactive construction and combination of models, with support for univariate and multivariate models, complex time-varying (dy- namic) models, non-Gaussian models, and various standard models such as ARIMA and structural time-series models. The software includes standard functions for Kalman fil- tering and smoothing, simulation smoothing, likelihood evaluation, parameter estimation, signal extraction and forecasting, with incorporation of exact initialization for filters and smoothers, and support for missing observations and multiple time series input with com- mon analysis structure. The software also includes implementations of TRAMO model selection and Hillmer-Tiao decomposition for ARIMA models. The software will provide a general toolbox for time series analysis on the MATLAB platform, allowing users to take advantage of its readily available graph plotting and general matrix computation capabilities.},
   issn = {1548-7660},
   pages = {1--26},
   doi = {10.18637/jss.v041.i06}
}

@article{Koopman11,
title = "Modeling frailty-correlated defaults using many macroeconomic covariates",
journal = "Journal of Econometrics",
volume = "162",
number = "2",
pages = "312--325",
year = "2011",
author = "Siem Jan Koopman and André Lucas and Bernd Schwaab",
keywords = "Systematic default risk, Frailty-correlated defaults, State space methods, Credit risk management"
}

@misc{LAPACKThread,
author = {Nicolas Cellier},
title = {Thread Titled: Cholseski Update of Rank One on LAPACK-Forum},
note = {Retrieved April 2017, http://icl.cs.utk.edu/lapack-forum/viewtopic.php?f=2&t=2646}, 
}

@article{Poyiadjis11,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/29777165},
 abstract = {Particle methods are popular computational tools for Bayesian inference in nonlinear non-Gaussian state space models. For this class of models, we present two particle algorithms to compute the score vector and observed information matrix recursively. The first algorithm is implemented with computational complexity 𝒪(N) and the second with complexity 𝒪(N²), where N is the number of particles. Although cheaper, the performance of the 𝒪(N) method degrades quickly, as it relies on the approximation of a sequence of probability distributions whose dimension increases linearly with time. In particular, even under strong mixing assumptions, the variance of the estimates computed with the 𝒪(N) method increases at least quadratically in time. The more expensive 𝒪(N²) method relies on a nonstandard particle implementation and does not suffer from this rapid degradation. It is shown how both methods can be used to perform batch and recursive parameter estimation.},
 author = {George Poyiadjis and Arnaud Doucet and Sumeetpal S. Singh},
 journal = {Biometrika},
 number = {1},
 pages = {65--80},
 publisher = {Biometrika Trust},
 title = {Particle approximations of the score and observed information matrix in state space models with application to parameter estimation},
 volume = {98},
 year = {2011}
}

@article{Chava11,
author = {Chava, Sudheer and Stefanescu, Catalina and Turnbull, Stuart},
title = {Modeling the Loss Distribution},
journal = {Management Science},
volume = {57},
number = {7},
pages = {1267-1287},
year = {2011},
doi = {10.1287/mnsc.1110.1345},

URL = { 
        https://doi.org/10.1287/mnsc.1110.1345
    
},
eprint = { 
        https://doi.org/10.1287/mnsc.1110.1345
    
}
,
    abstract = { In this paper, we focus on modeling and predicting the loss distribution for credit risky assets such as bonds and loans. We model the probability of default and the recovery rate given default based on shared covariates. We develop a new class of default models that explicitly accounts for sector specific and regime dependent unobservable heterogeneity in firm characteristics. Based on the analysis of a large default and recovery data set over the horizon 1980–2008, we document that the specification of the default model has a major impact on the predicted loss distribution, whereas the specification of the recovery model is less important. In particular, we find evidence that industry factors and regime dynamics affect the performance of default models, implying that the appropriate choice of default models for loss prediction will depend on the credit cycle and on portfolio characteristics. Finally, we show that default probabilities and recovery rates predicted out of sample are negatively correlated and that the magnitude of the correlation varies with seniority class, industry, and credit cycle. This paper was accepted by Wei Xiong, finance. }
}

@book{Buhlmann11,
  title={Statistics for high-dimensional data: methods, theory and applications},
  author={B{\"u}hlmann, Peter and Van De Geer, Sara},
  year={2011},
  publisher={Springer Science \& Business Media}
}

@article{Sun11,
title = "AdaBoost ensemble for financial distress prediction: An empirical comparison with data from Chinese listed companies",
journal = "Expert Systems with Applications",
volume = "38",
number = "8",
pages = "9305--9312",
year = "2011",
author = "Jie Sun and Jia, Ming {Y}ue and Li, Hui",
keywords = "Financial distress prediction, AdaBoost ensemble, Single attribute test, Decision tree, Support vector machine"
}

@article{Giesecke11,
 abstract = {This paper develops dynamic measures of the systemic risk of the financial sector as a whole. It defines systemic risk as the conditional probability of failure of a sufficiently large fraction of the total population of financial institutions. This definition recognizes that the cause of systemic distress is the correlated failure of institutions to meet obligations to creditors, customers, and trading partners. The likelihood estimators of the failure probability are based on a dynamic hazard model of correlated failure timing that captures the influence on failure timing of time-varying macroeconomic and sector-specific risk factors, and of spillover effects. Tests indicate that our measures provide accurate out-of-sample forecasts of the term structure of systemic risk in the United States for the period from 1998 to 2009.},
 author = {Kay Giesecke and Baeho Kim},
 journal = {Management Science},
 number = {8},
 pages = {1387--1405},
 publisher = {INFORMS},
 title = {Systemic Risk: What Defaults Are Telling Us},
 volume = {57},
 year = {2011}
}

@INPROCEEDINGS{Charles11,
author={A. Charles and M. S. Asif and J. Romberg and C. Rozell},
booktitle={2011 45th Annual Conference on Information Sciences and Systems},
title={Sparsity Penalties in Dynamical System Estimation},
year={2011},
pages={1-6},
keywords={Kalman filters;approximation theory;optimisation;recursive estimation;signal reconstruction;state estimation;Kalman filter;coefficient estimation;compressive sensing;dynamical system estimation;one-step update optimization procedure;sparse approximation;sparsity penalty;state estimation;write recursive optimization programs;Estimation;Kalman filters;Noise;Noise measurement;Optimization;Steady-state;Technological innovation;Compressive Sensing;Dynamical Systems;State Estimation},
doi={10.1109/CISS.2011.5766179},
month={March},}

@techreport{Moodys2011,
title = "Corporate Default and Recovery Rates, 1920-2010",
institution  = "Moody's Investor Service",
year = "February 28, 2011",
author = "Moody's"
}

@article{Simon11,
   author = {Noah Simon and Jerome Friedman and Trevor Hastie and Rob Tibshirani},
   title = {Regularization Paths for Cox's Proportional Hazards Model via Coordinate Descent},
   journal = {Journal of Statistical Software},
   volume = {39},
   number = {1},
   year = {2011},
   keywords = {},
   abstract = {We introduce a pathwise algorithm for the Cox proportional hazards model, regularized by convex combinations of l1 and l2 penalties (elastic net). Our algorithm fits via cyclical coordinate descent, and employs warm starts to find a solution along a regularization path. We demonstrate the efficacy of our algorithm on real and simulated data sets, and find considerable speedup between our algorithm and competing methods.},
   issn = {1548-7660},
   pages = {1--13},
   doi = {10.18637/jss.v039.i05}
}

@article{Berkowitz11,
 abstract = {We present new evidence on disaggregated profit and loss (P/L) and value-at-risk (VaR) forecasts obtained from a large international commercial bank. Our data set includes the actual daily P/L generated by four separate business lines within the bank. All four business lines are involved in securities trading and each is observed daily for a period of at least two years. Given this unique data set, we provide an integrated, unifying framework for assessing the accuracy of VaR forecasts. We use a comprehensive Monte Carlo study to assess which of these many tests have the best finite-sample size and power properties. Our desk-level data set provides importance guidance for choosing realistic P/L-generating processes in the Monte Carlo comparison of the various tests. The conditional autoregressive value-at-risk test of Engle and Manganelli (2004) performs best overall, but duration-based tests also perform well in many cases.},
 author = {Jeremy Berkowitz and Peter Christoffersen and Denis Pelletier},
 journal = {Management Science},
 number = {12},
 pages = {2213--2227},
 publisher = {INFORMS},
 title = {Evaluating Value-at-Risk Models with Desk-Level Data},
 volume = {57},
 year = {2011}
}

@article{Petris11,
   author = {Giovanni Petris and Sonia Petrone},
   title = {State Space Models in R},
   journal = {Journal of Statistical Software},
   volume = {41},
   number = {1},
   year = {2011},
   keywords = {},
   abstract = {We give an overview of some of the software tools available in R, either as built- in functions or contributed packages, for the analysis of state space models. Several illustrative examples are included, covering constant and time-varying models for both univariate and multivariate time series. Maximum likelihood and Bayesian methods to obtain parameter estimates are considered.},
   issn = {1548-7660},
   pages = {1--25},
   doi = {10.18637/jss.v041.i04}
}

@article{Tusell11,
   author = {Fernando Tusell},
   title = {Kalman Filtering in R},
   journal = {Journal of Statistical Software},
   volume = {39},
   number = {1},
   year = {2011},
   keywords = {},
   abstract = {Support in R for state space estimation via Kalman filtering was limited to one package, until fairly recently. In the last five years, the situation has changed with no less than four additional packages offering general implementations of the Kalman filter, including in some cases smoothing, simulation smoothing and other functionality.  This paper reviews some of the offerings in R to help the prospective user to make an informed choice.},
   issn = {1548-7660},
   pages = {1--27},
   doi = {10.18637/jss.v039.i02}
}

@article{Duan12,
title = {{Multiperiod corporate default prediction--A forward intensity approach}},
journal = "Journal of Econometrics",
volume = "170",
number = "1",
pages = "191 - 209",
year = "2012",
issn = "0304-4076",
doi = "https://doi.org/10.1016/j.jeconom.2012.05.002",
url = "http://www.sciencedirect.com/science/article/pii/S0304407612001145",
author = "Jin-Chuan Duan and Jie Sun and Tao Wang",
keywords = "Default, Bankruptcy, Forward intensity, Maximum pseudo-likelihood, Forward default probability, Cumulative default probability, Accuracy ratio",
abstract = "A forward intensity model for the prediction of corporate defaults over different future periods is proposed. Maximum pseudo-likelihood analysis is then conducted on a large sample of the US industrial and financial firms spanning the period 1991–2011 on a monthly basis. Several commonly used factors and firm-specific attributes are shown to be useful for prediction at both short and long horizons. Our implementation also factors in momentum in some variables and documents their importance in default prediction. The model’s prediction is very accurate for shorter horizons. Its accuracy deteriorates somewhat when the horizon is increased to two or three years, but the performance still remains reasonable. The forward intensity model is also amenable to aggregation, which allows for an analysis of default behavior at the portfolio and/or economy level."
}

@article{Koopman12,
author = { Siem   Jan   Koopman  and  André   Lucas  and  Bernd   Schwaab },
title = {Dynamic Factor Models With Macro, Frailty, and Industry Effects for U.S. Default Counts: The Credit Crisis of 2008},
journal = {Journal of Business \& Economic Statistics},
volume = {30},
number = {4},
pages = {521-532},
year  = {2012},
publisher = {Taylor & Francis},
doi = {10.1080/07350015.2012.700859},

URL = { 
        https://doi.org/10.1080/07350015.2012.700859
    
},
eprint = { 
        https://doi.org/10.1080/07350015.2012.700859
    
}

}

@book{durbin12,
  title={Time Series Analysis by State Space Methods},
  author={Durbin, James and Koopman, Siem Jan},
  volume={38},
  year={2012},
  publisher={Oxford University Press, Oxford}
}

@INPROCEEDINGS{Xianyi12,
author={Z. Xianyi and W. Qian and Z. Yunquan},
booktitle={2012 IEEE 18th International Conference on Parallel and Distributed Systems},
title={Model-Driven Level 3 BLAS Performance Optimization on Loongson 3A Processor},
year={2012},
pages={684-691},
keywords={cache storage;floating point arithmetic;linear algebra;mathematics computing;multi-threading;multiprocessing systems;optimisation;parallel architectures;public domain software;shared memory systems;software performance evaluation;Chinese Academy of Sciences;Institute of Computing Technology;L1 data cache misses;Loongson 3A 128-bit memory;Loongson 3A architecture;Loongson 3A quadcore processor;OpenBLAS;bank conflicts;basic linear algebra subprograms;extension instructions;fundamental math library;general-purpose 64-bit MIPS64 quadcore processor;mainstream processor vendor;mainstream x86 CPU;model-driven level 3 BLAS performance optimization;multiple threads;open source BLAS project;parallel performance improvement;performance model;register blocking;scientific computing;single precision floating point SIMD instructions;single thread optimization;software prefetching;word length 128 bit;word length 64 bit;Kernel;Optimization;Pipelines;Prefetching;Registers;BLAS;Loongson 3A;MIPS64;Multi-core;Optimization},
doi={10.1109/ICPADS.2012.97},
ISSN={1521-9097},
month={Dec},}

@ARTICLE{Gustafsson12,
author={F. Gustafsson and G. Hendeby},
journal={IEEE Transactions on Signal Processing},
title={Some Relations Between Extended and Unscented Kalman Filters},
year={2012},
volume={60},
number={2},
pages={545-555},
keywords={Kalman filters;Riccati equations;nonlinear filters;discrete Riccati equation;extended Kalman filter;linearized model;posterior distribution;second-order Taylor expansion;sensor network;sigma point function evaluation;sigma points;standard EKF algorithm;target tracking;unscented Kalman filter;unscented transformation;Approximation methods;Covariance matrix;Jacobian matrices;Kalman filters;Riccati equations;Taylor series;Transforms;Extended Kalman filter (EKF);transformations;unscented Kalman filter (UKF)},
doi={10.1109/TSP.2011.2172431},
ISSN={1053-587X},
month={Feb},}

@ARTICLE{Lin12,
author={W. Y. Lin and Y. H. Hu and C. F. Tsai},
journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
title={Machine Learning in Financial Crisis Prediction: A Survey},
year={2012},
volume={42},
number={4},
pages={421--436},
keywords={financial management;learning (artificial intelligence);pattern classification;bankruptcy prediction;business failures;credit score modeling;decision trees;ensemble classifiers;financial crisis prediction;financial institutions;hybrid classifiers;machine-learning techniques;neural networks;Accuracy;Boosting;Genetic algorithms;Neural networks;Predictive models;Training;Bankruptcy prediction;credit scoring;ensemble classifiers;hybrid classifiers;machine learning}}

@ARTICLE{Holmes12,
  author = {Elizabeth E. Holmes and Eric J. Ward and Kellie Wills},
  title = {MARSS: Multivariate Autoregressive State-Space Models for Analyzing Time-Series Data},
  year = {2012},
  journal = {The R Journal},
  url = {https://journal.r-project.org/archive/2012/RJ-2012-002/index.html},
  pages = {11--19},
  volume = {4},
  number = {1}
}

@ARTICLE{Holmes13,
   author = {Holmes, E.~E.},
    title = {Derivation of an EM Algorithm for Constrained and Unconstrained Multivariate Autoregressive State-Space (MARSS) Models},
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1302.3919},
 primaryClass = "stat.ME",
 keywords = {Statistics - Methodology},
     year = 2013,
    month = feb,
   adsurl = {http://adsabs.harvard.edu/abs/2013arXiv1302.3919H},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{David13,
author = {Lando, David and Medhat, Mamdouh and Nielsen, Mads Stenbo and Nielsen, Søren Feodor},
title = {Additive Intensity Regression Models in Corporate Default Analysis},
journal = {Journal of Financial Econometrics},
volume = {11},
number = {3},
pages = {443--485},
year = {2013}
}

@Manual{biglm,
    title = {biglm: Bounded Memory Linear and Generalized Linear Models},
    author = {Thomas Lumley},
    year = {2013},
    note = {R package version 0.9-1},
    url = {https://CRAN.R-project.org/package=biglm},
}

@Manual{glmpath,
    title = {glmpath: L1 Regularization Path for Generalized Linear Models and Cox Proportional Hazards Model},
    author = {Mee Young Park and Trevor Hastie},
    year = {2013},
    note = {R package version 0.97},
    url = {https://CRAN.R-project.org/package=glmpath},
  }

@article{Tinoco13,
title = "Financial distress and bankruptcy prediction among listed companies using accounting, market and macroeconomic variables",
journal = "International Review of Financial Analysis",
volume = "30",
pages = "394--419",
year = "2013",
author = "Mario Hernandez Tinoco and Nick Wilson",
keywords = "Bankruptcy, Listed companies, Financial distress, Logit regression, Neural networks"
}

@unpublished{Duan13,
  title ={Multiperiod Corporate Default Prediction with the Partially-Conditioned Forward Intensity},
  author={Duan, Jin-Chuan and Fulop, Andras},
  year={2013},
  note    = {Working paper}
} 

@Manual{openmp,
  title = {OpenMP Application Program Interface},
  author = {OpenMP Architecture Review Board},
  year = {2013},
  url = {http://www.openmp.org/},
  note = {Version 4.0},
}

@article{Wood13,
 ISSN = {00063444},
 abstract = {The problem of testing smooth components of an extended generalized additive model for equality to zero is considered. Confidence intervals for such components exhibit good across-the-function coverage probabilities if based on the approximate result f̂(i) ~ N{f(i), Vf(i,i)}, where f is the vector of evaluated values for the smooth component of interest and Vf is the covariance matrix for f according to the Bayesian view of the smoothing process. Based on this result, a Wald-type test of = 0 is proposed. It is shown that care must be taken in selecting the rank used in the test statistic. The method complements previous work by extending applicability beyond the Gaussian case, while considering tests of zero effect rather than testing the parametric hypothesis given by the null space of the component's smoothing penalty. The proposed p-values are routine and efficient to compute from a fitted model, without requiring extra model fits or null distribution simulation.},
 author = {Simon N. Wood},
 journal = {Biometrika},
 number = {1},
 pages = {221--228},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {On p-values for smooth components of an extended generalized additive model},
 volume = {100},
 year = {2013}
}

@ARTICLE{Natekin13,
AUTHOR={Natekin, Alexey and Knoll, Alois},   	 
TITLE={Gradient boosting machines, a tutorial},      	
JOURNAL={Frontiers in Neurorobotics},      	
VOLUME={7},      
PAGES={21},     	
YEAR={2013},   
ABSTRACT={Gradient boosting machines are a family of powerful machine-learning techniques that have shown considerable success in a wide range of practical applications. They are highly customizable to the particular needs of the application, like being learned with respect to different loss functions. This article gives a tutorial introduction into the methodology of gradient boosting methods. A theoretical information is complemented with many descriptive examples and illustrations which cover all the stages of the gradient boosting model design. Considerations on handling the model complexity are discussed. A set of practical examples of gradient boosting applications are presented and comprehensively analyzed.
}
}

@article{Chen14,
title = "Default prediction with dynamic sectoral and macroeconomic frailties",
journal = "Journal of Banking \& Finance",
volume = "40",
pages = "211 - 226",
year = "2014",
issn = "0378-4266",
doi = "https://doi.org/10.1016/j.jbankfin.2013.11.036",
url = "http://www.sciencedirect.com/science/article/pii/S0378426613004573",
author = "Peimin Chen and Chunchi Wu",
keywords = "Default risk, Hazard rate function, Frailty, Distance to default, Tail loss, Monte Carlo expectations maximization (EM), Gibbs sampler",
abstract = "This paper extends the macroeconomic frailty model to include sectoral frailty factors that capture default correlations among firms in a similar business. We estimate sectoral and macroeconomic frailty factors and their effects on default intensity using the data for Japanese firms from 1992 to 2010. We find strong evidence for the presence of sectoral frailty factors even after accounting for the effects of observable covariates and macroeconomic frailty on default intensity. The model with sectoral frailties performs better than that without. Results show that accounting for the sources of unobserved sectoral default risk covariations improves the accuracy of default probability estimation."
}

@article{Thomas14,
   author = {Laine Thomas and Eric Reyes},
   title = {Tutorial: Survival Estimation for Cox Regression  Models with Time-Varying Coefficients Using SAS and R},
   journal = {Journal of Statistical Software},
   volume = {61},
   number = {1},
   year = {2014},
   keywords = {},
   abstract = {Survival estimates are an essential compliment to multivariable regression models for time-to-event data, both for prediction and illustration of covariate effects. They are easily obtained under the Cox proportional-hazards model. In populations defined by an initial, acute event, like myocardial infarction, or in studies with long-term followup, the proportional-hazards assumption of constant hazard ratios is frequently violated. One alternative is to fit an interaction between covariates and a prespecified function of time, implemented as a time-dependent covariate. This effectively creates a time-varying coefficient that is easily estimated in software such as SAS and R. However, the usual programming statements for survival estimation are not directly applicable. Unique data manipulation and syntax is required, but is not well documented for either software. This paper offers a tutorial in survival estimation for the time-varying coefficient model, implemented in SAS and R. We provide a macro coxtvc to facilitate estimation in SAS where the current functionality is more limited. The macro is validated in simulated data and illustrated in an application. },
   issn = {1548-7660},
   pages = {1--23},
   doi = {10.18637/jss.v061.c01}
}

@article{Rebora14,
  author = {Paola Rebora and Agus Salim and Marie Reilly},
  title = {bshazard: A Flexible Tool for Nonparametric Smoothing of the
          Hazard Function},
  year = {2014},
  journal = {The R Journal},
  url = {https://journal.r-project.org/archive/2014/RJ-2014-028/index.html},
  pages = {114--122},
  volume = {6},
  number = {2}
}

@misc{backblazesmartstatuse,
author = {BackBlaze},
year = 2014,
title = {Hard Drive SMART Stats},
note = {Retrieved May 2017, https://www.backblaze.com/blog/hard-drive-smart-stats/}
}

@article{Qi14,
title = "Unobserved systematic risk factor and default prediction",
journal = "Journal of Banking \& Finance",
volume = "49",
pages = "216 - 227",
year = "2014",
issn = "0378-4266",
doi = "https://doi.org/10.1016/j.jbankfin.2014.09.009",
url = "http://www.sciencedirect.com/science/article/pii/S0378426614003094",
author = "Min Qi and Xiaofei Zhang and Xinlei Zhao",
keywords = "Observed systematic risk factors, Unobserved systematic risk factor, Corporate default prediction, Rank order, Predictive accuracy",
abstract = "We conduct a thorough analysis on the role played by the unobserved systematic risk factor in default prediction. We find that this latent factor outweighs the observed systematic risk factors and can substantially improve the in-sample predictive accuracy at the firm, rating group, and aggregate levels. Thus it might be helpful to include the unobserved systematic risk factor when simulating portfolio credit losses. However, we also find that this factor only marginally improves out-of-sample model performance. Therefore, although the models we investigated all show reasonably good ability to rank order firms by default risk, accurate prediction of default rate remains challenging even when the unobserved systematic risk factor is considered."
}

@article {Wood15,
author = {Wood, Simon N. and Goude, Yannig and Shaw, Simon},
title = {Generalized additive models for large data sets},
journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
volume = {64},
number = {1},
pages = {139--155},
keywords = {Correlated additive model, Electricity load prediction, Generalized additive model estimation},
year = {2015},
}

@manual{survival,
  author = {Terry M. Therneau},
  title = {A Package for Survival Analysis in S},
  url = {https://CRAN.R-project.org/package=survival},
  version = {2.38},
  year = {2015},
}

@article{Zhou15,
   author = {Yan Zhou},
   title = {vSMC: Parallel Sequential Monte Carlo in C++},
   journal = {Journal of Statistical Software, Articles},
   volume = {62},
   number = {9},
   year = {2015},
   keywords = {},
   abstract = {Sequential Monte Carlo is a family of algorithms for sampling from a sequence of distributions. Some of these algorithms, such as particle filters, are widely used in physics and signal processing research. More recent developments have established their application in more general inference problems such as Bayesian modeling.
These algorithms have attracted considerable attention in recent years not only be- cause that they have desired statistical properties, but also because they admit natural and scalable parallelization. However, they are perceived to be difficult to implement. In addition, parallel programming is often unfamiliar to many researchers though conceptually appealing.
A C++ template library is presented for the purpose of implementing generic sequential Monte Carlo algorithms on parallel hardware. Two examples are presented: a simple particle filter and a classic Bayesian modeling problem.},
   issn = {1548-7660},
   pages = {1--49},
   doi = {10.18637/jss.v062.i09}
}

@article{Bates15,
   author = {Douglas M. Bates and Martin Mächler and Ben Bolker and Steve Walker},
   title = {Fitting Linear Mixed-Effects Models Using lme4},
   journal = {Journal of Statistical Software},
   volume = {67},
   number = {1},
   year = {2015},
   keywords = {sparse matrix methods; linear mixed models; penalized least squares; Cholesky decomposition},
   abstract = {Maximum likelihood or restricted maximum likelihood (REML) estimates of the parameters in linear mixed-effects models can be determined using the lmer function in the lme4 package for R. As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed- and random-effects terms. The formula and data together determine a numerical representation of the model from which the profiled deviance or the profiled REML criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the profiled deviance or REML criterion, and the structure of classes or types that represents such a model. Sufficient detail is included to allow specialization of these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.},
   pages = {1--48}
}

@misc{backblazest3tb,
author = {Andy Klein},
year = 2015,
title = {CSI: Backblaze – Dissecting 3TB Drive Failure},
note = {Retrieved May 2017, https://www.backblaze.com/blog/3tb-hard-drive-failure/}
}

@article{Tian15,
title = "Variable selection and corporate bankruptcy forecasts",
journal = "Journal of Banking \& Finance",
volume = "52",
pages = "89--100",
year = "2015",
author = "Shaonan Tian and Yan Yu and Hui Guo",
keywords = "Discrete hazard model, Financial ratios, LASSO, Market information"
}

@Manual{polspline,
    title = {polspline: Polynomial Spline Routines},
    author = {Charles Kooperberg},
    year = {2015},
    note = {R package version 1.1.12},
    url = {https://CRAN.R-project.org/package=polspline},
}

@Article{flexsurv,
    title = {flexsurv: A Platform for Parametric Survival Modeling in R},
    author = {Christopher Jackson},
    journal = {Journal of Statistical Software},
    year = {2016},
    volume = {70},
    number = {8},
    pages = {1--33},
    doi = {10.18637/jss.v070.i08},
  }

@article{Filipe16,
title = "Forecasting distress in European SME portfolios",
journal = "Journal of Banking \& Finance",
volume = "64",
pages = "112 - 135",
year = "2016",
issn = "0378-4266",
doi = "https://doi.org/10.1016/j.jbankfin.2015.12.007",
url = "http://www.sciencedirect.com/science/article/pii/S0378426615003611",
author = "Sara Ferreira Filipe and Theoharry Grammatikos and Dimitra Michala",
keywords = "Credit risk, Distress, Forecasting, SMEs, Logit",
abstract = "In this paper, we examine idiosyncratic and systematic distress predictors for small and medium sized enterprises (SMEs) in Europe over the period 2000–2009. We find that SMEs across European regions are vulnerable to common idiosyncratic factors but systematic factors vary. Moreover, systematic factors move average distress rates and small SMEs are more vulnerable to these factors compared to large SMEs. By including many very small companies in the sample, our models offer unique insights into the European small business sector. By exploring distress in a multi-country setting, the models uncover regional vulnerabilities. Finally, by incorporating systematic dependencies, the models capture distress co-movements."
}

@Manual{concreg,
    title = {concreg: Concordance Regression},
    author = {Georg Heinze and Meinhard Ploner and Daniela Dunkler},
    year = {2016},
    note = {R package version 0.6},
    url = {https://CRAN.R-project.org/package=concreg},
}

@Manual{rstpm2,
  title = {rstpm2: Generalized Survival Models},
  author = {Mark Clements and Xing-Rong Liu},
  year = {2016},
  note = {R package version 1.3.4},
  url = {https://CRAN.R-project.org/package=rstpm2},
}

@Article{Rizopoulos16,
    title = {The R Package JMbayes for Fitting Joint Models for Longitudinal and Time-to-Event Data Using MCMC},
    author = {Dimitris Rizopoulos},
    journal = {Journal of Statistical Software},
    year = {2016},
    volume = {72},
    number = {7},
    pages = {1--45},
    doi = {10.18637/jss.v072.i07},
  }

@article{Zieba16,
title = "Ensemble boosted trees with synthetic features generation in application to bankruptcy prediction",
journal = "Expert Systems with Applications",
volume = "58",
pages = "93--101",
year = "2016",
author = "Zięba, Maciej and Tomczak, Sebastian K. and Tomczak, Jakub M.",
keywords = "Bankruptcy prediction, Extreme gradient boosting, Synthetic features generation, Imbalanced data"
}

@article{Schwaab16,
author = {Schwaab, Bernd and Koopman, Siem Jan and Lucas, André},
title = {Global Credit Risk: World, Country and Industry Factors},
journal = {Journal of Applied Econometrics},
volume = {32},
number = {2},
pages = {296-317},
doi = {10.1002/jae.2521},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.2521},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/jae.2521},
year = {2017}
}

@inproceedings{Chen16,
 author = {Chen, Tianqi and Guestrin, Carlos},
 title = {XGBoost: A Scalable Tree Boosting System},
 booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 year = {2016},
 location = {San Francisco, California, USA},
 pages = {785--794},
 numpages = {10},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {large-scale machine learning},
}

@article{menegaz16,
  title={Unscented Kalman Filtering on Euclidean and Riemannian Manifolds},
  author={Menegaz, Henrique Marra Taira},
  year={2016}
}

@article{King16,
   author = {Aaron King and Dao Nguyen and Edward Ionides},
   title = {Statistical Inference for Partially Observed Markov Processes via the R Package pomp},
   journal = {Journal of Statistical Software},
   volume = {69},
   number = {1},
   year = {2016},
   keywords = {Markov processes; hidden Markov model; state space model; stochastic dynamical system; maximum likelihood; plug-and-play; time series; mechanistic model; sequential Monte Carlo; R},
   abstract = {Partially observed Markov process (POMP) models, also known as hidden Markov models or state space models, are ubiquitous tools for time series analysis. The R package pomp provides a very flexible framework for Monte Carlo statistical investigations using nonlinear, non-Gaussian POMP models. A range of modern statistical methods for POMP models have been implemented in this framework including sequential Monte Carlo, iterated filtering, particle Markov chain Monte Carlo, approximate Bayesian computation, maximum synthetic likelihood estimation, nonlinear forecasting, and trajectory matching. In this paper, we demonstrate the application of these methodologies using some simple toy problems. We also illustrate the specification of more complex POMP models, using a nonlinear epidemiological model with a discrete population, seasonality, and extra-demographic stochasticity. We discuss the specification of user-defined models and the development of additional methods within the programming environment provided by pomp.},
   issn = {1548-7660},
   pages = {1--43},
   doi = {10.18637/jss.v069.i12}
}

@article{Sanderson16,
  doi = {10.21105/joss.00026},
  year  = {2016},
  month = {jun},
  publisher = {The Open Journal},
  volume = {1},
  number = {2},
  author = {Conrad Sanderson and Ryan Curtin},
  title = {Armadillo: A Template-Based C++ Library for Linear Algebra},
  journal = {The Journal of Open Source Software}
}

@Manual{pch,
    title = {pch: Piecewise Constant Hazards Models for Censored and Truncated Data},
    author = {Paolo Frumento},
    year = {2016},
    note = {R package version 1.3},
    url = {https://CRAN.R-project.org/package=pch},
  }

@book{Tutz16,
  title={Modeling Discrete Time-to-Event Data},
  author={Tutz, Gerhard and Schmid, Matthias},
  year={2016},
  publisher={Springer-Verlag, NY}
}

@misc{backblaze2016Q1,
author = {Andy Klein},
year = 2016,
title = {One Billion Drive Hours and Counting: Q1 2016 Hard Drive Stats},
note = {Retrieved May 2017, https://www.backblaze.com/blog/hard-drive-reliability-stats-q1-2016/}
}

@article{Liu16,
author = {Xing-Rong Liu and Yudi Pawitan and Mark Clements},
title ={Parametric and penalized generalized survival models},
journal = {Statistical Methods in Medical Research},
volume = {27},
number = {5},
pages = {1531-1546},
year = {2016},
doi = {10.1177/0962280216664760},

URL = { 
        https://doi.org/10.1177/0962280216664760
    
},
eprint = { 
        https://doi.org/10.1177/0962280216664760
    
}
,
    abstract = { We describe generalized survival models, where g(S(t|z)), for link function g, survival S, time t, and covariates z, is modeled by a linear predictor in terms of covariate effects and smooth time effects. These models include proportional hazards and proportional odds models, and extend the parametric Royston–Parmar models. Estimation is described for both fully parametric linear predictors and combinations of penalized smoothers and parametric effects. The penalized smoothing parameters can be selected automatically using several information criteria. The link function may be selected based on prior assumptions or using an information criterion. We have implemented the models in R. All of the penalized smoothers from the mgcv package are available for smooth time effects and smooth covariate effects. The generalized survival models perform well in a simulation study, compared with some existing models. The estimation of smooth covariate effects and smooth time-dependent hazard or odds ratios is simplified, compared with many non-parametric models. Applying these models to three cancer survival datasets, we find that the proportional odds model is better than the proportional hazards model for two of the datasets. }
}

@article{Nordh17,
   author = {Jerker Nordh},
   title = {pyParticleEst: A Python Framework for Particle-Based Estimation Methods},
   journal = {Journal of Statistical Software, Articles},
   volume = {78},
   number = {3},
   year = {2017},
   keywords = {particle filter; particle smoother; expectation-maximization; system identification; Rao-Blackwellized; Python},
   abstract = {Particle methods such as the particle filter and particle smoothers have proven very useful for solving challenging nonlinear estimation problems in a wide variety of fields during the last decade. However, there are still very few existing tools available to support and assist researchers and engineers in applying the vast number of methods in this field to their own problems. This paper identifies the common operations between the methods and describes a software framework utilizing this information to provide a flexible and extensible foundation which can be used to solve a large variety of problems in this domain, thereby allowing code reuse to reduce the implementation burden and lowering the barrier of entry for applying this exciting field of methods. The software implementation presented in this paper is freely available and permissively licensed under the GNU Lesser General Public License, and runs on a large number of hardware and software platforms, making it usable for a large variety of scenarios.},
   issn = {1548-7660},
   pages = {1--25},
   doi = {10.18637/jss.v078.i03}
}

@misc{backblazepods,
author = {BackBlaze},
title = {The Backblaze Storage Pod},
note = {Retrieved May 2017, https://www.backblaze.com/b2/storage-pod.html}
}

@Manual{eha17,
    title = {eha: Event History Analysis},
    author = {Göran Broström},
    year = {2017},
    note = {R package version 2.5.0},
    url = {https://CRAN.R-project.org/package=eha},
}

@Manual{rms17,
    title = {rms: Regression Modeling Strategies},
    author = {Frank E {Harrell Jr}},
    year = {2017},
    note = {R package version 5.1-1},
    url = {https://CRAN.R-project.org/package=rms},
  }

@misc{backblazestats,
author = {BackBlaze},
year = 2017,
title = {Hard Drive Data and Stats},
note = {Retrieved May 2017, https://www.backblaze.com/b2/hard-drive-test-data.html}
}

@article{Jensen17,
title = "Cyclicality and Firm Size in Private Firm Defaults",
abstract = "The Basel II/III and CRD IV Accords reduce capital charges on bank loans to smaller firms by assuming that the default probabilities of smaller firms are less sensitive to macroeconomic cycles. We test this assumption in a default intensity framework using a large sample of bank loans to private Danish firms. We find that controlling only for size, the default probabilities of small firms are, in fact, less cyclical than the default probabilities of large firms. However, accounting for firm characteristics other than size, we find that the default probabilities of small firms are equally cyclical or even more cyclical than the default probabilities of large firms. These results hold using a multiplicative Cox model as well as an additive Aalen model with time-varying coefficients.",
author = "Jensen, {Thais L{\ae}rkholm} and David Lando and Mamdouh Medhat",
year = "2017",
language = "English",
volume = "13",
pages = "97--145",
journal = "International Journal of Central Banking",
issn = "1815-4654",
publisher = "Association of the International Journal of Central Banking",
number = "4",
}

@book{Wood17,
    title = {Generalized Additive Models: An Introduction with R},
    year = {2017},
    author = {S.N Wood},
    edition = {2},
    publisher = {Chapman and Hall/CRC},
  }

@article {Jones17,
author = {Jones, Stewart and Johnstone, David and Wilson, Roy},
title = {Predicting Corporate Bankruptcy: An Evaluation of Alternative Statistical Frameworks},
journal = {Journal of Business Finance \& Accounting},
volume = {44},
number = {1-2},
pages = {3--34},
keywords = {corporate bankruptcy prediction, binary classifiers, statistical learning},
year = {2017},
}

@Manual{sas17,
  title = {SAS/STAT Software, Version~9.4},
  author = {{SAS Institute Inc}},
  address = {Cary, NC},
  year = {2017},
  url = {http://support.sas.com/},
}

@Manual{stan,
  title = {The Stan Core Library},
  author = {Stan Development Team},
  address = {Cary, NC},
  year = {2017},
  note = {Version 2.17.0},
  url = {http://mc-stan.org},
}

@article{Nickerson17,
title = "Debt correlations in the wake of the financial crisis: What are appropriate default correlations for structured products?",
journal = "Journal of Financial Economics",
volume = "125",
number = "3",
pages = "454--474",
year = "2017",
author = "Jordan Nickerson and John M. Griffin",
keywords = "Credit ratings, Financial crises, Structured finance, Default correlations"
}

@Manual{stata17,
  title = {Stata Statistical Software: Release 15},
  author = {StataCorp},
  organization = {StataCorp LLC},
  address = {College Station, TX},
  year = {2017},
}

@Manual{dynsurv,
    title = {dynsurv: Dynamic Models for Survival Data},
    author = {Xiaojing Wang and Ming-Hui Chen and Wenjie Wang and Jun Yan},
    note = {R package version 0.3-5},
    year = {2017},
    url = {https://CRAN.R-project.org/package=dynsurv},
  }

@techreport{rossum17,
 author = {Rossum, Guido},
 title = {Python Reference Manual},
 year = {2017},
 note = {Release 2.7.14},
 url = {https://docs.python.org/2.7/library/},
}

@book{rigatos17,
  title={State-Space Approaches for Modelling and Control in Financial Engineering, Systems Theory and Machie Learning Methods},
  author={Rigatos, G},
  year={2017},
  publisher={Springer-Verlag, New York}
}

@article{Liu17,
author = {Liu, Xing-Rong and Pawitan, Yudi and Clements, Mark S.},
title = {Generalized survival models for correlated time-to-event data},
journal = {Statistics in Medicine},
volume = {36},
number = {29},
pages = {4743-4762},
year = {2017},
keywords = {adaptive Gauss-Hermite quadrature, correlated survival data, generalized survival models, link functions, random effects, tensor product},
doi = {10.1002/sim.7451},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7451},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.7451},
abstract = {Our aim is to develop a rich and coherent framework for modeling correlated time-to-event data, including (1) survival regression models with different links and (2) flexible modeling for time-dependent and nonlinear effects with rich postestimation. We extend the class of generalized survival models, which expresses a transformed survival in terms of a linear predictor, by incorporating a shared frailty or random effects for correlated survival data. The proposed approach can include parametric or penalized smooth functions for time, time-dependent effects, nonlinear effects, and their interactions. The maximum (penalized) marginal likelihood method is used to estimate the regression coefficients and the variance for the frailty or random effects. The optimal smoothing parameters for the penalized marginal likelihood estimation can be automatically selected by a likelihood-based cross-validation criterion. For models with normal random effects, Gauss-Hermite quadrature can be used to obtain the cluster-level marginal likelihoods. The Akaike Information Criterion can be used to compare models and select the link function. We have implemented these methods in the R package rstpm2. Simulating for both small and larger clusters, we find that this approach performs well. Through 2 applications, we demonstrate (1) a comparison of proportional hazards and proportional odds models with random effects for clustered survival data and (2) the estimation of time-varying effects on the log-time scale, age-varying effects for a specific treatment, and two-dimensional splines for time and age.}
}

@Manual{pomp,
  title = {pomp: {S}tatistical Inference for Partially Observed {M}arkov Processes},
  author = {Aaron A. King and Edward L. Ionides and Carles Martinez Bret\'o and Stephen P. Ellner and Matthew J. Ferrari and Bruce E. Kendall and Michael Lavine and Dao Nguyen and Daniel C. Reuman and Helen Wearing and Simon N. Wood},
  year = {2017},
  note = {R~package, version~1.15},
  url = {http://kingaa.github.io/pomp},
}

@article{helske17,
   author = {Jouni Helske},
   title = {KFAS: Exponential Family State Space Models in R},
   journal = {Journal of Statistical Software},
   volume = {78},
   number = {10},
   year = {2017},
   keywords = {R; exponential family; state space models; time series; forecasting; dynamic linear models},
   abstract = {State space modeling is an efficient and flexible method for statistical inference of a broad class of time series and other data. This paper describes the R package KFAS for state space modeling with the observations from an exponential family, namely Gaussian, Poisson, binomial, negative binomial and gamma distributions. After introducing the basic theory behind Gaussian and non-Gaussian state space models, an illustrative example of Poisson time series forecasting is provided. Finally, a comparison to alternative R packages suitable for non-Gaussian time series modeling is presented.},
   issn = {1548-7660},
   pages = {1--39},
   doi = {10.18637/jss.v078.i10}
}

@Manual{pilon17,
  author       = {Cameron Davidson-Pilon and
                  Jonas Kalderstam and
                  Ben Kuhn and
                  Andrew Fiore-Gartland and
                  Alex Parij and
                  Kyle Stark and
                  Steven Anton and
                  Lilian Besson and
                  Jona and
                  Harsh Gadgil and
                  Dave Golland and
                  Sean Hussey and
                  anmolgarg and
                  Trent Hauck and
                  Robert Schwarz and
                  zaxtax and
                  akkineniramesh and
                  Niels Bantilan and
                  Nick Furlotte and
                  Nick Evans and
                  Lukasz and
                  Jonathan Séguin and
                  Jeff Rose and
                  Isaac Slavitt and
                  Eric Martin and
                  Eduardo Ochoa and
                  dhuynh and
                  Chris Fournier and
                  André F. Rendeiro},
  title        = {lifelines: 0.11.1},
  month        = jun,
  year         = 2017,
  url          = {https://doi.org/10.5281/zenodo.815943}
}

@Manual{Enea17,
    title = {speedglm: Fitting Linear and Generalized Linear Models to Large Data Sets},
    author = {Marco Enea},
    year = {2017},
    note = {R package version 0.3-2},
    url = {https://CRAN.R-project.org/package=speedglm},
  }

@article{Azizpour18,
title = "Exploring the sources of default clustering",
journal = "Journal of Financial Economics",
volume = "129",
number = "1",
pages = "154 - 183",
year = "2018",
issn = "0304-405X",
doi = "https://doi.org/10.1016/j.jfineco.2018.04.008",
url = "http://www.sciencedirect.com/science/article/pii/S0304405X1830103X",
author = "S Azizpour and K. Giesecke and G. Schwenkler",
keywords = "Default clustering, Contagion, Frailty, Correlated default risk",
abstract = "We study the sources of corporate default clustering in the United States. We reject the hypothesis that firms’ default times are correlated only because their conditional default rates depend on observable and latent systematic factors. By contrast, we find strong evidence that contagion, through which the default by one firm has a direct impact on the health of other firms, is a significant clustering source. The amount of clustering that cannot be explained by contagion and firms’ exposure to observable and latent systematic factors is insignificant. Our results have important implications for the pricing and management of correlated default risk."
}

@article{Kwon18,
title = "Industry specific defaults",
journal = "Journal of Empirical Finance",
volume = "45",
pages = "45--58",
year = "2018",
author = "Tae Yeon Kwon and Yoonjung Lee",
keywords = "Intensity credit risk model, Within industry default correlation, Between industries default correlation, Frailty, MCEM"
}


@unpublished{Christoffersen18,
title = {Can Machine Learning Models Capture Correlations in Corporate Distresses?},
author = {Christoffersen, Benjamin and Matin, Rastin and M{\o}lgaard, Pia},
year = {2018},
note = {Available at SSRN: \url{https://ssrn.com/abstract=3273985}}
}

@Manual{R2018,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2018},
    url = {https://www.R-project.org/},
 }

@Manual{Christoffersen19,
    title = {dynamichazard: Dynamic Hazard Models using State Space Models},
    author = {Benjamin Christoffersen},
    note = {R package version 0.6.4},
    year = {2019},
    url = {https://github.com/boennecd/dynamichazard},
 }

@misc{StatKONK9,
  author = "{Danmarks Statistik}",
  year = 2018,
  title = {KONK9: Erklærede konkurser (historisk sammendrag)},
  note  = {Retrieved September 2018, http://www.statistikbanken.dk/KONK9}
}

@misc{StatFIKS9,
  author = "{Danmarks Statistik}",
  year = 2018,
  title = {FIKS9: Firmaernes køb og salg, historisk sammendrag efter beløb},
  note  = {Retrieved September 2018, http://www.statistikbanken.dk/FIKS9}}
}

