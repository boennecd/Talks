---
title: "Imputation Using Gaussian Copulas"
bibliography: ref.bib
biblio-style: apa
output: 
  revealjs::revealjs_presentation:
    css: styles.css
    theme: black
    center: false
    transition: slide
    highlight: monochrome
    self_contained: true
    reveal_options:
      slideNumber: true
    includes:
      in_header: header.html
      after_body: doc_suffix.html
---

## dummy slide

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.height = 5, cache.path = "cache/")
.par_use <- list(cex = 1.33, cex.lab = 1.2)
options(digits = 3)
```

```{r load_pkgs, echo = FALSE}
library(splines2)
library(mdgc)
library(mvtnorm)
library(bench)
library(missForest)
library(mixedgcImp)
```


<!--html_preserve-->
<script>
(function() {
  document.getElementById("dummy-slide").remove(); 
  
  var front_div = document.getElementsByTagName("section")[0];
  front_div.classList.add("front");
  front_div.classList.add("center");
  
  // add second header
  var second_head = document.createElement("h1");
  var node = document.createTextNode("Using Quasi-Random Numbers");
  second_head.appendChild(node);
  second_head.style.margin = "0";
  front_div.appendChild(second_head);
  
  // add author 
  var credit_div = document.createElement('div');
  credit_div.innerHTML += "<div class='w-small'><p>Benjamin Christoffersen</p><p class='smallish'>KI, Department of Medical Epidemiology and Biostatistics, <a href='mailto:benjamin.christoffersen@ki.se'>benjamin.christoffersen@ki.se</a></p><p class='smallish'>KTH, Division of Robotics, Perception and Learning, <a href='mailto:benchr@kth.se'>benchr@kth.se</a></p></div>";
  credit_div.classList.add("authors");
  front_div.appendChild(credit_div);
})();
</script>
<!--end dummy slide-->

</section>

<section>
<section class="large-first center slide level2">
<h1>Introduction</h1>
<!--/html_preserve-->

<div style="display: none;">
$$
\renewcommand\vec{\boldsymbol}
\def\bigO#1{\mathcal{O}(#1)}
\def\Cond#1#2{\left(#1\,\middle|\, #2\right)}
\def\mat#1{\boldsymbol{#1}}
\def\der{{\mathop{}\!\mathrm{d}}}
\def\argmax{\text{arg}\,\text{max}}
\def\Prob{\text{P}}
\def\diag{\text{diag}}
$$
</div>

## Introduction Example

```{r def_sim_func, echo = FALSE}
# simulates a data set and mask some of the data.
#
# Args: 
#   n: number of observations. 
#   p: number of variables. 
#   n_lvls: number of levels for the ordinal variables. 
# 
# Returns: 
#   Simluated masked data and true covariance matrix. 
sim_dat <- function(n, p = 3L, n_lvls = 5L){
  # get the covariance matrix
  Sig <- cov2cor(drop(rWishart(1L, p, diag(p))))
    
  # draw the observations
  truth <- matrix(rnorm(n * p), n) %*% chol(Sig)
  
  # determine the type
  type <- rep(1:3, each = floor((p + 3 - 1) / 3))[1:p]
  is_con <- type == 1L
  is_bin <- type == 2L
  is_ord <- type == 3L
  
  # sample which are masked data 
  is_mask <- matrix(runif(n * p) < .3, n)
  
  # make sure we have no rows with all missing data
  while(any(all_nans <- rowSums(is_mask) == NCOL(is_mask)))
    is_mask[all_nans, ] <- runif(sum(all_nans) * p) < .3
  
  # create observed data
  truth_obs <- data.frame(truth)
  knots <- c(.1, .5, .75)
  co <- c(0.534586065364649, 0.117916322013627, 0.00547016125945546, 
          0.0885828123367887, 0.253444639025481)
  con_u <- pnorm(do.call(c, truth_obs[, is_con]))
  con_u <- drop(iSpline(con_u, knots = knots, degree = 2) %*% co)
  con_u <- pmax(con_u,     .Machine$double.eps^(3/4))
  con_u <- pmin(con_u, 1 - .Machine$double.eps^(3/4))
  truth_obs[, is_con] <- con_u
  
  is_con <- which(is_con)
  is_norm <- is_con[is_con %% 3L == 0L]
  truth_obs[, is_norm]  <- qnorm(
    do.call(c, truth_obs[, is_norm, drop = FALSE]))
  is_exp <- is_con[is_con %% 3L == 1L]
  truth_obs[, is_exp]   <- qexp(
    do.call(c, truth_obs[, is_exp, drop = FALSE]))
  
  bs_border <- 0
  truth_obs[, is_bin] <- 
    truth_obs[, is_bin] > rep(bs_border, each = NROW(truth_obs))
  
  bs_ord <- qnorm(seq(0, 1, length.out = n_lvls + 1L))
  truth_obs[, is_ord] <- as.integer(cut(truth[, is_ord], breaks = bs_ord))
  for(i in which(is_ord)){
    truth_obs[, i] <- ordered(truth_obs[, i])
    levels(truth_obs[, i]) <- 
      LETTERS[seq_len(length(unique(truth_obs[, i])))]
  }

  # mask the data
  seen_obs <- truth_obs
  seen_obs[is_mask] <- NA
  
  list(truth = truth, truth_obs = truth_obs, seen_obs = seen_obs, 
       Sigma = Sig)
}

# simulate and show the data
set.seed(1)
p <- 9L
dat <- sim_dat(2000L, p = p)
dat_obs <- dat$seen_obs
```

```{r show_dat}
head(dat_obs, 10)
```

## Copula Methods

<ol type = "1">
<li>
Estimate the $p$ marginal cumulative distribution functions (CDFs) $\widehat F_1, \dots, \widehat F_p$.</li>
<li class="fragment">Use CDFs, $\widehat F_i$s, to map to $(0, 1)^p$.</li>
<li class="fragment">Make assumptions for the joint distribution on the $(0, 1)^p$ scale.</li>
</ol>

<!--html_preserve-->
</section>
<section class="center-horiz">
<h2>Marginal CDFs</h2>
<!--/html_preserve-->

```{r show_margs, eval = TRUE, echo = FALSE}
par(mar = c(5, 5, 1, 1), mfcol = c(2, 2))
plot(ecdf(dat_obs$X1), main = "", lty = 1, xlab = expression(X[1]), 
     ylab = expression(hat("F")[1]), bty = "l", xaxs="i", yaxs="i")
grid()
plot(ecdf(dat_obs$X2), main = "", lty = 1, xlab = expression(X[2]), 
     ylab = expression(hat("F")[2]), bty = "l", xaxs="i", yaxs="i")
grid()
plot(ecdf(dat_obs$X3), main = "", lty = 1, xlab = expression(X[3]), 
     ylab = expression(hat("F")[3]), bty = "l", xaxs="i", yaxs="i")
grid()
```

## Marginal CDFs
Continuous variables maps to a point in $(0, 1)$. 

<div class = "fragment w-small">
Binary/ordinal variables maps to intervals in $(0, 1)$.
<p class = "smallish">
Result of discretization of a hidden variable.</p>
</div>

<p class = "fragment">
Make assumptions on the joint distribution of hidden variables.</p>

## Presentation Outline
Motivation.

<p class = "fragment">
Show the marginal likelihood.</p>

<p class = "fragment">
Show CDF approximations, gradient approximation, and imputation method.</p>

<p class = "fragment">
Simulation example and simulation study.</p>

<p class = "fragment"> 
Conclusions.</p>

<!--html_preserve-->
</section>
<!-- need extra end tag before next section -->
</section>



<section>
<section class="large-first center slide level2">
<h1>Motivation</h1>
<!--/html_preserve-->

## Motivation
@zhao19 suggest a method to estimate the model. 

<p class = "fragment"> 
Use approximate E-step in an EM algorithm and approximate imputation 
method.</p>

<p class = "fragment"> 
Show six-fold reduction in computation time compared with the MCMC 
method suggested by @hoff07.</p> 

<p class = "fragment"> 
Show good performance compared with alternative methods.</p>

## Critique
@zhao19 approximations may fail in some cases. 

<p class = "fragment">
Direct approximation of the log marginal likelihood is possible for 
a moderate amount of variables ($p < 100$).</p>

<!--html_preserve-->
</section>
<!-- need extra end tag before next section -->
</section>


<section>
<section class="large-first center slide level2">
<h1>Marginal Likelihood</h1>
<!--/html_preserve-->

## Setup
Have $\vec x_1, \dots, \vec x_n$ outcomes. 

<p class = "fragment">
Each come from a hidden vector $\vec z_1, \dots, \vec z_n\in\mathbb{R}^p$.</p>

<div class = "fragment">
$$\vec z_i \sim N^{(p)}(\vec 0, \mat C)$$

where $\mat C$ is an unknown correlation matrix.
</div>

## Continuous Variables
<div class = "w-small">
Continuous variable $x_{ij}$ is the result of $F_j^{-1}\circ \Phi$ applied 
to $z_{ij}$. 
<p class="smallish">
$\Phi$ is the standard normal distribution's CDF
and $F_j$ is an unknown CDF.</p>
</div>

<p class = "fragment">
The inverse we will use is $\Phi^{-1} \circ \widehat F_j$.</p>

## Ordinal/Binary Variables
Ordinal variable $j$ with $k_j$-levels is a discretization of $z_{ij}$. 

<p class = "fragment">
It has unknown borders given by 
$b_{j0} = 0 < b_{j1} < b_{j2} < \cdots < b_{jk_j} = 1$.</p>

<div class = "fragment">
$x_{ij}$ is the result of $F_j^{-1}\circ \Phi$ applied 
to $z_{ij}$ where $F_j^{-1}:\, (0, 1)\rightarrow \{1, \dots, k_j \}$ with

$$
F_j^{-1}(u) = l \Leftrightarrow b_{j,l-1} < u \leq b_{jl}.
$$
</div>

## Ordinal/Binary Variables
We estimate $\widehat b_{j1}, \dots, \widehat b_{j,k_j - 1}$ and define
$\widehat F_j:\, \{1, \dots, k_j\} \rightarrow (0, 1)^2$ such that 

$$\widehat F_j(x) = (\widehat b_{j,l - 1}, \widehat b_{jl}] \Leftrightarrow x = l.$$

<div class = "w-small fragment">
Map from the level $x_{ij}$ to an interval in $\mathbb R^2$ 
using $\Phi^{-1} \circ \widehat F_j$
<p class="smallish">
where $\Phi^{-1}$ is applied elementwise.</p>
</div>

## Notation
Let 

$$\vec x_{i\mathcal I} = (x_{il_1}, \dots, x_{il_m})^\top, \qquad \mathcal I = \{l_1, \dots, l_m\}.$$

<div class = "w-small fragment" data-fragment-index=1>
We observe:

<ol type = "1">
<li>the continuous variables in $\mathcal C_i \subseteq \{1,\dots, p\}$.</li> 
<li class = "fragment" data-fragment-index=2>the ordinal variables in $\mathcal O_i \subseteq \{1,\dots, p\}$.</li>
<li class = "fragment" data-fragment-index=3>miss the variables in $\mathcal M_i \subseteq \{1,\dots, p\}$.</li>
</ol>

<p class="smallish fragment" data-fragment-index=3>
Where 
$\mathcal C_i \cap \mathcal O_i = \mathcal C_i \cap \mathcal M_i = \mathcal O_i \cap \mathcal M_i = \emptyset$ 
and $\mathcal C_i \cup \mathcal O_i \cup \mathcal M_i = \{1, \dots, p\}$.</p>
</div>

## Notation 
Let

$$
\begin{align*}\mat C_{\mathcal I\mathcal J} &= 
  \begin{pmatrix} c_{l_1h_1} & \cdots & c_{l_1h_s} \\ 
  \vdots & \ddots & \vdots \\ 
  c_{l_mh_1} & \cdots & c_{l_mh_s}\end{pmatrix}, 
  \qquad \mathcal J = \{h_1, \dots, h_s\} \\
\widehat{\vec z}_{\mathcal C_i} &= (\Phi^{-1}(\widehat F_j(x_{ij})))_{
  j \in \mathcal C_i} \\
\mathcal D_i &= 
  \left\{j \in \mathcal O_i: (\Phi^{-1}(\widehat b_{j,x_{ij} - 1}), 
                              \Phi^{-1}(\widehat b_{jx_{ij}})]\right\}
\end{align*}
$$

## Log Marginal Likelihood
<div class = "w-small">
$$
\begin{align*}
L_i(\mat V) &= \log \phi^{(\lvert\mathcal C_i\rvert)}
  (\widehat{\vec z}_{\mathcal C_i}; \vec 0, 
   \mat V_{\mathcal C_i\mathcal C_i}) +
   \log\Phi^{(\lvert\mathcal O_i\rvert)}(\\
&\hspace{20pt} \mathcal D_i; \mat V_{\mathcal O_i\mathcal C_i} 
    \mat V_{\mathcal C_i\mathcal C_i}^{-1}\widehat{\vec z}_{\mathcal C_i},
    \mat V_{\mathcal C_i\mathcal C_i} - 
    \mat V_{\mathcal O_i\mathcal C_i}\mat V_{\mathcal C_i\mathcal C_i}^{-1}
    \mat V_{\mathcal C_i\mathcal O_i}) \\
L(\mat V) &= \sum_{i = 1}^n L_i(\mat V)
\end{align*}
$$
<p class="smallish">
where $\phi^{(s)}$ is a $s$-dimensional multivariate normal density and 
$\Phi^{(s)}$ is a $s$-dimensional multivariate normal CDF over a 
hypercube.</p>
</div>

<p class = "fragment">
Need an approximation for the CDF.</p>

<!--html_preserve-->
</section>
<!-- need extra end tag before next section -->
</section>


<section>
<section class="large-first center slide level2">
<h1>Approximations</h1>
<!--/html_preserve-->

## CDF Approximation
Want an approximation of 

$$
\Phi^{(h)}(\mathcal P; \vec\mu, \mat\Sigma) = 
  \int_{a_1}^{b_1}\cdots\int_{a_h}^{b_h}
  \phi^{(h)}(\vec z; \vec\mu,\mat\Sigma)\der\vec z
$$

for some $\mathcal P = (a_1, b_1]\times \cdots \times (a_h, b_h]$. 

<div class = "w-small fragment">
Use method from @Genz02.
<p class="smallish">
Closely follow @genz09.</p>
</div>

## Separation-of-Variables 
Let $\mat S$ be Cholesky decomposition of $\mat\Sigma = \mat S\mat S^\top$. Then 
use $\mat S\vec y + \vec \mu = \vec z$:

$$
\begin{align*}
\Phi^{(h)}(\mathcal P; \vec\mu, \mat\Sigma) = 
  \int_{\tilde a_1}^{\tilde b_1}\phi(y_1)
  \int_{\tilde a_2(y_1)}^{\tilde b_2(y_1)}\phi(y_2)
  \cdots\int_{\tilde a_h(y_1, \dots, y_{h-1})}^{
  \tilde b_h(y_1, \dots, y_{h-1})}
  \phi(y_h)\der\vec y
\end{align*}
$$

$$
\tilde a_j(y_1,\dots y_{j - 1}) = 
  \frac{a_j - \sum_{l = 1}^{j - 1}s_{jl}y_l}{s_{jj}}
$$

and $\tilde b_j$ similarly.

## Separation-of-Variables 
Use $\Phi^{-1}(u_j) = y_j$:

$$
\begin{align*}
\Phi^{(h)}(\mathcal P; \vec\mu, \mat\Sigma) = 
  \int_{\Phi(\tilde a_1)}^{\Phi(\tilde b_1)}
  \int_{\Phi(\tilde a_2(\Phi^{-1}(u_1)))}^{\Phi(\tilde b_2(\Phi^{-1}(u_1)))}
  \cdots\int_{\Phi(\tilde a_h(\Phi^{-1}(u_1), \dots, \Phi^{-1}(u_{h-1})))}^{
  \Phi(\tilde b_h(\Phi^{-1}(u_1), \dots, \Phi^{-1}(u_{h-1})))}
  \der\vec u
\end{align*}
$$

## Separation-of-Variables 
Re-scale and relocate with: 

$$
\begin{align*}
\bar a_j + (\bar b_j - \bar a_j)w_j &= u_j \\
\bar a_j(w_1, \dots, w_{j -1}) &= 
  \Phi(\tilde a_h(\Phi^{-1}(u_1), \dots, \Phi^{-1}(u_{j-1}))) \\
\bar b_j(w_1, \dots, w_{j -1}) &= 
  \Phi(\tilde b_h(\Phi^{-1}(u_1), \dots, \Phi^{-1}(u_{j-1}))) \\
\Phi^{(h)}(\mathcal P; \vec\mu, \mat\Sigma) &= 
  (\bar b_1 - \bar a_1)
  \int_0^1(\bar b_2(w_1) - \bar a_2(w_1))\cdots \\
&\hspace{20pt}
  \cdot\int_0^1(\bar b_h(w_1, \dots, w_{h - 1}) - \bar a_h(w_1, \dots, w_{h - 1}))
  \int_0^1\der\vec w
\end{align*}
$$

<!--html_preserve-->
</section>
<section class="center-horiz">
<h2>Example with Original Integrand</h2>
<!--/html_preserve-->

```{r org_integrand_dens, echo = FALSE}
Sigma <- matrix(c(1, 3/5, 1/3, 3/5, 1, 11/15, 1/3, 11/15, 1), 3L)
b <- c(1, 4, 2)

xs <- seq(-3, b[1], length.out = 50)
ys <- seq(-3, b[2], length.out = 50)
xy <- expand.grid(y = ys, x = xs)
integrand <- mapply(function(x, y){
  v1 <- dmvnorm(c(x, y), sigma = Sigma[1:2, 1:2])
  m <- Sigma[3, 1:2] %*% solve(Sigma[1:2, 1:2], c(x, y))
  v <- Sigma[3, 3] - Sigma[3, 1:2] %*% solve(Sigma[1:2, 1:2], Sigma[1:2, 3])
  v1 * pnorm(b[3], m, sqrt(v))
}, x = xy$x, y = xy$y)
integrand <- integrand / max(integrand)

par(mar = c(4.5, 4.5, 1, 1))
cl <- gray.colors(end = 0, start = .9, 20)
filled.contour(x = xs, y = ys, matrix(integrand, length(xs)), 
               xlab = expression(z[1]), ylab = expression(z[2]), col = cl, 
               frame.plot = FALSE)
```

<!--html_preserve-->
</section>
<section class="center-horiz">
<h2>Example with Transformed Integrand</h2>
<!--/html_preserve-->

```{r sov_integrand_dens, echo = FALSE}
xs <- seq(.Machine$double.eps, 1 - .Machine$double.eps, length.out = 100)
ys <- xs
xy <- expand.grid(y = ys, x = xs)
S <- t(chol(Sigma))
bar_b <- function(ys){
  n_ele <- length(ys)
  if(n_ele < 1L)
    return(b[1L] / S[1L, 1L])
  idx <- n_ele + 1L
  drop(b[idx] - S[idx, 1:n_ele] %*% ys) / S[idx, idx]
}
integrand <- mapply(function(x, y){
  zs <- numeric(2L)
  ys <- numeric(2L)
  bs <- numeric(2L)

  bs[1L] <- pnorm(bar_b(numeric()))
  out <- bs[1L]
  zs[1L] <- bs[1L] * x
  ys[1L] <- qnorm(zs[1L])
  
  bs[2L] <- pnorm(bar_b(ys[1L]))
  out <- out * bs[2L]
  zs[2L] <- bs[2L] * y
  ys[2L] <- qnorm(zs[2L])
  
  pnorm(bar_b(ys)) * out
}, x = xy$x, y = xy$y)
integrand <- integrand / max(integrand)

par(mar = c(4.5, 4.5, 1, 1))
filled.contour(x = xs, y = ys, matrix(integrand, length(xs)), 
               xlab = expression(w[1]), ylab = expression(w[2]), col = cl, 
               frame.plot = FALSE)
```

<!--html_preserve-->
</section>
<section class="center-horiz">
<h2>Example with Permutated Integrand</h2>
<!--/html_preserve-->

<div class = "w-small">

```{r permu_integrand_dens, echo = FALSE}
new_ord <- c(1L, 3L, 2L)
Sigma <- Sigma[new_ord, new_ord]
b <- b[new_ord]

S <- t(chol(Sigma))
integrand <- mapply(function(x, y){
  zs <- numeric(2L)
  ys <- numeric(2L)
  bs <- numeric(2L)

  bs[1L] <- pnorm(bar_b(numeric()))
  out <- bs[1L]
  ys[1L] <- qnorm(bs[1L] * x)
  
  bs[2L] <- pnorm(bar_b(ys[1L]))
  out <- out * bs[2L]
  ys[2L] <- qnorm(bs[2L] * y)
  
  pnorm(bar_b(ys)) * out
}, x = xy$x, y = xy$y)
integrand <- integrand / max(integrand)

par(mar = c(4.5, 4.5, 1, 1))
filled.contour(x = xs, y = ys, matrix(integrand, length(xs)), 
               xlab = expression(w[1]), ylab = expression(w[2]), col = cl, 
               frame.plot = FALSE)
```

<p class="smallish">
Notice the legend.</p>
</div>

## Variable Re-ordering
A heuristic variable re-ordering can reduce the variance integrand.

## Quasi-random Numbers
@Genz02 use randomized Korobov rules.

<p class = "fragment">
Yields an error which is $\mathcal O (s^{-1}(\log s)^h)$ instead 
of $\mathcal O(s^{-1/2})$ where $s$ is the number samples.</p>

## Derivatives and Imputation
Derivatives w.r.t. $\vec \mu$ and $\mat\Sigma$ and mean imputation 
requires:

$$
\int_{a_1}^{b_1}\cdots\int_{a_h}^{b_h}
  \vec h(\vec z, \vec \mu, \mat\Sigma)\phi^{(h)}(\vec z; \vec\mu,\mat\Sigma)\der\vec z
$$

for some function $\vec h$. 

<p class = "fragment">
If $\vec g:\, (0,1)^h\rightarrow \mathbb R^h$ is the map from $\vec w$ 
to $\vec z$ then using a transformation and permutation like before is 
beneficial if $\vec h(\vec g(\vec w), \vec\mu, \mat\Sigma)$ is not too 
variable.</p>

## The Implementation 
Extends the Fortran code By @Genz02. 

<div class = "w-small fragment">
Allows for computation in parallel.
<p class="smallish">
The computation is compute-bound so this is very beneficial.</p>
</div>

## Examples of Integral Approximations

```{r int_aprx}
nrow(dat_obs) # number of observations

# approximate the log marginal likelihood
mdgc_obj <- get_mdgc(dat_obs)
comp_ptr <- get_mdgc_log_ml(mdgc_obj)
mdgc_log_ml(comp_ptr, vcov = dat$Sigma, n_threads = 4L, rel_eps = 1e-3)
```

## Examples of Integral Approximations

Median running times:

```{r show_comp_bound, echo = FALSE, cache = 1}
out <- mark(
  `1 thread ` = mdgc_log_ml(comp_ptr, vcov = dat$Sigma, n_threads = 1L, 
                            rel_eps = 1e-3),
  `2 threads` = mdgc_log_ml(comp_ptr, vcov = dat$Sigma, n_threads = 2L,
                            rel_eps = 1e-3),
  `3 threads` = mdgc_log_ml(comp_ptr, vcov = dat$Sigma, n_threads = 3L,
                            rel_eps = 1e-3),
  `4 threads` = mdgc_log_ml(comp_ptr, vcov = dat$Sigma, n_threads = 4L,
                            rel_eps = 1e-3),
  check = FALSE, min_time = 5)

knitr::kable(cbind(` ` = attr(out$expression,"description"), out[, c("median")]))
```

<!--html_preserve-->
</section>
<!-- need extra end tag before next section -->
</section>


<section>
<section class="large-first center slide level2">
<h1>Simulations</h1>
<!--/html_preserve-->

## Performing Estimation and Imputation

Estimation using stochastic gradient methods. 

<p class = "fragment">
ADAM and SVRG is implemented.</p>

## Performing Estimation and Imputation

```{r est_mod, cache = 1}
# estimate the model and perform the imputation
set.seed(1)
system.time(
  res <- mdgc(dat_obs, lr = 1e-3, maxit = 25L, maxpts = 5000L, 
              conv_crit = 1e-6, n_threads = 4L, batch_size = 100L))
```

## Imputations

```{r show_est_mod}
head(res$ximp, 3)
head(dat$truth_obs, 3) # true data
```

<!--html_preserve-->
</section>
<section class="center-horiz" data-transition="slide-in fade-out">
<h2>Est. Correlation Matrix</h2>
<!--/html_preserve-->

```{r vcov_est_mod, echo = FALSE, fig.height = 3.5}
do_plot <- function(est, truth, main){
  par_old <- par(mfcol = c(1, 3), mar  = c(1, 1, 4, 1))
  on.exit(par(par_old))
  sc <- colorRampPalette(c("Red", "White", "Blue"))(201)
  
  f <- function(x, main)
    image(x[, NCOL(x):1], main = main, col = sc, zlim = c(-1, 1), 
          xaxt = "n", yaxt = "n", bty = "n")
  f(est, main)
  f(truth, "Truth")
  f(est - truth, "Difference")
}

do_plot(res$vcov, dat$Sigma, "Estimates") 
```

<!--html_preserve-->
</section>
<section class="center-horiz" data-transition="fade-in slide-out">
<h2>Est. Correlation Matrix Zhao et al.</h2>
<!--/html_preserve-->

```{r zhao_ets_vcov, echo = FALSE, cache = 1, fig.height = 3.5}
dat_pass <- dat$seen_obs
is_cat <- sapply(dat_pass, function(x) is.logical(x) | is.ordered(x))
dat_pass[, is_cat] <- lapply(dat_pass[, is_cat], as.integer)
mixedgc_res <- impute_mixedgc(dat_pass, eps = 1e-4)
do_plot(mixedgc_res$R, dat$Sigma, "Estimates (aprx. EM)")
```

## Simulation Study

```{r def_util_funcs, echo = FALSE}
get_classif_error <- function(impu_dat, truth = dat$truth_obs, 
                              observed = dat$seen_obs){
  is_cat <- sapply(truth, function(x)
    is.logical(x) || is.ordered(x))
  is_match <- impu_dat[, is_cat] == truth[, is_cat]
  is_match[!is.na(observed[, is_cat])] <- NA_integer_
  1 - colMeans(is_match, na.rm = TRUE)
}
get_rmse <- function(impu_dat, truth = dat$truth_obs,
                     observed = dat$seen_obs){
  is_con <- sapply(truth, is.numeric)
  err <- as.matrix(impu_dat[, is_con] - truth[, is_con])
  err[!is.na(observed[, is_con])] <- NA_real_
  sqrt(colMeans(err^2, na.rm = TRUE))
}
get_rel_err <- function(est, keep = seq_len(NROW(truth)), truth = dat$Sigma)
  norm(truth[keep, keep] - est[keep, keep], "F") / 
  norm(truth, "F")
threshold <- function(org_data, imputed){
  # checks
  stopifnot(NROW(org_data) == length(imputed), 
            is.list(imputed), is.data.frame(org_data))
  
  # threshold
  is_cont <- which(sapply(org_data, is.numeric))
  is_bin  <- which(sapply(org_data, is.logical)) 
  is_ord  <- which(sapply(org_data, is.ordered))
  stopifnot(
    length(is_cont) + length(is_bin) + length(is_ord) == NCOL(org_data))
  is_cat <- c(is_bin, is_ord)
  
  trans_to_df <- function(x){
    if(is.matrix(x))
      as.data.frame(t(x))
    else
      as.data.frame(  x )
  }
  
  out_cont <- trans_to_df(sapply(imputed, function(x) unlist(x[is_cont])))
  out_cat <- trans_to_df(sapply(imputed, function(x) 
    sapply(x[is_cat], which.max)))
  out <- cbind(out_cont, out_cat)
  
  # set factor levels etc. 
  out <- out[, order(c(is_cont, is_bin, is_ord))]
  if(length(is_bin) > 0)
    out[, is_bin] <- out[, is_bin] > 1L
  if(length(is_ord) > 0)
    for(i in is_ord){
      out[[i]] <- ordered(
        unlist(out[[i]]), labels = levels(org_data[, i]))
    }
  
  colnames(out) <- colnames(org_data)
  out
}
```

<div class = "w-small">

```{r sim_study, message = FALSE, echo = FALSE}
# the seeds we will use
seeds <- c(293498804L, 311878062L, 370718465L, 577520465L, 336825034L, 661670751L, 750947065L, 255824398L, 281823005L, 721186455L, 251974931L, 643568686L, 273097364L, 328663824L, 490259480L, 517126385L, 651705963L, 43381670L, 503505882L, 609251792L, 643001919L, 244401725L, 983414550L, 850590318L, 714971434L, 469416928L, 237089923L, 131313381L, 689679752L, 344399119L, 330840537L, 6287534L, 735760574L, 477353355L, 579527946L, 83409653L, 710142087L, 830103443L, 94094987L, 422058348L, 117889526L, 259750108L, 180244429L, 762680168L, 112163383L, 10802048L, 440434442L, 747282444L, 736836365L, 837303896L, 50697895L, 231661028L, 872653438L, 297024405L, 719108161L, 201103881L, 485890767L, 852715172L, 542126886L, 155221223L, 18987375L, 203133067L, 460377933L, 949381283L, 589083178L, 820719063L, 543339683L, 154667703L, 480316186L, 310795921L, 287317945L, 30587393L, 381290126L, 178269809L, 939854883L, 660119506L, 825302990L, 764135140L, 433746745L, 173637986L, 100446967L, 333304121L, 225525537L, 443031789L, 587486506L, 245392609L, 469144801L, 44073812L, 462948652L, 226692940L, 165285895L, 546908869L, 550076645L, 872290900L, 452044364L, 620131127L, 600097817L, 787537854L, 15915195L, 64220696L)

# gather or compute the results
run_sim_study <- function(n, p, s = seeds) lapply(s, function(s){
  file_name <- 
    if(p == 9L)
      file.path("sim-res", sprintf("seed-%d.RDS", s))
    else if(p == 60L)
      file.path("sim-res", sprintf("seed-large-%d.RDS", s))
    else 
      stop("Invalid p")
  
  if(file.exists(file_name)){
    message(sprintf("Reading '%s'", file_name))
    out <- readRDS(file_name)
  } else { 
    message(sprintf("Running '%s'", file_name))
    
    # simulate the data
    set.seed(s)
    dat <- sim_dat(n = n, p = p)
    
    # fit models and impute
    mdgc_time <- system.time(
      mdgc_res <- mdgc(
        dat$seen_obs, lr = 1e-3, maxit = 25L, maxpts = 5000L, 
              conv_crit = 1e-6, n_threads = 4L, batch_size = 100L))
    
    dat_pass <- dat$seen_obs
    is_cat <- sapply(dat_pass, function(x) is.logical(x) | is.ordered(x))
    dat_pass[, is_cat] <- lapply(dat_pass[, is_cat], as.integer)
    mixedgc_time <- 
      system.time(mixedgc_res <- impute_mixedgc(dat_pass, eps = 1e-4))
    
    if(p <= 9L){
      miss_forest_arg <- dat$seen_obs
      is_log <- sapply(miss_forest_arg, is.logical)
      miss_forest_arg[, is_log] <- lapply(
        miss_forest_arg[, is_log], as.factor)
      sink(tempfile())
      miss_time <- system.time(
        miss_res <- missForest(miss_forest_arg, verbose = FALSE))
      sink()
      
      miss_res$ximp[, is_log] <- lapply(
        miss_res$ximp[, is_log], function(x) as.integer(x) > 1L)
      
    } else {
      miss_time <- rep(NA_real_, 3L)
      miss_res <- NA
      
    }
    
    # impute using the other estimate
    mdgc_obj <- get_mdgc(dat$seen_obs)
    impu_mixedgc_est <- mdgc_impute(mdgc_obj, mixedgc_res$R, n_threads = 4L)
    impu_mixedgc_est <- threshold(dat$seen_obs, impu_mixedgc_est)
    
    # gather output for the correlation matrix estimates
    vcov_res <- list(truth = dat$Sigma, mdgc = mdgc_res$vcov, 
                     mixedgc = mixedgc_res$R)
    get_rel_err <- function(est, truth, keep = seq_len(NROW(truth)))
      norm(truth[keep, keep] - est[keep, keep], "F") / norm(truth, "F")
    
    vcov_res <- within(vcov_res, {
      mdgc_rel_err    = get_rel_err(mdgc   , truth)
      mixedgc_rel_err = get_rel_err(mixedgc, truth)
    })
    
    # gather output for the imputation 
    mixedgc_imp_res <- as.data.frame(mixedgc_res$Ximp)
    is_bin <- sapply(dat$seen_obs, is.logical)
    mixedgc_imp_res[, is_bin] <- 
      lapply(mixedgc_imp_res[, is_bin, drop = FALSE], `>`, e2 = 0)
    is_ord <- sapply(dat$seen_obs, is.ordered)
    mixedgc_imp_res[, is_ord] <- mapply(function(x, idx)
      ordered(x, labels = levels(dat$seen_obs[[idx]])), 
      x = mixedgc_imp_res[, is_ord, drop = FALSE], 
      i = which(is_ord), SIMPLIFY = FALSE)
    
    get_bin_err <- function(x){
      . <- function(z) z[, is_bin, drop = FALSE]
      get_classif_error(
        .(x), truth = .(dat$truth_obs), observed = .(dat$seen_obs))
    }
    get_ord_err <- function(x){
      . <- function(z) z[, is_ord, drop = FALSE]
      get_classif_error(
        .(x), truth = .(dat$truth_obs), observed = .(dat$seen_obs))
    }
          
    
    no_missForest <- length(miss_res) == 1L && is.na(miss_res)
    err <- list(
      mdgc_bin = get_bin_err(mdgc_res$ximp), 
      mixedgc_bin = get_bin_err(mixedgc_imp_res), 
      mixed_bin = get_bin_err(impu_mixedgc_est),
      missForest_bin = if(no_missForest)
        NULL else get_bin_err(miss_res$ximp),
      
      mdgc_class = get_ord_err(mdgc_res$ximp), 
      mixedgc_class = get_ord_err(mixedgc_imp_res), 
      mixed_class = get_ord_err(impu_mixedgc_est),
      missForest_class = if(no_missForest)
        NULL else get_ord_err(miss_res$ximp),
      
      mdgc_rmse = get_rmse(
        mdgc_res$ximp, truth = dat$truth_obs, observed = dat$seen_obs),
      mixedgc_rmse = get_rmse(
        mixedgc_imp_res, truth = dat$truth_obs, observed = dat$seen_obs),
      mixed_rmse = get_rmse(
        impu_mixedgc_est, truth = dat$truth_obs, observed = dat$seen_obs), 
      missForest_rmse = if(no_missForest)
        NULL else get_rmse(
          miss_res$ximp, truth = dat$truth_obs, observed = dat$seen_obs))
    
    # gather the times
    times <- list(mdgc = mdgc_time, mixedgc = mixedgc_time, 
                  missForest = miss_time)
    
    # save stats to check convergence
    conv_stats <- list(mdgc = mdgc_res$logLik, 
                       mixedgc = mixedgc_res$loglik)
    
    # save output 
    out <- list(vcov_res = vcov_res, err = err, times = times, 
                conv_stats = conv_stats)
    saveRDS(out, file_name)
  }
  
  # print summary stat to the console while knitting
  . <- function(x)
    if(!is.null(x))
      message(paste(sprintf("%8.3f", x), collapse = " "))
  mea <- function(x, ...)
    if(!is.null(x))
      base::mean(x) else NA_real_
  
  with(out, {
    message(paste(
      "mdgc    logLik", 
      paste(sprintf("%.2f", conv_stats$mdgc), collapse = " ")))
    message(paste(
      "mixedgc logLik", 
      paste(sprintf("%.2f", conv_stats$mixedgc), collapse = " ")))
    message(sprintf(
      "Relative correlation matrix estimate errors are %.4f %.4f", 
      vcov_res$mdgc_rel_err, vcov_res$mixedgc_rel_err))
    message(sprintf(
      "Times are %.2f %.2f %.2f", 
      times$mdgc["elapsed"], times$mixedgc["elapsed"], 
      times$missForest["elapsed"]))
    
    message(sprintf(
      "Binary classification errors are %.2f %.2f %.2f (%.2f)", 
      mea(err$mdgc_bin), mea(err$mixedgc_bin), 
      mea(err$missForest_bin), mea(err$mixed_bin)))
    # .(err$mdgc_bin)
    # .(err$mixedgc_bin)
    # .(err$mixed_bin)
    
    message(sprintf(
      "Ordinal classification errors are %.2f %.2f %.2f (%.2f)", 
      mea(err$mdgc_class), mea(err$mixedgc_class), 
      mea(err$missForest_class), mea(err$mixed_class)))
    # .(err$mdgc_class)
    # .(err$mixedgc_class)
    # .(err$mixed_class)
    
    message(sprintf(
      "Mean RMSEs are %.2f %.2f %.2f (%.2f)",
      mea(err$mdgc_rmse), mea(err$mixedgc_rmse), 
      mea(err$missForest_rmse), mea(err$mixed_rmse)))
    .(err$mdgc_rmse)
    .(err$mixedgc_rmse)
    .(err$mixed_rmse)
    message("")
  })
  
  out  
})

res <- run_sim_study(2000L, p = 9L)

# summarize the results
get_sim_stats <- function(v1, v2, v3, what, sub_ele = NULL){
  vals <- sapply(res, function(x) 
    do.call(rbind, x[[what]][c(v1, v2, v3)]), 
    simplify = "array")
  if(!is.null(sub_ele))
    vals <- vals[, sub_ele, , drop = FALSE]
    
  # Means and standard errors
  mea_se <- function(x)
    c(mean = mean(x), SE = sd(x) / sqrt(length(x)))
  r1 <- t(apply(vals, 1L, mea_se))
  
  # Differences
  r2 <- t(apply(
    c(vals[v1, , ]) - 
      aperm(vals[c(v2, v3), , , drop = FALSE], c(3L, 2L, 1L)), 
    3L, mea_se))
  r2 <- rbind(NA_real_, r2)
  
  list(stats = r1, delta = r2)
}

tis <- get_sim_stats(1L, 2L, 3L, "times", "elapsed")
rel_nom <- get_sim_stats("mdgc_rel_err", "mixedgc_rel_err", NULL, "vcov_res")
rel_nom$stats <- rbind(rel_nom$stats, NA_real_)
rel_nom$delta <- rbind(rel_nom$delta, NA_real_)
err_bin <- get_sim_stats("mdgc_bin", "mixedgc_bin", "missForest_bin", "err")
err_ord <- get_sim_stats("mdgc_class", "mixedgc_class", "missForest_class", "err")
err_con <- get_sim_stats("mdgc_rmse", "mixedgc_rmse", "missForest_rmse", "err")

to_tab <- function(x, name, digits){
  stats <- round(x$stats, digits = digits)
  colnames(stats) <- c(name, "SE")
  delta <- round(x$delta, digits = digits)
  colnames(delta) <- c(sprintf("%s (Δ)", name), "SE") 
  cbind(stats, delta)
}

tab <- cbind(to_tab(tis, "Time", 1L), 
             to_tab(rel_nom, "Est. error", 3L))
tab_form <- apply(tab, 2L, format, digits = getOption("digits"))
tab_form[is.na(tab)] <- ""
rownames(tab_form) <- c("mdgc", "mixedgc", "missForest")
knitr::kable(tab_form)
```

<p class="smallish">
mdgc is the presented method, mixedgc is the method suggested by @zhao19, 
and missForest is the method suggested by @Stekhoven12 [@Stekhoven13].
`r length(res)` samples are used in the study.</p>

<p class="smallish">
The "Est. error" is
$\lVert \widehat{\mat C} - \mat C\rVert \big/\lVert\mat C\rVert$.</p>

</div>

## Simulation Study

<div class = "w-small">

```{r cont_sim_study, echo = FALSE}
tab <- cbind(to_tab(err_bin, "Bin. ", 3L), 
             to_tab(err_ord, "Ord.", 3L))
tab_form <- apply(tab, 2L, format, digits = getOption("digits"))
tab_form[is.na(tab)] <- ""
rownames(tab_form) <- c("mdgc", "mixedgc", "missForest")
knitr::kable(tab_form)
```

```{r rmse_sim_study, echo = FALSE}
tab <- to_tab(err_con, "Con. ", 3L)
tab_form <- apply(tab, 2L, format, digits = getOption("digits"))
tab_form[is.na(tab)] <- ""
rownames(tab_form) <- c("mdgc", "mixedgc", "missForest")
knitr::kable(tab_form)
```

<p class="smallish">
Bin. is binary classification error, ord. is ordinal classification error, 
and con. is continuous' variables RMSE.</p>

</div>

## More Samples

<div class = "w-small">

```{r run_large, message = FALSE, warning = FALSE, echo = FALSE}
n_large <- 10000L
p_large <- 60L
res <- run_sim_study(n_large, p = p_large, s = head(seeds, 10L))

tis <- get_sim_stats(1L, 2L, NULL, "times", "elapsed")
rel_nom <- get_sim_stats("mdgc_rel_err", "mixedgc_rel_err", NULL, "vcov_res")
err_bin <- get_sim_stats("mdgc_bin", "mixedgc_bin", NULL, "err")
err_ord <- get_sim_stats("mdgc_class", "mixedgc_class", NULL, "err")
err_con <- get_sim_stats("mdgc_rmse", "mixedgc_rmse", NULL, "err")

tab <- cbind(to_tab(tis, "Time", 1L), 
             to_tab(rel_nom, "Est. error", 3L))
tab_form <- apply(tab, 2L, format, digits = getOption("digits"))
tab_form[is.na(tab)] <- ""
rownames(tab_form) <- c("mdgc", "mixedgc")
knitr::kable(tab_form)
```

<p class="smallish">
Larger problem with $p = `r p_large`$ and $n = `r n_large`$. 
`r length(res)` samples are used.</p>

</div>

## More Samples

```{r cont_run_large, echo = FALSE}
tab <- cbind(to_tab(err_bin, "Bin. ", 3L), 
             to_tab(err_ord, "Ord.", 3L))
tab_form <- apply(tab, 2L, format, digits = getOption("digits"))
tab_form[is.na(tab)] <- ""
rownames(tab_form) <- c("mdgc", "mixedgc")
knitr::kable(tab_form)
```

```{r rmse_run_large, echo = FALSE}
tab <- to_tab(err_con, "Con. ", 3L)
tab_form <- apply(tab, 2L, format, digits = getOption("digits"))
tab_form[is.na(tab)] <- ""
rownames(tab_form) <- c("mdgc", "mixedgc")
knitr::kable(tab_form)
```

<!--html_preserve-->
</section>
<!-- need extra end tag before next section -->
</section>



<section>
<section class="large-first center slide level2">
<h1>Conclusion</h1>
<!--/html_preserve-->

## Summary
Introduced Gaussian copulas method to impute missing values. 

<p class = "fragment">
Covered quasi-random numbers method to approximate the log marginal 
likelihood and to perform imputations.</p>

<p class = "fragment">
Showed that the method is efficient and fast even for larger problems
with $p = 60$ variables.</p>

## Extensions
Like @zhao20, we can use a low rank approximation:

$$
\mat C = \mat W\mat W^\top + \sigma^2\mat I
$$

where $\mat W\in\mathbb{R}^{p\times k}$ with $k < p$. 

<p class = "fragment">
Easy to add prediction intervals. Probabilities of each 
level is already provided.</p>

## Extensions
$x_{ij}$ is multinomial with $k_j$-categories.

Suppose

$$
\begin{align*}
\vec A_{ij} &\sim N^{(k_j)}(\vec \mu_j, 2^{-1}\mat I) \\
x_{ij} &= l\Leftrightarrow A_{ijl} > A_{ijl'} \quad \forall l'\neq l  
\end{align*}
$$

$\vec\mu_j$ is given by the marginal distribution. 


## Identity

Let 

$$
\Phi^{(k)}(\vec a, \vec b;\vec\mu,\mat\Sigma) = 
  \int_{a_1}^{b_1}\cdots\int_{a_k}^{b_k}
  \phi^{(k)}(\vec x;\vec\mu,\mat\Sigma)\der\vec x
$$

Then

$$
\begin{align*}
\int\phi^{(k_1)}(\vec z_1; \vec\mu_1, \mat\Sigma_1)
  \Phi^{(k_2)}(\vec a + \mat K\vec z_1, \vec b + \mat K\vec z_1; 
  \vec\mu_2,\mat\Sigma_2)\der\vec z_1 & \\
&\hspace{-200pt}=
  \Phi^{(k_2)}(\vec a, \vec b; \vec\mu_2 - \mat K\vec\mu_1, 
  \mat\Sigma_2 + \mat K\mat\Sigma_1\mat K^\top)
\end{align*}
$$

## Marginal Distribution

$$
\begin{align*}
\Prob(X_{ij} = l) &= \int \phi(a;\mu_{jl},2^{-1})
  \Phi^{(k_j - 1)}(-\vec\infty, \vec 1a;\vec\mu_{j(-l)}, 2^{-1}\mat I)\der a \\
&= \Phi^{(k_j - 1)}(-\vec\infty, \vec 1\mu_{jl} - \vec\mu_{j(-l)}; \vec 0, 
  2^{-1}\mat I + 2^{-1}\vec 1\vec 1 ^\top)
\end{align*}
$$

where $\vec\mu_{j(-l)} = (\mu_{j1}, \dots, \mu_{j,l - 1}, \mu_{j,l + 1}, \dots \mu_{jk_j})$. 

## Marginal Distribution (2D)
$$
\begin{align*}
\Prob(X_{ij} = 1) &= \Phi(\mu_{j1} - \mu_{j2}; 0, 1) = \Phi(\mu_{j1} - \mu_{j2}) \\
\Prob(X_{ij} = 2) &= \Phi(\mu_{j2} - \mu_{j1}; 0, 1) = \Phi(\mu_{j2} - \mu_{j1})
\end{align*}
$$

<div class = "fragment">
Select $\mu_{j2} = 0$ for identifiability:

$$
\begin{align*}
\Prob(X_{ij} = 1) &= \Phi( \mu_{j1}) \\
\Prob(X_{ij} = 2) &= \Phi(-\mu_{j1})
\end{align*}
$$

I.e. a binary variable with a border at $0 < \Phi(\mu_{j1}) < 1$.
</div>

## Using the Identity 

Suppose $x_{ij} = l$ and write:

$$
\vec V_i = (A_{ijl}, \vec A_{ij(-l)}^\top, \vec z_{i\mathcal O_i}^\top)^\top
$$

where $\vec z_{i\mathcal O_i}$ are the remaining variables (ordinal/binary). 

<div class = "fragment">
Denote:

$$
\vec V_i \sim N^{(h)}\left(
  \begin{pmatrix} \mu_{g} \\ \vec\mu_{(-g)}\end{pmatrix},
  \begin{pmatrix} \sigma_{gg} & \vec\sigma_{g(-g)} \\ 
                  \vec\sigma_{(-g)g} & \mat\Sigma_{(-g)(-g)}\end{pmatrix}
\right)
$$
</div>

## Using the Identity 
The marginal likelihood factor is:

$$
\begin{align*}
\int \phi(z;\mu_g;\sigma_{gg}^2) \Phi^{(h - 1)}\bigg(&\vec a_{(-g)} + \vec dz - \mat K(z - \mu_g), \vec b_{(-g)} + \vec dz - \mat K(z - \mu_g); \\
& \vec\mu_{(-g)}, \mat\Sigma_{(-g)(-g)} - \frac{\vec\sigma_{(-g)g}\vec\sigma_{(-g)g}^\top}{\sigma_{gg}}\bigg)\der z \\
\end{align*}
$$

with $\mat K = \vec\sigma_{(-g)g}/\sigma_{gg}$ and 

$$\vec d = (\underbrace{\,\,\,\vec 1^\top\,\,\,}_{k_j - 1\text{ times}}, \vec 0^\top)^\top$$

## Using the Identity 
Thus, we get:

$$
\Phi^{(h - 1)}\bigg(\vec a_{(-g)}, \vec b_{(-g)}; 
        \vec\mu_{(-g)} - \vec d\mu_g, \mat\Sigma_{(-g)(-g)} + \vec d\vec d^\top\sigma_{gg}\bigg)
$$

<p class = "fragment">
Easy to extend to multiple multinomial variables.</p>

## Extending 
We have $G$ categorical variables. The latent variables for the 
$l = 1,\dots, G$ categorical variable
is at indices $r_l + 1, \dots, r_l + k_l$. 

<p class = "fragment">
Suppose that we observe the category for the $l$th categorical variable
which latent variable is at index $c_l$ and let 
$\mathcal C = \{c_1,\dots, c_G\}$.</p>

## Extending 
Let $\mat D = (\vec d_1, \dots \vec d_G)_{(\mathcal -C)\cdot}$,
the matrix which columns are $\vec d_1, \dots, \vec d_G$ without the rows 
in $\mathcal C$. 

<p class = "fragment">
Define a vector 
$\vec d_l = (\vec 0^\top, \underbrace{\,\,\,\vec 1^\top\,\,\,}_{k_l\text{ times}}, \vec 0^\top)^\top$
which has ones at indices $r_l + 1, \dots, r_l + k_l$.</p>

## Extending 
Then intractable integral is:

$$
\begin{align*}
\int \phi^{(G)}(\vec z;\vec \mu_{\mathcal C};\mat\Sigma_{\mathcal C\mathcal C}) 
  \Phi^{(h - G)}\bigg(\hspace{-75pt}& \\
& \vec a_{(-\mathcal C)} + \mat D\vec z - 
  \mat K(\vec z - \vec\mu_{\mathcal C}), \vec b_{(-\mathcal C)} + 
  \mat D\vec z - \mat K(\vec z - \vec \mu_{\mathcal C}); \\
& \vec\mu_{(-\mathcal C)}, \mat\Sigma_{(-\mathcal C)(-\mathcal C)} - 
  \mat\Sigma_{(-\mathcal C)\mathcal C}\mat\Sigma_{\mathcal C\mathcal C}^{-1}\mat\Sigma_{\mathcal C(-\mathcal C)}\bigg)
  \der \vec z \\
\end{align*}
$$

with $\mat K = \mat\Sigma_{(-\mathcal C)\mathcal C}\mat\Sigma_{\mathcal C\mathcal C}^{-1}$.


## Extending 
Applying the identity yields:

$$
\Phi^{(h - G)}\bigg(\vec a_{(-\mathcal C)}, \vec b_{(-\mathcal C)}; 
        \vec\mu_{(-\mathcal C)} - \mat D\vec\mu_{\mathcal C}, 
        \mat\Sigma_{(-\mathcal C)(-\mathcal C)} + 
        \mat D\mat\Sigma_{\mathcal C\mathcal C}\mat D^\top)
$$




<!--html_preserve-->
</section>
<!-- need extra end tag before next section -->
</section>


<section>
<section class="center final">
<h1>Thank You!</h1>

<div class="w-small">
<p class="smallish">The presentation is at  
<a href="https://rpubs.com/boennecd/Gaussian-copula-KTH">rpubs.com/boennecd/Gaussian-copula-KTH</a>.</p>
<p class="smallish">The markdown is at  
<a href="https://github.com/boennecd/Talks">github.com/boennecd/Talks</a>.</p>
<p class="smallish">Code and more examples at  
<a href="https://github.com/boennecd/mdgc">github.com/boennecd/mdgc</a>.</p>
<p class="smallish">References are on the next slide.</p>
</div>

</section>
<!-- need extra end tag before next section -->
</section>


<section>
<h1>References</h1>

<!--/html_preserve-->