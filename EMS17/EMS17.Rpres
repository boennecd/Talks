dynamichazard Dynamic Hazard Models using State Space Models
========================================================
author: Benjamin Christoffersen
date: 28/07/2017
css: custom.css
autosize: false
width:1200
height:800

<!-- Two line header + add affiliation -->
<script>
var h1s = document.getElementsByTagName("h1");
h1s[0].outerHTML = "<h1 style='margin-bottom: 0;'>dynamichazard</h1><h2 style='margin-top: 0;'>Dynamic Hazard Models <br /> using State Space Models</h2>";

// IE11 does not support xpath
//var ptag = document.evaluate("//p[contains(text(),'Benjamin Christoffersen')]", 
//  document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null).singleNodeValue;
var ptag = document.getElementsByTagName("p")[0];
ptag.innerHTML = ptag.innerHTML.replace(
  "Christoffersen",
  "Christoffersen<br/>Copenhagen Business School<br/>Center for Statistics");
</script>  

<!-- See this link for macros with mathjax: http://docs.mathjax.org/en/latest/tex.html#tex-macros -->
<script>
MathJax.Hub.Config({
  // See http://docs.mathjax.org/en/latest/configuration.html and http://docs.mathjax.org/en/latest/output.html

  jax: ["input/TeX","output/SVG"],
  
  // See https://stackoverflow.com/a/25329062/5861244
  "HTML-CSS": { scale: 80 },

  TeX: {
    Macros: {
      mat: ["\\mathbf{#1}", 1],
      
      Lparen: ["\\left( #1\\right)", 1],
      Lbrace: ["\\left\\{ #1\\right\\}", 1],
      
      Cond: ["\\left. #1 \\vphantom{#2} \\right\\vert #2", 2],
      
      emNotee: ["#1_{\\left. #2 \\right\\vert #3}", 3],
      
      Prob: ["\\text{P}"],
      propp: ["\\Prob\\Lparen{#1}", 1],
      proppCond: ["\\propp{\\Cond{#1}{#2}}", 2],
      
      E: ["\\text{E}"],
      expecp: ["\\E\\Lparen{#1}", 1],
      expecpCond: ["\\expecp{\\Cond{#1}{#2}}", 2],
      
      deter: ["\\left| #1 \\right|", 1],
      
      argminu: ["\\underset{#1}{\\text{argmin}}\\:", 1]
    }
  }
});
</script>


```{r setup, include=FALSE, echo=FALSE}
######
# Citations
library(knitcitations)
bib <- read.bibtex("references.bib")
 
######
# Knitr hooks
with(new.env(), {
  knitr_par <- par(no.readonly = TRUE)

  par_default <- function(cex_mult = 1, ...){
    cex <- .8 * cex_mult

    list(
      mar = c(4, 4, 1, 1),
      bty = "L",
      xaxs = "i",
      pch=16,
      cex= cex,
      cex.axis = 1,
      cex.lab = 1,
      lwd= 1)
  }
 
  knitr::knit_hooks$set(
    par_1x1 =
      function(before, options, envir) {
        if(!options$par_1x1)
          return()

        if (before){
          par(par_default(1.66))
        }
    },
    
    par_2x1 =
      function(before, options, envir) {
        if(!options$par_2x1)
          return()

        if (before){
          par(mfcol = c(2, 1))
          tmp <- par_default(1.33)
          par(tmp)
        }
    },
    
    par_2x2 =
      function(before, options, envir) {
        if(!options$par_2x2)
          return()

        if (before){
          par(mfcol = c(2, 2))
          tmp <- par_default()
          tmp$mar <- tmp$mar + c(0.5, 0, 0, 0)
          par(tmp)
        }
    })
})

knitr::knit_hooks$set(
  default_par = function(before, options, envir) {
    if (before) 
      par(mar = c(4, 4, .1, .1), cex = 1.8, lwd = 2)
})

options(digits = 3, width = 60)

knitr::opts_chunk$set(
  dpi=144, fig.width = 14, default_par = TRUE,
  cache = TRUE,
  
  # See opts_hooks definition
  fig.height = -1, fig.width = 8)

knitr::opts_hooks$set(
  fig.height = function(options) {
    if(options$fig.height > 0)
      return(options)

    if ((!is.null(options$par_2x1) && options$par_2x1) |
        (!is.null(options$par_2x2) && options$par_2x2)){
      options$fig.height <- 5
    } else
      options$fig.height <- 4

    options
  })

##### 
# Other
print.data.frame <- print.data.frame
formals(print.data.frame)$row.names <- FALSE
```

Presentation
========================================================
incremental: true

* Model
* Hard disk failures
* Estimation methods
* End comments

Presentation: Focus
========================================================
incremental: true

Survival analysis with time-varying coefficients

Continuous time model

Fast and scalable


Model
========================================================
incremental: true

Individual $i = 1,\dots,n$

Failure times $T_1,\dots,T_n\in \mathbb{R}_+$

Timer intervals $t = 1, \dots, d$

Binary outcomes $y_{it} = \left\{ \begin{matrix} 1 & T_i \in (t-1, t] \\ 0 & \text{otherwise} \end{matrix}\right.$

Censoring indicators $D_{i1},\dots,D_{i2},\dots$ <small class="inline"> (one if censored)</small>

Covariates $\vec{x}_{i1}, \vec{x}_{i2}, \dots$

Model
========================================================

<div class="fragment">
<h2>Observational model</h2>

$$
\begin{array}{c}
\proppCond{Y_{it} = 1}{ \vec{\alpha}_t} =  h(\vec{\alpha}_t^\top \vec{x}_{it}) \\
h(\eta) = 1 / (1 + \exp(-\eta))
\end{array}
$$
</div>

<div class="fragment">
## State model

$$\begin{array}{ll}
  \vec{\alpha}_{t + 1} = \mat{F}\vec{\alpha}_t + \mat{R}\vec{\eta}_t \qquad &
  \vec{\eta}_t \sim N(\vec{0}, \mat{Q}) \\
  & \vec{\alpha}_{0} \sim N(\vec{a}_0, \mat{Q}_0)
\end{array} , \qquad t = 1,\dots, d$$

1. and 2. order random walk
<div>

Hard disk failures
========================================================
left: 50%

```{r load_hd_dat, echo = FALSE}
hd_dat <- readRDS("data/HDs.RDS")

# Few have data from time zero so we set a few days in as time zero
new_start <- 24 * 4
hd_dat$tstart <- pmax(new_start, hd_dat$tstart)
hd_dat$tstart <- hd_dat$tstart - new_start
hd_dat$tstop <- hd_dat$tstop - new_start

# We need to remove the records that ends before or at the starting time
# sum(hd_dat$tstart >= hd_dat$tstop) # Number of rows thrown away
hd_dat <- hd_dat[hd_dat$tstart < hd_dat$tstop, ]

hd_dat$serial_number <- droplevels(hd_dat$serial_number)

# Re-scale time
tmp <- 24 * 30
hd_dat$tstart  <- hd_dat$tstart / tmp
hd_dat$tstop <- hd_dat$tstop / tmp

# Make sure that data is sorted
hd_dat <- hd_dat[order(hd_dat$serial_number, hd_dat$tstart), ]

# Fill in blanks with carry the last observation forward
# Define function to fill in the blanks
# TODO: Check with BackBlaze if this is a good choice
library(zoo, quietly = T, warn.conflicts = F)

func <- function(x)
  na.locf0(c(0, x))[-1]
func <- compiler::cmpfun(func)

# Use the function
for(n in colnames(hd_dat)["smart_12" == colnames(hd_dat)]){
  hd_dat[[n]] <- unlist(
    tapply(hd_dat[[n]], as.integer(hd_dat$serial_number), func),
    use.names = F)
}

# We take those larger than a given size
n_per_model <-
  xtabs(~ model, hd_dat, subset = !duplicated(serial_number))
factor_cut <- 400
models_to_keep <- names(n_per_model)[n_per_model >= factor_cut]
hd_dat <- hd_dat[hd_dat$model %in% models_to_keep, ]
hd_dat$model <- droplevels(hd_dat$model)

# Order factor levels by the number of unique hard disks
n_per_model <-
  xtabs(~ model, hd_dat, subset = !duplicated(serial_number))

fac_ord <- order(n_per_model, decreasing = TRUE)
stopifnot(is.factor(hd_dat$model))
hd_dat$model <- factor(hd_dat$model, levels(hd_dat$model)[fac_ord])

n_per_model <-
  xtabs(~ model, hd_dat, subset = !duplicated(serial_number))

# Winsorize
win_lvl <- .99
hd_dat$smart_12 <- pmin(hd_dat$smart_12, quantile(
  hd_dat$smart_12, win_lvl))
```

<img class="icon" alt="BackBlaze logo" src="dl-figure/backblaze-logo.gif"/>

<p class="fragment">Storage provider</p>

<p class="fragment">More than 65000 hard disks Today</p>

<p class="fragment">3 years of data <small class="inline">(2013Q2‒2016Q3)</small></p>

<p class="fragment">`r length(unique(hd_dat$serial_number))` observations and `r nrow(hd_dat)` data rows</p>


***
<img class="dl-web" alt="Vault image" src="dl-figure/backblaze-b2-07-datacenter-aisle-half.jpg"/>

Hard disk failures
========================================================

```{r show_data}
hd_dat[
  1:10, c("serial_number", "model", "tstart", 
          "tstop", "fails", "smart_12")]
```

Hard disk failures
========================================================

```{r def_get_pretty, echo = FALSE}
library(stringr)
get_pretty_model_factors <- function(x){
  f <- function(lvls){
    lvls <- str_replace(lvls, "^model", "")
    lvls <- str_replace(lvls, "^[A-z]+\\ ", "")

    lvls
  }

  if(class(x) == "fahrmeier_94"){
    colnames(x$state_vecs) <- paste("Param.", f(colnames(x$state_vecs)))
    return(x)
  }

  f(x)
}
```


```{r hd_table, echo = FALSE, results="asis"}
#####
# Make table with failures and number of observations
tbl_dat <- by(
  hd_dat, hd_dat$model, function(x){
    c(length(unique(x$serial_number)), 
      sum(tapply(
        x$fails == 1, 
        droplevels(x$serial_number), 
        any)))
    })

tbl_dat <- do.call(rbind, tbl_dat)
colnames(tbl_dat) <- 
  c("Number of hard disks", "Number of failures")
rownames(tbl_dat) <- get_pretty_model_factors(
  rownames(tbl_dat))

##### 
# Format table and print
library(xtable)
x_tbl <- xtable(tbl_dat[1:6, ])
print(x_tbl, type = "html")
```

<small>Top 6 hard disk versions by number of disks</small>


Estimation methods: Log likelihood
========================================================
$$
L(\mat{Q},\mat{Q}_0, \vec{a}_0)
	= p(\vec{\alpha}_0)\prod_{t = 1}^d \proppCond{\vec{\alpha}_t}{\vec{\alpha}_{t-1}}
		\prod_{i \in R_t} \proppCond{y_{it}}{\vec{\alpha}_t}
$$
 
Risk set $R_t = \Lbrace{i \in \{1,\dots,n\}: D_{it} = 0}$


Estimation methods: Log likelihood
========================================================

$$
\begin{aligned}
	\log L \Lparen{\mat{Q},\mat{Q}_0, \vec{a}_0}
		= & - \frac{1}{2} \Lparen{\vec{\alpha}_0 - \vec{a}_0}^\top \mat{Q}^{-1}_0\Lparen{\vec{\alpha}_0 - \vec{a}_0} \\
	&  - \frac{1}{2} \sum_{t = 1}^d \Lparen{\vec{\alpha}_t - \mat{F}\vec{\alpha}_{t - 1}}^\top\mat{R}^\top\mat{Q}^{-1}\mat{R}\Lparen{\vec{\alpha}_t - \mat{F}\vec{\alpha}_{t - 1}} \\
	&  - \frac{1}{2} \log \deter{\mat{Q}_0} - \frac{1}{2d} \log \deter{\mat{Q}} \\
	&  + \sum_{t = 1}^d \sum_{i \in R_t} l_{it}({\vec{\alpha}_t}) + \dots
\end{aligned}
$$

Estimation methods: Log likelihood
========================================================

<div class="fragment">
<h2>E-step</h2>
Find smoothed estimates of $\vec{\alpha}_0,\dots, \vec{\alpha}_d$ and smoothed covariance matrices given $\mat{Q}$, $\mat{Q}_0$ and $\vec{a}_0$
</div>

<div class="fragment">
<h2>M-step</h2>

Update $\mat{Q}$ and $\vec{a}_0$
</div>

<div class="fragment">
<h2>References</h2>
<small>Due to `r citet(bib["fahrmeir92"])` and `r citet(bib["fahrmeir94"])`. Like `r citet(bib["Shumway82"])` </small>
</div>

Estimation methods: E-step
========================================================

<div class="fragment">
<h2>Filtering</h2>

<ul>
<li>Extended Kalman Filter (EKF)</li>
<li>Unscented Kalman filter</li>
<li>Sequential approximation of posterior modes</li>
<li>Mode estimation</li>
</ul>

</div>

<div class="fragment">

<h2>Smoothing</h2>
Rauch-Tung-Striebel algorithm

</div>

Estimation methods: Kalman Filter (KF)
========================================================

<div class="fragment">
$$
\emNotee{\vec{a}}{t}{s} = \expecpCond{\vec{\alpha}_t}{\vec{y}_1,\dots,\vec{y}_s},  \qquad
    \emNotee{\mat{V}}{t}{s} = \expecpCond{\mat{V}_t}{\vec{y}_1,\dots,\vec{y}_s}
$$
</div>

<div class="fragment">
<h2>Prediction step</h2>
Compute $\emNotee{\vec{a}}{t}{t - 1}$ &  $\emNotee{\mat{V}}{t}{t - 1}$ with $\emNotee{\vec{a}}{t - 1}{t - 1}$ and $\emNotee{\mat{V}}{t - 1}{t - 1}$
</div>

<div class="fragment">
<h2>Correction step</h2>
Compute $\emNotee{\vec{a}}{t}{t}$ &  $\emNotee{\mat{V}}{t}{t}$ with $\emNotee{\vec{a}}{t}{t-1}$,  $\emNotee{\mat{V}}{t}{t - 1}$ and $\vec{y}_t$
</div>

Estimation methods: Extended Kalman Filter
========================================================
incremental: true

Same prediction step as KF

Make 1. order Taylor expansion around $\emNotee{\vec{a}}{t}{t - 1}$

Same as one Fisher scoring step in:

$$
\begin{array}{c}
\emNotee{\vec{a}}{t}{t} = \argminu{\vec{\alpha}}
  - \log \proppCond{\vec{\alpha}}{\emNotee{\vec{a}}{t}{t-1}, \emNotee{\mat{V}}{t}{t-1}}
    -\sum_{i \in R_t} \log\proppCond{y_{it}}{\vec{\alpha}} \\
%
\vec{\alpha} \sim N\Lparen{\emNotee{\vec{a}}{t}{t-1}, \emNotee{\mat{V}}{t}{t-1}}
\end{array}
$$

Essentially L2 penalized generalized linear models

Estimation methods: mode approximation
========================================================
incremental: true

*Mode approximation:* Minimize directly using Newton Raphson 

*EKF with extra iteration:* More iterations using working responses as `glm`

Example: Only factors
========================================================

```{r factor model}
library(dynamichazard)

frm <- Surv(tstart, tstop, fails) ~ -1 + model

system.time(
  ddfit <- ddhazard(
    formula = frm, data = hd_dat,
    id = hd_dat$serial_number,
    Q_0 = diag(1, 17), Q = diag(.01, 17),
    by = 1, max_T = 60,
    control = list(
      method = "EKF",
      eps = .005,
      NR_eps = .00001)))
```

```{r change_fac_levels, echo = FALSE}
ddfit <- get_pretty_model_factors(ddfit)
```


```{r check_manual, echo = FALSE, eval = FALSE}
plot(ddfit, cov_index = 1:9)
plot(ddfit, cov_index = 10:17)
```

Example: Only factors
========================================================

```{r define_add_hist, echo = FALSE}
add_hist <- with(new.env(), {
  tmp_dat <- get_survival_case_weights_and_data(
    Surv(tstart, tstop, fails) ~ smart_12,
    data = hd_dat, by = 1, max_T = 60, use_weights = F,
    id = hd_dat$serial_number)

  # Find number of observations through time for each model
  n_obs <- xtabs(~ model + t, tmp_dat$X)
  
  # Find number of failures
  n_fails <- xtabs(~ model + t, tmp_dat$X, subset = which(tmp_dat$X$Y))

  xright <- as.numeric(colnames(n_obs))
  xleft <- xright - 1

  # Cleanup
  rm(tmp_dat)

  # Define function to plot histogram in background
  function(i){
    y_lim <- par("usr")[3:4]
    obs_dat <- n_obs[i, ]
    fail_dat <- n_fails[i, ]

    rect(
      xleft = xleft, xright = xright,
      ybottom = rep(y_lim[1], length(xright)),
      ytop <- (.9 * obs_dat) / max(obs_dat) * diff(y_lim) + y_lim[1],
      col = rgb(0, 0, 0, .1),
      border = rgb(1, 1, 1, 0.25),
      lwd = par()$lwd * 2)
    
    rect(
      xleft = xleft, xright = xright,
      ybottom = rep(y_lim[1], length(xright)),
      ytop <- (.9 * fail_dat) / max(obs_dat) * diff(y_lim) + y_lim[1],
      col = rgb(0, 0, 1, .33),
      border = rgb(1, 1, 1, 0.25),
      lwd = par()$lwd * 2)
    
    invisible(cbind(
      obs_dat = obs_dat, 
      fail_dat = fail_dat))
  }
})
```


```{r plot_only_factors, echo = FALSE, par_2x1 = TRUE}
for(i in c(1, 5)){
  plot(ddfit, cov_index = i)
  add_hist(i)
}
```

Example: Only factors
========================================================

```{r plot_only_factors_1, echo = FALSE, par_1x1 = TRUE}
plot(ddfit, cov_index = 6)
add_hist(6)
```

<small>See this <a href='https://www.backblaze.com/blog/3tb-hard-drive-failure/'>blog post</a> for explanation</small>

<!--
Background: 
Beginning in January 2012, Backblaze deployed 4,829 Seagate 3TB hard drives, model ST3000DM001, into Backblaze Storage Pods. In our experience, 80% of the hard drives we deploy will function at least 4 years. As of March 31, 2015, just 10% of the Seagate 3TB drives deployed in 2012 are still in service. This is the story of the 4,345 Seagate 3TB drives that are no longer in service. [Remeber data we have starts April 2013 and ends Q3 2016]

[Deployed over ~2 years See figure in the post]

...

As of March 31, 2015, 1,423 of the 4,829 deployed Seagate 3TB drives had failed, that’s 29.5% of the drives.

...

[On failure in service they start to test the others drivers if an error happen doing rebuilidng. These is not in the statistics!]

Let’s take a minute to describe what happens when a drive in a Storage Pod fails. When a drive fails, no data is compromised since we distribute data redundantly across multiple drives. Simply, the bad drive is replaced and the system is tested and rebuilt. During the entire process, the data is safe and available for file recovery as needed.

If during the rebuilding process, a second drives fails, the data is migrated to another Storage Pod where it is safe and available, and the Storage Pod with the second failed drive is taken off-line. Once off-line, technicians go through a series of steps to assess the health of the system.

One of the health assessment steps can be to remove all the drives from the Storage Pod for testing. There are two different tests. The first test is similar to “advanced” reformatting and takes about 20 minutes. The second basically writes and reads all the sectors on the drive and takes several hours. Only if a drives passes both tests can it be reformatted and reused.

...

The failure count continued to rise and in the Spring of 2014 we had decided that if a Storage Pod with Seagate 3TB drives showed any type of drive failure we would 1) immediately migrate all the data and then 2) remove and test all the drives in the Storage Pod.

In July alone, 189 hard drives failed and another 273 were removed from service. The total, 462, was 11.4% of the Seagate 3TB drives operational on July 1st, 2014.

To be clear, a drive is marked “Failed” because it failed in operation or during a rebuilding process. Drives marked “Removed” are those that were removed from a Storage Pod that contained failed drives. When the “Removed” drives were tested nearly 75% of them failed one of the two tests done after removal. It could be argued that 25% of the “Removed” drives were still good, even though they were assigned to the removed category, but these drives were never reinstalled.

...

As a reminder, about 75% of the “Removed” drives failed one of the bench tests once they were removed from a Storage Pod.
-->

Example: Slope
========================================================

<img class="dl-web" alt="Storage pod" src="dl-figure/pod-50-top-background.jpg"/>

Example: Slope
========================================================

```{r est_w_slope, echo = FALSE}
#####
# Make new factor with only some of the factors for the hard disk version with
# many disks
tmp_n_mods <- 
  tapply(hd_dat$serial_number, 
         hd_dat$model, 
         function(x) length(unique(x)))

tmp_n_mods <- sort(tmp_n_mods, decreasing = TRUE)

models_w_many <- names(tmp_n_mods)[1:6]
hd_dat$model_large <- as.character(hd_dat$model)
hd_dat$model_large[!hd_dat$model_large %in% models_w_many] <- "other"

hd_dat$model_large <- factor(hd_dat$model_large)
# ftable(xtabs(~ model_large + model, hd_dat, subset = !duplicated(hd_dat$serial_number)), row.vars = 1:2) # check
```

```{r est_w_slope_show_formula}
frm <- Surv(tstart, tstop, fails) ~ -1 + model + smart_12 : model_large
```

Example: Slope
========================================================

```{r est_w_slope_est_n_plot, echo=FALSE, par_2x1 = TRUE}
#####
# Fit model
ddfit_w_slope <- ddhazard(
    formula = frm, data = hd_dat,
    id = hd_dat$serial_number,
    Q_0 = diag(1, 24), Q = diag(.01, 24),
    by = 1,max_T = 60, 
    control = list(
      method = "EKF",
      eps = .001,
      NR_eps = .0000001, 
      LR = .5))

#####
# Find index of example
hd_version <- "ST3000DM001"
var_idx <- which(grepl(hd_version, dimnames(ddfit_w_slope$state_vecs)[[2]]))

#####
# Plot example
hd_version_pretty <- get_pretty_model_factors(hd_version)
plot(ddfit_w_slope, cov_index = var_idx[1], 
     ylab = paste("Intercept", hd_version_pretty))
add_hist(6)
plot(ddfit_w_slope, cov_index = var_idx[2],
     ylab = paste("Slope", hd_version_pretty))
abline(h = 0, lty = 3)
```

```{r look_at_span, echo = FALSE, eval = FALSE}
hd_sub <- hd_dat[hd_dat$model == hd_version, ]
tmp_span <-
  by(hd_sub, hd_sub$serial_number, 
     function(x) max(x$tstop)- min(x$tstart))
hist(tmp_span, breaks = 50)
```

Example: Slope
========================================================
incremental: true

* First failure: Replace and *rebuilt*
* Extra test if another fails doing rebuilt
* Removal due to extra tests are not recorded!

Example: Slope
========================================================

<img class="dl-web" alt="Failure status" src="dl-figure/blog_seagate_3tb_2012_failures_cut.jpg"/>

Example: Slope
========================================================

```{r est_w_fixed_effect, echo = FALSE, eval = FALSE}
#####
# Estimate fixed effect
ddfit_w_fixed_slope <- ddhazard(
    formula = Surv(tstart, tstop, fails) ~ -1 + model + ddFixed(smart_12 * (model == hd_version)), 
    data = hd_dat,
    id = hd_dat$serial_number,
    Q_0 = diag(1, 17), Q = diag(.01, 17),
    by = 1,max_T = 60, 
    control = list(
      method = "EKF",
      eps = .001,
      NR_eps = .0000001, 
      LR = .5, 
      fixed_terms_method = "M_step"))

plot(ddfit_w_fixed_slope, cov_index = 6)
add_hist(6)

ddfit_w_fixed_slope$fixed_effects
```


```{r est_w_slope_manual_check_n_xtra_plot, echo = FALSE, par_2x1 = TRUE}
# #####
# # Check coefs
# plot(ddfit_w_slope, cov_index = 1:9)
# plot(ddfit_w_slope, cov_index = 10:18)
# plot(ddfit_w_slope, cov_index = 19:24)

##### 
# Check quantiles of smart_12 through time for the selected hd version
tmp_dat <- get_survival_case_weights_and_data(
  Surv(tstart, tstop, fails) ~ smart_12 + model,
  
  data = hd_dat, 
  
  by = 1, max_T = 60, use_weights = F,
  id = hd_dat$serial_number)
tmp_dat$X <- tmp_dat$X[tmp_dat$X$model == hd_version, ]

quants_through_time <- 
  tapply(tmp_dat$X$smart_12, tmp_dat$X$t, quantile, probs = (1:19) / 20)

quants_through_time <- do.call(rbind, quants_through_time)

# # Check number of observations
# xtabs(~ tmp_dat$X$t)

#####
# Plot
plot(ddfit_w_slope, cov_index = var_idx[2],
     ylab = paste("Slope", hd_version_pretty))
abline(h = 0, lty = 3)
add_hist(6)

plot(c(1, 60), range(quants_through_time), type = "n", 
     xlab = "Time", ylab = "Quantiles of on/off times")
x <- 1:nrow(quants_through_time)
lb <- rep(0, nrow(quants_through_time))
for(i in 1:ncol(quants_through_time)){
  ub <- quants_through_time[, i]
  col <- gray(1 - .5 *(i / 20))
  col_b <- gray(1 - i / 20)
  polygon(c(x, rev(x)), c(ub, rev(lb)), 
          col = col, 
          border = col_b, lty = 3)
  lb <- ub
}

#####
# Clean up
rm(tmp_dat)
```


Other options
========================================================
incremental: true

* S3 methods for plots, residuals, predict etc. 
* Second order random walk
* Bootstrap
* Fixed effects
* Continuous time observational model

<small>Slides at <a href="https://rpubs.com/boennecd/EMS17">rpubs.com/boennecd/EMS17</a> and code at <a href="https://github.com/boennecd/Talks">github/boennecd/Talks</a></small>

Demo
========================================================

```{r, eval = FALSE}
dynamichazard::ddhazard_app()
```


References
========================================================
<small><div class="references">
```{r, echo=FALSE, results="asis"}
bib
```
</div></small>
