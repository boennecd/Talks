@article{Dempster77,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2984875},
 abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
 author = {A. P. Dempster and N. M. Laird and D. B. Rubin},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {1--38},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Maximum Likelihood from Incomplete Data via the EM Algorithm},
 volume = {39},
 year = {1977}
}


@article{Meng93,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2337198},
 abstract = {Two major reasons for the popularity of the EM algorithm are that its maximum step involves only complete-data maximum likelihood estimation, which is often computationally simple, and that its convergence is stable, with each iteration increasing the likelihood. When the associated complete-data maximum likelihood estimation itself is complicated, EM is less attractive because the M-step is computationally unattractive. In many cases, however, complete-data maximum likelihood estimation is relatively simple when conditional on some function of the parameters being estimated. We introduce a class of generalized EM algorithms, which we call the ECM algorithm, for Expectation/Conditional Maximization (CM), that takes advantage of the simplicity of complete-data conditional maximum likelihood estimation by replacing a complicated M-step of EM with several computationally simpler CM-steps. We show that the ECM algorithm shares all the appealing convergence properties of EM, such as always increasing the likelihood, and present several illustrative examples.},
 author = {Xiao-Li Meng and Donald B. Rubin},
 journal = {Biometrika},
 number = {2},
 pages = {267--278},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Maximum Likelihood Estimation via the ECM Algorithm: A General Framework},
 volume = {80},
 year = {1993}
}



@ARTICLE{Gordon93, 
author={N. J. Gordon and D. J. Salmond and A. F. M. Smith}, 
journal={IEE Proceedings F - Radar and Signal Processing}, 
title={Novel approach to nonlinear/non-Gaussian Bayesian state estimation}, 
year={1993}, 
volume={140}, 
number={2}, 
pages={107-113}, 
keywords={Bayes methods;filtering and prediction theory;Kalman filters;state estimation;tracking;State estimation;state transition model;state vector density;nonlinear Bayesian state estimation;nonGaussian Bayesian state estimation;algorithm;bootstrap filter;recursive Bayesian filters;random samples;Gaussian noise;measurement model;simulation;bearings only tracking problem;extended Kalman filter;Bayes procedures;Filtering;Kalman filtering;Tracking;Prediction methods}, 
doi={10.1049/ip-f-2.1993.0015}, 
ISSN={0956-375X}, 
month={April}}

@article{Pitt99,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2670179},
 abstract = {This article analyses the recently suggested particle approach to filtering time series. We suggest that the algorithm is not robust to outliers for two reasons: the design of the simulators and the use of the discrete support to represent the sequentially updating prior distribution. Here we tackle the first of these problems.},
 author = {Michael K. Pitt and Neil Shephard},
 journal = {Journal of the American Statistical Association},
 number = {446},
 pages = {590--599},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Filtering via Simulation: Auxiliary Particle Filters},
 volume = {94},
 year = {1999}
}

@article{Shumway01,
 ISSN = {00219398, 15375374},
 URL = {http://www.jstor.org/stable/10.1086/209665},
 abstract = {I argue that hazard models are more appropriate than single‐period models for forecasting bankruptcy. Single‐period models are inconsistent, while hazard models produce consistent estimates. I describe a simple technique for estimating a discrete‐time hazard model. I find that about half of the accounting ratios that have been used in previous models are not statistically significant. Moreover, market size, past stock returns, and idiosyncratic returns variability are all strongly related to bankruptcy. I propose a model that uses both accounting ratios and market‐driven variables to produce out‐of‐sample forecasts that are more accurate than those of alternative models.},
 author = {Tyler Shumway},
 journal = {The Journal of Business},
 number = {1},
 pages = {101-124},
 publisher = {The University of Chicago Press},
 title = {Forecasting Bankruptcy More Accurately: A Simple Hazard Model},
 volume = {74},
 year = {2001}
}


@article{Friedman01,
 ISSN = {00905364},
 URL = {http://www.jstor.org/stable/2699986},
 abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
 author = {Jerome H. Friedman},
 journal = {The Annals of Statistics},
 number = {5},
 pages = {1189-1232},
 publisher = {Institute of Mathematical Statistics},
 title = {Greedy Function Approximation: A Gradient Boosting Machine},
 volume = {29},
 year = {2001}
}


@article{Friedman02,
title = "Stochastic gradient boosting",
journal = "Computational Statistics & Data Analysis",
volume = "38",
number = "4",
pages = "367 - 378",
year = "2002",
note = "Nonlinear Methods and Data Mining",
issn = "0167-9473",
doi = "https://doi.org/10.1016/S0167-9473(01)00065-2",
url = "http://www.sciencedirect.com/science/article/pii/S0167947301000652",
author = "Jerome H. Friedman"
}


@article{Chava04,
author = {Chava, Sudheer and Jarrow, Robert A.},
title = { Bankruptcy Prediction with Industry Effects *},
journal = {Review of Finance},
volume = {8},
number = {4},
pages = {537-569},
year = {2004},
doi = {10.1093/rof/8.4.537},
URL = { + http://dx.doi.org/10.1093/rof/8.4.537},
eprint = {/oup/backfile/content_public/journal/rof/8/4/10.1093_rof_8.4.537/3/8-4-537.pdf}
}

@article{Bates04,
title = "Linear mixed models and penalized least squares",
journal = "Journal of Multivariate Analysis",
volume = "91",
number = "1",
pages = "1 - 17",
year = "2004",
note = "Special Issue on Semiparametric and Nonparametric Mixed Models",
issn = "0047-259X",
doi = "https://doi.org/10.1016/j.jmva.2004.04.013",
url = "http://www.sciencedirect.com/science/article/pii/S0047259X04000867",
author = "Douglas M Bates and Saikat DebRoy",
keywords = "REML, Gradient, Hessian, EM algorithm, ECME algorithm, Maximum likelihood, Profile likelihood, Multilevel models"
}

@article{Vassalou04,
 ISSN = {00221082, 15406261},
 URL = {http://www.jstor.org/stable/3694915},
 abstract = {This is the first study that uses Merton's (1974) option pricing model to compute default measures for individual firms and assess the effect of default risk on equity returns. The size effect is a default effect, and this is also largely true for the book-to-market (BM) effect. Both exist only in segments of the market with high default risk. Default risk is systematic risk. The Fama-French (FF) factors SMB and HML contain some default-related information, but this is not the main reason that the FF model can explain the cross section of equity returns.},
 author = {Maria Vassalou and Yuhang Xing},
 journal = {The Journal of Finance},
 number = {2},
 pages = {831--868},
 publisher = {[American Finance Association, Wiley]},
 title = {Default Risk in Equity Returns},
 volume = {59},
 year = {2004}
}

@Article{Beaver05,
author="Beaver, William H.
and McNichols, Maureen F.
and Rhie, Jung-Wu",
title="Have Financial Statements Become Less Informative? Evidence from the Ability of Financial Ratios to Predict Bankruptcy",
journal="Review of Accounting Studies",
year="2005",
month="Mar",
day="01",
volume="10",
number="1",
pages="93--122",
abstract="Using a hazard model, we examine secular changes in the ability of financial statement data to predict bankruptcy from 1962 to 2002. We identify three trends in financial reporting that could influence predictive ability with respect to bankruptcy: FASB standards, the perceived increase in discretionary financial reporting behavior, and the increase in unrecognized assets and obligations. A parsimonious three-variable model provides significant explanatory power throughout the time period, with only a slight deterioration in predictive power from the first to the second time period. The striking feature of the results is the robustness of the predictive models over a forty-year period.",
issn="1573-7136",
doi="10.1007/s11142-004-6341-9",
url="https://doi.org/10.1007/s11142-004-6341-9"
}

@TECHREPORT{Basel06,
  title={International Convergence of Capital Measurement and Capital Standards: a revised framework, (comprehensive version)},
  author={{Basel Committee on Banking Supervision}}, 
  year={2006},
  month={June}, 
  INSTITUTION={Bank for International Settlements}
}

@article{Duffie07,
title = "Multi-period corporate default prediction with stochastic covariates",
journal = "Journal of Financial Economics",
volume = "83",
number = "3",
pages = "635 - 665",
year = "2007",
issn = "0304-405X",
doi = "https://doi.org/10.1016/j.jfineco.2005.10.011",
url = "http://www.sciencedirect.com/science/article/pii/S0304405X06002029",
author = "Darrell Duffie and Leandro Saita and Ke Wang",
keywords = "Default, Bankruptcy, Duration analysis, Doubly stochastic, Distance to default"
}




@article{Buhlmann07,
 ISSN = {08834237},
 URL = {http://www.jstor.org/stable/27645854},
 abstract = {We present a statistical perspective on boosting. Special emphasis is given to estimating potentially complex parametric or nonparametric models, including generalized linear and additive models as well as regression models for survival analysis. Concepts of degrees of freedom and corresponding Akaike or Bayesian information criteria, particularly useful for regularization and variable selection in high-dimensional covariate spaces, are discussed as well. The practical aspects of boosting procedures for fitting statistical models are illustrated by means of the dedicated open-source software package mboost. This package implements functions which can be used for model fitting, prediction and variable selection. It is flexible, allowing for the implementation of new boosting algorithms optimizing user-specified loss functions.},
 author = {Peter Bühlmann and Torsten Hothorn},
 journal = {Statistical Science},
 number = {4},
 pages = {477-505},
 publisher = {Institute of Mathematical Statistics},
 title = {Boosting Algorithms: Regularization, Prediction and Model Fitting},
 volume = {22},
 year = {2007}
}


@article {Daniel07,
author = {Berg, Daniel},
title = {Bankruptcy prediction by generalized additive models},
journal = {Applied Stochastic Models in Business and Industry},
volume = {23},
number = {2},
publisher = {John Wiley & Sons, Ltd.},
issn = {1526-4025},
url = {http://dx.doi.org/10.1002/asmb.658},
doi = {10.1002/asmb.658},
pages = {129--143},
keywords = {bankruptcy prediction, generalized additive models, default horizon, performance depreciation, multi-year model},
year = {2007},
}


@article {Campbell08,
author = {Campbell, John Y. And Hilscher, Jens and Szilagyi, Jan},
title = {In Search of Distress Risk},
journal = {The Journal of Finance},
volume = {63},
number = {6},
publisher = {Blackwell Publishing Inc},
issn = {1540-6261},
url = {http://dx.doi.org/10.1111/j.1540-6261.2008.01416.x},
doi = {10.1111/j.1540-6261.2008.01416.x},
pages = {2899--2939},
year = {2008},
}


@article{Alfaro08,
title = "Bankruptcy forecasting: An empirical comparison of AdaBoost and neural networks",
journal = "Decision Support Systems",
volume = "45",
number = "1",
pages = "110 - 122",
year = "2008",
note = "Data Warehousing and OLAP",
issn = "0167-9236",
doi = "https://doi.org/10.1016/j.dss.2007.12.002",
url = "http://www.sciencedirect.com/science/article/pii/S016792360700214X",
author = "Esteban Alfaro and Noelia García and Matías Gámez and David Elizondo",
keywords = "Corporate Failure Prediction, Neural Network, AdaBoost"
}


@article{Shumway08,
author = {Bharath, Sreedhar T. and Shumway, Tyler},
title = {Forecasting Default with the Merton Distance to Default Model},
journal = {The Review of Financial Studies},
volume = {21},
number = {3},
pages = {1339-1369},
year = {2008},
doi = {10.1093/rfs/hhn044},
URL = {http://dx.doi.org/10.1093/rfs/hhn044},
eprint = {/oup/backfile/content_public/journal/rfs/21/3/10.1093_rfs_hhn044/1/hhn044.pdf}
}


@article {Duffie09,
author = {Duffie, Darrell and Eckner, Andreas and Horel, Guillaume and Saita, Leandro},
title = {Frailty Correlated Default},
journal = {The Journal of Finance},
volume = {64},
number = {5},
publisher = {Blackwell Publishing Inc},
issn = {1540-6261},
url = {http://dx.doi.org/10.1111/j.1540-6261.2009.01495.x},
doi = {10.1111/j.1540-6261.2009.01495.x},
pages = {2089--2123},
year = {2009},
}

@article{Fearnhead10,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/25734097},
 abstract = {In this paper we propose a new particle smoother that has a computational complexity of O(N), where N is the number of particles. This compares favourably with the O(N²) computational cost of most smoothers. The new method also overcomes some degeneracy problems in existing algorithms. Through simulation studies we show that substantial gains in efficiency are obtained for practical amounts of computational cost. It is shown both through these simulation studies, and by the analysis of an athletics dataset, that our new method also substantially outperforms the simple filter-smoother, the only other smoother with computational cost that is O(N).},
 author = {Paul Fearnhead and David Wyncoll and Jonathan Tawn},
 journal = {Biometrika},
 number = {2},
 pages = {447--464},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {A sequential smoothing algorithm with linear computational cost},
 volume = {97},
 year = {2010}
}


@article{Dakovic10,
author = { Rada   Dakovic  and  Claudia   Czado  and  Daniel   Berg },
title = {Bankruptcy prediction in Norway: a comparison study},
journal = {Applied Economics Letters},
volume = {17},
number = {17},
pages = {1739-1746},
year  = {2010},
publisher = {Routledge},
doi = {10.1080/13504850903299594},

URL = { 
        https://doi.org/10.1080/13504850903299594
    
},
eprint = { 
        https://doi.org/10.1080/13504850903299594
    
}
}



@ARTICLE{Zhou10,
   author = {{Zhou}, S.},
    title = "{Thresholded Lasso for high dimensional variable selection and statistical estimation}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1002.1583},
 keywords = {Mathematics - Statistics},
     year = 2010,
    month = feb,
   adsurl = {http://adsabs.harvard.edu/abs/2010arXiv1002.1583Z},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@book{buhlmann11,
  title={Statistics for high-dimensional data: methods, theory and applications},
  author={B{\"u}hlmann, Peter and Van De Geer, Sara},
  year={2011},
  publisher={Springer Science \& Business Media}
}


@article{Koopman11,
title = "Modeling frailty-correlated defaults using many macroeconomic covariates",
journal = "Journal of Econometrics",
volume = "162",
number = "2",
pages = "312 - 325",
year = "2011",
issn = "0304-4076",
doi = "https://doi.org/10.1016/j.jeconom.2011.02.003",
url = "http://www.sciencedirect.com/science/article/pii/S0304407611000303",
author = "Siem Jan Koopman and André Lucas and Bernd Schwaab",
keywords = "Systematic default risk, Frailty-correlated defaults, State space methods, Credit risk management"
}

@article{Chava11,
author = {Chava, Sudheer and Stefanescu, Catalina and Turnbull, Stuart},
title = {Modeling the Loss Distribution},
journal = {Management Science},
volume = {57},
number = {7},
pages = {1267-1287},
year = {2011},
doi = {10.1287/mnsc.1110.1345},

URL = { 
        https://doi.org/10.1287/mnsc.1110.1345
    
},
eprint = { 
        https://doi.org/10.1287/mnsc.1110.1345
    
}
,
    abstract = { In this paper, we focus on modeling and predicting the loss distribution for credit risky assets such as bonds and loans. We model the probability of default and the recovery rate given default based on shared covariates. We develop a new class of default models that explicitly accounts for sector specific and regime dependent unobservable heterogeneity in firm characteristics. Based on the analysis of a large default and recovery data set over the horizon 1980–2008, we document that the specification of the default model has a major impact on the predicted loss distribution, whereas the specification of the recovery model is less important. In particular, we find evidence that industry factors and regime dynamics affect the performance of default models, implying that the appropriate choice of default models for loss prediction will depend on the credit cycle and on portfolio characteristics. Finally, we show that default probabilities and recovery rates predicted out of sample are negatively correlated and that the magnitude of the correlation varies with seniority class, industry, and credit cycle. This paper was accepted by Wei Xiong, finance. }
}

@article{Duan12,
title = "Multiperiod corporate default prediction—A forward intensity approach",
journal = "Journal of Econometrics",
volume = "170",
number = "1",
pages = "191 - 209",
year = "2012",
issn = "0304-4076",
doi = "https://doi.org/10.1016/j.jeconom.2012.05.002",
url = "http://www.sciencedirect.com/science/article/pii/S0304407612001145",
author = "Jin-Chuan Duan and Jie Sun and Tao Wang",
keywords = "Default, Bankruptcy, Forward intensity, Maximum pseudo-likelihood, Forward default probability, Cumulative default probability, Accuracy ratio"
}

@article{Hwang12,
title = "A varying-coefficient default model",
journal = "International Journal of Forecasting",
volume = "28",
number = "3",
pages = "675 - 688",
year = "2012",
issn = "0169-2070",
doi = "https://doi.org/10.1016/j.ijforecast.2011.11.006",
url = "http://www.sciencedirect.com/science/article/pii/S0169207012000052",
author = "Ruey-Ching Hwang",
keywords = "Discrete-time hazard model, Local likelihood, Expanding rolling window approach, Predicted number of defaults, Predictive interval, Varying-coefficient model"
}


@article {Wood15,
author = {Wood, Simon N. and Goude, Yannig and Shaw, Simon},
title = {Generalized additive models for large data sets},
journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
volume = {64},
number = {1},
issn = {1467-9876},
url = {http://dx.doi.org/10.1111/rssc.12068},
doi = {10.1111/rssc.12068},
pages = {139--155},
keywords = {Correlated additive model, Electricity load prediction, Generalized additive model estimation},
year = {2015},
}

@article{David13,
author = {Lando, David and Medhat, Mamdouh and Nielsen, Mads Stenbo and Nielsen, Søren Feodor},
title = {Additive Intensity Regression Models in Corporate Default Analysis},
journal = {Journal of Financial Econometrics},
volume = {11},
number = {3},
pages = {443-485},
year = {2013},
doi = {10.1093/jjfinec/nbs018},
URL = {http://dx.doi.org/10.1093/jjfinec/nbs018},
eprint = {/oup/backfile/content_public/journal/jfec/11/3/10.1093/jjfinec/nbs018/2/nbs018.pdf}
}

@article{Qi14,
title = "Unobserved systematic risk factor and default prediction",
journal = "Journal of Banking & Finance",
volume = "49",
pages = "216 - 227",
year = "2014",
issn = "0378-4266",
doi = "https://doi.org/10.1016/j.jbankfin.2014.09.009",
url = "http://www.sciencedirect.com/science/article/pii/S0378426614003094",
author = "Min Qi and Xiaofei Zhang and Xinlei Zhao",
keywords = "Observed systematic risk factors, Unobserved systematic risk factor, Corporate default prediction, Rank order, Predictive accuracy"
}


@article{Bates15,
   author = {Douglas Bates and Martin Mächler and Ben Bolker and Steve Walker},
   title = {Fitting Linear Mixed-Effects Models Using lme4},
   journal = {Journal of Statistical Software, Articles},
   volume = {67},
   number = {1},
   year = {2015},
   keywords = {sparse matrix methods; linear mixed models; penalized least squares; Cholesky decomposition},
   abstract = {Maximum likelihood or restricted maximum likelihood (REML) estimates of the parameters in linear mixed-effects models can be determined using the lmer function in the lme4 package for R. As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed- and random-effects terms. The formula and data together determine a numerical representation of the model from which the profiled deviance or the profiled REML criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the profiled deviance or REML criterion, and the structure of classes or types that represents such a model. Sufficient detail is included to allow specialization of these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.},
   issn = {1548-7660},
   pages = {1--48},
   doi = {10.18637/jss.v067.i01},
   url = {https://www.jstatsoft.org/v067/i01}
}


@article{Zieba16,
title = "Ensemble boosted trees with synthetic features generation in application to bankruptcy prediction",
journal = "Expert Systems with Applications",
volume = "58",
pages = "93 - 101",
year = "2016",
issn = "0957-4174",
doi = "https://doi.org/10.1016/j.eswa.2016.04.001",
url = "http://www.sciencedirect.com/science/article/pii/S0957417416301592",
author = "Maciej Zięba and Sebastian K. Tomczak and Jakub M. Tomczak",
keywords = "Bankruptcy prediction, Extreme gradient boosting, Synthetic features generation, Imbalanced data"
}


@article{Azizpour16,
  title={Exploring the sources of default clustering},
  author={Azizpour, Shahriar and Giesecke, Kay and Schwenkler, Gustavo},
  year={2016}
}


@article {Jones17,
author = {Jones, Stewart and Johnstone, David and Wilson, Roy},
title = {Predicting Corporate Bankruptcy: An Evaluation of Alternative Statistical Frameworks},
journal = {Journal of Business Finance & Accounting},
volume = {44},
number = {1-2},
issn = {1468-5957},
url = {http://dx.doi.org/10.1111/jbfa.12218},
doi = {10.1111/jbfa.12218},
pages = {3--34},
keywords = {corporate bankruptcy prediction, binary classifiers, statistical learning},
year = {2017},
}


@article{Nickerson17,
title = "Debt correlations in the wake of the financial crisis: What are appropriate default correlations for structured products?",
journal = "Journal of Financial Economics",
volume = "125",
number = "3",
pages = "454 - 474",
year = "2017",
issn = "0304-405X",
doi = "https://doi.org/10.1016/j.jfineco.2017.06.011",
url = "http://www.sciencedirect.com/science/article/pii/S0304405X17301289",
author = "Jordan Nickerson and John M. Griffin",
keywords = "Credit ratings, Financial crises, Structured finance, Default correlations"
}


@Book{Wood17,
    title = {Generalized Additive Models: An Introduction with R},
    year = {2017},
    author = {S.N Wood},
    edition = {2},
    publisher = {Chapman and Hall/CRC},
}

@article{Jensen17,
  title={Cyclicality and Firm-size in Private Firm Defaults},
  author={Jensen, Thais and Lando, David and Medhat, Mamdouh},
  year={2017},
  Journal={International Journal of Central Banking},
  volume={13}, 
  number={4}, 
  pages={97--145}
}

@Book{Wood17,
    title = {Generalized Additive Models: An Introduction with R},
    year = {2017},
    author = {S.N Wood},
    edition = {2},
    publisher = {Chapman and Hall/CRC},
  }


@article{kwon18,
title = "Industry specific defaults",
journal = "Journal of Empirical Finance",
volume = "45",
pages = "45 - 58",
year = "2018",
issn = "0927-5398",
doi = "https://doi.org/10.1016/j.jempfin.2017.10.002",
url = "http://www.sciencedirect.com/science/article/pii/S0927539817300920",
author = "Tae Yeon Kwon and Yoonjung Lee",
keywords = "Intensity credit risk model, Within industry default correlation, Between industries default correlation, Frailty, MCEM"
}

@article{Azizpour18,
title = "Exploring the sources of default clustering",
journal = "Journal of Financial Economics",
volume = "129",
number = "1",
pages = "154 - 183",
year = "2018",
issn = "0304-405X",
doi = "https://doi.org/10.1016/j.jfineco.2018.04.008",
url = "http://www.sciencedirect.com/science/article/pii/S0304405X1830103X",
author = "S Azizpour and K. Giesecke and G. Schwenkler",
keywords = "Default clustering, Contagion, Frailty, Correlated default risk"
}
