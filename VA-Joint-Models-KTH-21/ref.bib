@Manual{Christoffersen20,
    title = {psqn: Partially Separable Quasi-Newton},
    author = {Benjamin Christoffersen},
    year = {2020},
    note = {R package version 0.1.3},
    url = {https://github.com/boennecd/psqn},
  }

@Manual{Coppola20,
    title = {lbfgs: Limited-Memory BFGS Optimization},
    author = {Antonio Coppola and Brandon Stewart and Naoaki Okazaki},
    year = {2020},
    note = {R package version 1.2.2},
  }
  
@article{Ong18,
author = {Victor M.-H. Ong and David J. Nott and Michael S. Smith},
title = {Gaussian Variational Approximation With a Factor Covariance Structure},
journal = {Journal of Computational and Graphical Statistics},
volume = {27},
number = {3},
pages = {465-478},
year  = {2018},
publisher = {Taylor & Francis},
doi = {10.1080/10618600.2017.1390472},
}

@InProceedings{Miller17,
  title = 	 {Variational Boosting: Iteratively Refining Posterior Approximations},
  author =       {Andrew C. Miller and Nicholas J. Foti and Ryan P. Adams},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2420--2429},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  abstract = 	 {We propose a black-box variational inference method to approximate intractable distributions with an increasingly rich approximating class. Our method, variational boosting, iteratively refines an existing variational approximation by solving a sequence of optimization problems, allowing a trade-off between computation time and accuracy. We expand the variational approximating class by incorporating additional covariance structure and by introducing new components to form a mixture. We apply variational boosting to synthetic and real statistical models, and show that the resulting posterior inferences compare favorably to existing variational algorithms.}
}
  
@Article{Bates15,
    title = {Fitting Linear Mixed-Effects Models Using {lme4}},
    author = {Douglas Bates and Martin M{\"a}chler and Ben Bolker and Steve Walker},
    journal = {Journal of Statistical Software},
    year = {2015},
    volume = {67},
    number = {1},
    pages = {1--48},
    doi = {10.18637/jss.v067.i01},
  }
  
@article{Challis13,
  author  = {Edward Challis and David Barber},
  title   = {Gaussian {K}ullback-{L}eibler Approximate Inference},
  journal = {Journal of Machine Learning Research},
  year    = {2013},
  volume  = {14},
  number  = {32},
  pages   = {2239-2286},
}
  
@article{Ormerod12,
 ISSN = {10618600},
 URL = {http://www.jstor.org/stable/23248820},
 abstract = {Variational approximation methods have become a mainstay of contemporary machine learning methodology, but currently have little presence in statistics. We devise an effective variational approximation strategy for fitting generalized linear mixed models (GLMMs) appropriate for grouped data. It involves Gaussian approximation to the distributions of random effects vectors, conditional on the responses. We show that Gaussian variational approximation is a relatively simple and natural alternative to Laplace approximation for fast, non-Monte Carlo, GLMM analysis. Numerical studies show Gaussian variational approximation to be very accurate in grouped data GLMM contexts. Finally, we point to some recent theory on consistency of Gaussian variational approximation in this context. Supplemental materials are available online.},
 author = {J. T. Ormerod and M. P. Wand},
 journal = {Journal of Computational and Graphical Statistics},
 number = {1},
 pages = {2--17},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
 title = {Gaussian Variational Approximate Inference for Generalized Linear Mixed Models},
 volume = {21},
 year = {2012}
}



@article{Ormerod11,
author = { J. T.   Ormerod  and  M. P.   Wand },
title = {Gaussian Variational Approximate Inference for Generalized Linear Mixed Models},
journal = {Journal of Computational and Graphical Statistics},
volume = {21},
number = {1},
pages = {2-17},
year  = {2012},
publisher = {Taylor & Francis},
doi = {10.1198/jcgs.2011.09118},

URL = { 
        https://doi.org/10.1198/jcgs.2011.09118
    
},
eprint = { 
        https://doi.org/10.1198/jcgs.2011.09118
    
}

}

@book{nocedal06,
  title={Numerical optimization},
  author={Nocedal, Jorge and Wright, Stephen},
  year={2006},
  publisher={Springer Science \& Business Media},
  edition = {2},
  isbn = {978-0-387-30303-1},
  doi = {10.1007/978-0-387-40065-5}
}