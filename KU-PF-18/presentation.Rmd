---
title: "Particle Smoothing"
bibliography: refs.bib
output: 
  revealjs::revealjs_presentation: 
    theme: simple
    keep_mdK: true
    center: false
    transition: slide
    css: css/styles.css
    self_contained: true
    reveal_options:
      slideNumber: true
    includes:
      in_header: header.html
      after_body: doc_suffix.html
---

```{r setup, include=FALSE}
options(width = 100, digits = 4, scipen = 8)
knitr::opts_chunk$set(
  error = FALSE, cache = FALSE, warning = FALSE, message = FALSE, dpi = 128, 
  stop = FALSE, fig.height = 3.75, fig.width = 5, 
  cache.path = paste0("cache", .Platform$file.sep), 
  fig.path = paste0("fig", .Platform$file.sep))

knitr::knit_hooks$set(inline = function(x) {
  prettyNum(x, big.mark = ",")
})
```

## dummy slide

<!--javascript to remove dummy slide-->
<!--html_preserve-->
<script>
(function() {
  document.getElementById("dummy-slide").remove(); 
  
  var header_extra = document.createElement("h2");
  header_extra.innerHTML = "Hidden Markov Models for Modeling Financial Distress";
  document.getElementsByTagName("section")[0].appendChild(header_extra);
  
  var credit_div = document.createElement('div');
  credit_div.innerHTML = "<p>Benjamin Christoffersen<br>Copenhagen Business School<br>Department of Finance, Center for Statistics</p>";
  credit_div.className += "left";
  document.getElementsByTagName("section")[0].appendChild(credit_div);
  
  document.getElementsByTagName("section")[0].classList.add("front");
})();
</script>
<!--/html_preserve-->

<!--end dummy slide-->
</section>

<!--html_preserve-->
<section>

$$
\definecolor{cora}{RGB}{230,159,0}
\definecolor{blugre}{RGB}{0,158,115}
\definecolor{vermilion}{RGB}{213,94,0}
\def\mat#1{{\mathbf{#1}}}
\def\vect#1{{\boldsymbol{#1}}}
\def\diff{{\mathop{}\!\mathrm{d}}}
\def\LP#1{{\left(#1\right)}}
\def\LB#1{{\left\{#1\right\}}}
\def\LPV #1#2{{\left(\left.#1\vphantom{#2} \right\vert #2\right)}}
\def\PF#1#2#3{\overrightarrow{#1}_{#2}^{(#3)}}
\def\PB#1#2#3{\overleftarrow{#1}_{#2}^{(#3)}}
\def\PS#1#2#3{\overleftrightarrow{#1}_{#2}^{(#3)}}
\def\bigO#1{\mathcal{O}\LP{#1}}
\def\argmax{\mathop{\mathrm{argmax}}}
$$

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
</script>

<section class="titleslide slide level1">
<h1>Introduction</h1>
<div class ="left">
<div class="no-top-marg-next">
<!--/html_preserve-->

Discrete-time models are popular corporate distress models.
<small>E.g., the event that a firms miss an interest payment. 
@Shumway01, @Chava04, and @Campbell08 had `r 2395L + 846L + 1635L`
citations on Google Scholar on 7/11/2018.</small></div>

<div class = "fragment">

$$
\begin{align*}
y_{it} &\in\{0,1\} \\
h\LP{E\LPV{y_{it}}{\vect z_t}} &= \vect\beta^\top\vect z_{it}
\end{align*}
$$

<small>$h$ is a link function and $\vect z_{it}$ are know covariates.</small>

</div>

<!--html_preserve-->
</div>

<!-- there is a end tag from the previous slide -->
<!-- </section> -->
<!--/html_preserve-->

## Data set

```{r source_funcs, echo = FALSE}
source(file.path("R", "regres_funcs.R"))
# change default to center
formals(sp_w_c)$do_center <- formals(wz)$do_center <- TRUE 
```


<!-- I am sorry but I cannot share the data set so the data directory will 
     be empty -->
     
```{r load_dat, echo = FALSE}
tmp <- readRDS(file.path("data", "final.RDS"))
dat <- tmp$data
make_ym <- tmp$ym_funcs$make_ym
make_ym_inv <- tmp$ym_funcs$make_ym_inv
rm(tmp)
```

```{r data_prep, echo = FALSE}
# tstop of last event
max_is_ev <- with(subset(dat, y == TRUE), max(tstop))

# have to deal with recurrent events which are not directly supported by 
# `dynamichazard`. Thus, we make a per event unique identifier
library(data.table)
dat <- data.table(dat)
setkey(dat, gvkey, tstart) # sort data
# assumes that data is sorted
func <- function(y){
  y <- cumsum(y)
  y <- c(0, head(y, -1))
  c("", paste0(".", letters))[y + 1L]
}
# func(c(F, F, F, T, F, F, T)) # example
dat[, gvkey_unique := paste0(gvkey, func(y)), by = gvkey]
stopifnot(all(dat[, sum(y) %in% 0:1, by = gvkey_unique]$V1))
dat <- as.data.frame(dat)
dat$gvkey_int <- as.integer(as.factor(dat$gvkey_unique))

# define formula for fixed effects. Remove observations with missing variables
frm_fixed <- Surv(tstart, tstop, y) ~ wz(r_req_nn) + wz(r_niq_nn) + 
  wz(r_ltq_nn) + wz(r_actq_lctq) + wz(sigma) + wz(excess_ret) + wz(rel_size) + 
  wz(dtd) + log_market_ret + r1y + sp_w_c(r_niq_nn, 4) + sp_w_c(sigma, 4)
dat <- dat[complete.cases(dat[, all.vars(frm_fixed)]), ]

library(dynamichazard)
distress <- get_survival_case_weights_and_data(
  frm_fixed, dat, by = 1L, max_T = max_is_ev, id = dat$gvkey_int, 
  use_weights = TRUE, is_for_discrete_model = TRUE)$X

# we assume that a macro avariable is included to all such that each 
# row represents one month. We can check this by checking the `weights` column. 
# Otherwise some logic may need to change in the rest of this file...
stopifnot(all(distress$weights == 1))

base_fit <- glm(update(frm_fixed, y ~ .), binomial(), distress)
```

<div class = "left">
Use data set with US public firms. The data set contains at least 
`r min(tapply(distress$tstop, distress$tstop, length))` firms and at most 
`r max(tapply(distress$tstop, distress$tstop, length))` firms at each month 
$t$. There is `r nrow(distress)` monthly observations, 
`r length(unique(distress$gvkey))` firms, and `r sum(distress$y)` distress 
events.</div>

## Cannot capture aggregate distress levels

```{r sim_agg_distres_base, echo = FALSE, cache = 1}
do_sims <- function(phat, tvar, nsim = 10000, spec = 4){
  library(parallel)
  cl <- makeCluster(spec)
  on.exit(stopCluster(cl))
  clusterSetRNGStream(cl)
  
  sims <- parSapply(cl, 1:nsim, function(..., tvar, phat){
    y <- phat > runif(length(phat))
    tapply(y, tvar, mean)
  }, tvar = tvar, phat = phat)
  
  lbs <- apply(sims, 1, quantile, probs = .025)
  ubs <- apply(sims, 1, quantile, probs = .975)
  
  list(lbs = lbs, ubs = ubs, mea = tapply(phat, tvar, mean))
}

set.seed(85664431)
base_sims <- do_sims(
  phat = predict(base_fit, type = "response", newdata = distress), 
  tvar = distress$tstop)
```

```{r plot_agg_distres_base, echo = FALSE}
make_sims_plot <- function(sims, data, ylim = NA){
  require(lubridate)
  rea <- tapply(data$y, data$tstop, mean)
  x <- make_ym_inv(as.integer(names(rea)))
  is_miss <- rea < sims$lbs | rea > sims$ubs
  plot(rea ~ x, pch = 16, ylim = ylim, 
       col = ifelse(is_miss, "Black", "DarkGray"), cex = .7, 
       xlab = "Year", ylab = "Predicted/realised distress rate", 
       xaxt = "n")
  min_d <- as.POSIXlt(min(x))
  month(min_d) <- 1L
  max_d <- as.POSIXlt(max(x))
  ax <- min_d
  while(tail(ax, 1)$year + 1L <= max_d$year){
    new_ax <- tail(ax, 1)
    new_ax$year <- new_ax$year + 1L
    ax <-  c(ax, new_ax)
  }
  labs <- format(ax, "%Y")
  ix <- seq_along(labs)
  labs[ix %% 2 == 0] <- ""
  axis(side = 1, at = as.Date(ax), labels = labs, las = 2)
  
  lines(make_ym_inv(as.integer(names(rea))), sims$mea)
  polygon(c(x, rev(x)), c(sims$lbs, rev(sims$ubs)), 
          col = rgb(0, 0, 0, .1), border = NA)
}

par(mar = c(5, 4, 1, 1))
make_sims_plot(base_sims, distress, c(0, .011))
```

<div class = "left"><small>Gray area is pointwise 95% prediction intervals. The line is the 
predicted distress rate. Dots are realized distress rates. Black dots are not 
covered by prediction intervals.</small></div>



## Model

$$
\begin{align*}
h\LP{E\LPV{y_{it}}{\vect z_t, \vect x_t, \vect u_{it}}} &= 
  \vect\beta^\top\vect z_{it} + \vect x_t^\top\vect u_{it} \\
\vect x_t &\sim \mathcal N\LP{\mat F\vect x_{t-1}, \mat \Sigma}
\end{align*}
$$

<small>$\vect u_{it}$ are known firm covariates.</small>


## Hidden Markov model

$$
g_t\LPV{\vect Y_t}{\vect X_t} \qquad f\LPV{\vect X_t}{\vect X_{t-1}}
  \qquad f\LP{\vect X_1}
$$

$$
P\LP{\vect y_{1:d}} = \int f\LP{\vect x_1} g_1\LPV{\vect y_1}{\vect x_1}
    \prod_{t = 2}^d g_t\LPV{\vect y_t}{\vect x_t}f\LPV{\vect x_t}{\vect x_{t - 1}} 
    \diff \vect x_{1:d}
$$

<div class = "left">
All implicitly depend on the parameters in the model. $g_t$s implicitly depend
on $\mat Z_t$ and $\mat U_t$.</diV>


## EM algorithm

$$
\begin{align*}
Q\LPV{\vect\theta}{\vect\theta^{(k)}} &= 
    E\LPV{\log P\LP{\vect y_{1:d}}}{\vect\theta^{(k)}} \\
\log P\LP{\vect y_{1:d}} &= 
    \log f\LP{\vect X_1} + 
    \sum_{t = 2}^d \log \varphi\LP{\vect X_t;\mat F\vect X_{t-1},\mat \Sigma} \\
    &\quad +
    \sum_{t = 1}^d\sum_{i \in R_t}
    y_{it}\log\LP{h^{-1}\LP{\eta_{it}}} + 
    \LP{1 - y_{it}}\log\LP{1 - h^{-1}\LP{\eta_{it}}} \\
\eta_{it}\LP{\vect x_t} &= 
    \vect\beta^\top\vect z_{it} + \vect x_t^\top\vect u_{it}
\end{align*}
$$

<small>where $\varphi$ is a multivariate normal density function, $R_t$ is the 
risk set at time $t$, and $h^{-1}$ is the inverse link function.</small>

## EM algorithm

Need to evaluate

$$
E\LPV{\phi\LP{\vect X_{t-1:t}}}{\vect\theta^{(k)},\vect y_{1:d}}
  \qquad E\LPV{\psi\LP{\vect X_t}}{\vect\theta^{(k)},\vect y_{1:d}}
$$


</section>
<!-- need extra end tag before next section -->
</section>





<!--html_preserve-->
<section>
<section class="titleslide slide level1">
<h1>Talk overview</h1>
<div class = "left">
Importance sampling.

<p class="fragment">Particle filtering.</p> 
<p class="fragment">Two Particle smoothers.</p>
<p class="fragment">The Implementation.</p>
<p class="fragment">Examples.</p>

</div>
</section>
<!-- need extra end tag before next section -->
</section>
<!--/html_preserve-->



<!--html_preserve-->
<section>
<section class="titleslide slide level1">
<h1>Importance sampling</h1>
<!--/html_preserve-->
<div class="left">

Want to approximate **target density** 
$\tilde d(x) = \alpha d(x)$. Only know $d$. 

<div class = "fragment">

1. Sample $x^{(1)},x^{(2)},\dots,x^{(N)}$ from **proposal distribution** with 
density $q$. 
2. Compute unnormalised weights $\tilde W^{(i)} = d(x^{(i)})/q(x^{(i)})$.
3. Normalize weights $W^{(i)} = \tilde W^{(i)} / \sum_{i=1}^N \tilde W^{(i)}$.

</div></div>

<!-- there is a end tag from the previous slide -->
<!-- </section> -->

## Importance sampling
<div class = "left">

Yields a discrete approximation of distribution 

$$\widehat P(X) = \sum_{i=1}^N W^{(i)} \delta_{x^{(i)}}(X)$$
$\delta$ is the Dirac delta function.

<div class="fragment">
$$
E\LP{\phi(X)} = \int \phi(x)\diff x \approx 
    \sum_{i = 1}^N W^{(i)} \phi\LP{x^{(i)}}
$$
</div><div class = "fragment">

Use **effetive sample size** to judge 

$$ESS = \LP{\sum_{i = 1}^N W^{(i)2}}^{-1}$$

```{r load_plotly_mvtnorm, echo = FALSE}
library(plotly)
library(mvtnorm)
```

</div></div>



## Example: Target

```{r target, echo = FALSE, warning=FALSE, message=FALSE, cache = TRUE}
y <- x <- seq(-25, 25, length = 80)
# g1 <- function(x, y) {
#   r <- sqrt(x^2+y^2)
#   o <- 10 * (sin(r)/r - (sin(4.49341) /  4.49341) + .001)
#   o[is.na(o)] <- 1
#   o
# }
# z <- outer(x, y, g1)
# persp(x, y, z, theta = 30, phi = 30, expand = 0.5, col = "lightblue")

# optim(-6, function(x) -sin(x) / x)
g2 <- function(x, y){
  r <- sqrt(x^2+y^2)
  o <- 10 * (sin(r)/r - (sin(4.49341) /  4.49341) + .001)
  o[is.na(o)] <- 1
  
  is_local_max <- 7.725293
  is_big <- abs(r) >= is_local_max
  o[is_big] <- exp(-(r[is_big] - is_local_max) / 2) * o[is_big]
  
  o
}
z <- outer(x, y, g2)

cn <-  list(c(0.0, "rgb(192,192,192)"), list(1, "rgb(0, 0, 0)"))
con <- list(
  x = list(highlight = FALSE), 
  y = list(highlight = FALSE), 
  z = list(highlight = FALSE))
plot_ly(showscale = FALSE, color = I("black")) %>% 
  add_surface(
    colorscale = cn, opacity = .9,
    z = matrix(z, length(x), length(y)), x = x, y = y, 
    hoverinfo = "none", contours = con) %>%
  layout(
    scene=list(
      xaxis=list(title = "x1", range = c(-12, 12), showspikes = FALSE),
      yaxis=list(title = "x2", range = c(-12, 12), showspikes = FALSE),
      zaxis=list(title = "d", showspikes = FALSE)), 
    plot_bgcolor = 'rgb(0, 0, 0, 0)', paper_bgcolor = 'rgba(0,0,0,0)') %>% 
  config(displayModeBar = FALSE)
# persp(x, y, z, theta = 30, phi = 30, expand = 0.5)
```

## Example: Proposal

```{r 1.5_proposal, echo = FALSE, warning=FALSE, message=FALSE, cache = TRUE, dependson="first_sample"}
sig1 <- diag(7^2, 2)
dens <- matrix(dmvnorm(expand.grid(x, y), sigma = sig1), ncol = length(x))
plot_ly(showscale = FALSE, color = I("black")) %>% 
  add_surface(z = dens, x = x, y = y, colorscale = cn, hoverinfo = "none",
              opacity = .9, contours = con) %>%
  layout(
    scene = list(
      xaxis=list(title = "x1", range = c(-10, 10), showspikes = FALSE),
      yaxis=list(title = "x2", range = c(-10, 10), showspikes = FALSE),
      zaxis=list(title = "density", showspikes = FALSE)), 
    plot_bgcolor = 'rgb(0, 0, 0, 0)', paper_bgcolor = 'rgba(0,0,0,0)') %>% 
  config(displayModeBar = FALSE)
```

## Example: Importance sampling

```{r first_sample, echo = FALSE, warning=FALSE, message=FALSE, cache = TRUE, dependson="target"}
get_smp <- function(si, n){
  smp <- rmvnorm(n, sigma = si)
  wt <- apply(smp, 1, function(x) log(g2(x[1], x[2]))) -
    apply(smp, 1, dmvnorm, sigma = si, log = TRUE)
  co <- max(wt)
  co <- co + log(sum(exp(wt - co)))
  wt <- exp(wt - co)
  
  list(smp = smp, wt = wt)
}

. <- function(si, n){
  list2env(get_smp(si, n), environment())
  
  smp2 <- rbind(smp, smp, smp)
  smp2[(2L * n + 1L):(3L * n), ] <- NA
  wt2 <- c(wt, rep(0, n), rep(NA_real_, n))
  idx <- c(sapply(1:n, function(i) c(i, i + n, i + 2L * n)))
  smp2 <- smp2[idx, ]
  wt2 <- wt2[idx]
    
  zn <- .8 * max(wt) * (z - min(z)) / (max(z) - min(z))
  
  rg <- max(abs(smp), 10)
  rg <- c(-rg, rg)
  
  p <- plot_ly(showlegend = F, color = I("black")) %>%
    add_surface(
      z = matrix(zn, length(x), length(y)), 
      colorscale = cn,
      x = x, y = y, showscale = FALSE, hoverinfo = "none", opacity = .5, 
      contours = con) %>% 
    add_markers(x = smp[, 1], y = smp[, 2], z = wt, hoverinfo = "none") %>%
    add_paths(x = smp2[, 1], y = smp2[, 2], z = wt2, hoverinfo = "none") %>%
    layout(
      scene = list(
        xaxis=list(title = "x1", range = rg, showspikes = FALSE),
        yaxis=list(title = "x2", range = rg, showspikes = FALSE),
        zaxis=list(title = "Importance weight", showspikes = FALSE)), 
      plot_bgcolor = 'rgb(0, 0, 0, 0)', paper_bgcolor = 'rgba(0,0,0,0)')  %>% 
    config(displayModeBar = FALSE)
  
  list(p = p, ess = sum(wt^2)^-1)
}

set.seed(37219838)
n <- 50L
o <- .(sig1, n)
o$p
```

Effective sample size is `r o$ess` with `r n` samples.

## Example: Proposal 

```{r 2_proposal, echo = FALSE, warning=FALSE, message=FALSE, cache = TRUE, dependson="first_sample"}
sig2 <- diag(2^2, 2)
dens <- matrix(dmvnorm(expand.grid(x, y), sigma = sig2), ncol = length(x))
plot_ly(showscale = FALSE, color = I("black")) %>% 
  add_surface(z = dens, x = x, y = y, colorscale = cn, hoverinfo = "none",
              opacity = .9, contours = con) %>%
  layout(
    scene = list(
      xaxis=list(title = "x1", range = c(-10, 10), showspikes = FALSE),
      yaxis=list(title = "x2", range = c(-10, 10), showspikes = FALSE),
      zaxis=list(title = "density", showspikes = FALSE)), 
    plot_bgcolor = 'rgb(0, 0, 0, 0)', paper_bgcolor = 'rgba(0,0,0,0)') %>% 
  config(displayModeBar = FALSE)
```

## Example: Importance sampling

```{r second_sample, echo = FALSE, warning=FALSE, message=FALSE, cache = TRUE, dependson="first_sample"}
set.seed(37219840)
o <- .(sig2, n)
o$p
```

Effective sample size is `r o$ess` with `r n` samples.

```{r check_sampling, echo = FALSE, eval = FALSE}
#####
# double check result
sm <- replicate(
  10, {
    tmp <- get_smp(sig, 100000)
    with(tmp, weighted.mean(smp[, 1]^2, w = wt))    
  })
mean(sm)
sm

library(cubature)
norm_const <- adaptIntegrate(
  function(z) g2(z[1], z[2]), lowerLimit = c(-25, -25), upperLimit = c(25, 25), 
  tol = 1e-7)
v <- adaptIntegrate(
  function(z) z[1]^2 * g2(z[1], z[2]), tol = 1e-7,
  lowerLimit = c(-25, -25), upperLimit = c(25, 25))
v$integral / norm_const$integral
```

## Requirements 

$$
d(x) > 0 \Rightarrow q(x) > 0
$$

$$
\frac{d(x)}{q(x)} < C < \infty
$$

## Relating to Hiden Markov models

<div class = "left">

Need

$$
P\LPV{\vect X_1}{\vect y_1} = 
    \frac{g_t\LPV{\vect y_1}{\vect x_1}f\LP{\vect x_1}}{P\LP{\vect y_1}}
$$

<div class = "fragment">

Use importance sampling to get 

$$
\widehat P\LPV{\vect X_1}{\vect y_1} = 
    \sum_{i = 1}^N \PF{W}{1}{i}\delta_{\PF{\vect x}{1}{i}}\LP{\vect X_1}
$$

</div></div>


## Notation and goal
<div class = "left">
"Forward" particle clouds 
$\{\PF{W}{t}{i}, \PF{\vect x}{t}{i}\}_{i=1,\dots,N}$.

<p class = "fragment">"Backward" particle clouds 
$\{\PB{W}{t}{i}, \PB{\vect x}{t}{i}\}_{i=1,\dots,N}$.</p>

<p class = "fragment">"Smoothed" particle clouds
$\{\PS{W}{t}{i}, \PS{\vect x}{t}{i}\}_{i=1,\dots,N}$.</p>

<div class = "fragment">End goal is 
$$
E\LPV{\phi(\vect X_t)}{\vect y_{1:d}} \approx 
    \sum_{i = 1}^N \PS{W}{t}{i} \phi\LP{\PS{\vect x}{t}{i}}
$$

</div></div>

<!--html_preserve-->
</section>
<!-- need extra end tag before next section -->
</section>
<!--/html_preserve-->




<!--html_preserve-->
<section>
<section class="titleslide slide level1">
<h1>Particle filtering</h1>
<!--/html_preserve-->
<div class="left">

Also known as sequential Monte Carlo.

<p class="fragment">Use $\widehat P\LPV{\vect X_{1:t-1}}{\vect y_{1:t-1}}$ in</p>

<div class="fragment">

$$
\begin{align*}
P\LPV{\vect x_{1:t}}{\vect y_{1:t}} &= 
  P\LPV{\vect x_{1:t-1}}{\vect y_{1:t-1}} \frac{
  g_t\LPV{\vect y_t}{\vect x_t}f\LPV{\vect x_t}{\vect x_{t-1}}
  }{P\LPV{\vect y_t}{\vect y_{1:t -1}}} \\
&\approx \sum_{i = 1}^N \PF{W}{t-1}{i}
  \delta_{\PF{\vect x}{1:{t-1}}{i}}\LP{\vect x_{1:t-1}}
  \frac{
  g_t\LPV{\vect y_t}{\vect x_t}f\LPV{\vect x_t}{\PF{\vect x}{t-1}{i}}
  }{P\LPV{\vect y_t}{\vect y_{1:t -1}}}
\end{align*}
$$
</div></div>
<!-- there is a end tag from the previous slide -->
<!-- </section> -->

## Sequential Importance Sampling

<!--html_preserve-->
<small><ol type = "1">
<li> Sample
<!--/html_preserve-->

$$
\PF{\vect x}{t}{i} \sim 
  \overrightarrow q_t\LPV{\cdot}{\PF{\vect x}{t-1}{i},\vect y_t}
$$
<!--html_preserve-->
</li><li class = "fragment">Compute and normalize weights
<!--/html_preserve-->

$$
\begin{align*}
\PF{\tilde W}{t}{i} &= \PF{W}{t-1}{i}
  \frac{
  g_t\LPV{\vect y_t}{\PF{\vect x}{t}{i}}f\LPV{\PF{\vect x}{t}{i}}{\PF{\vect x}{t-1}{i}}
  }{\overrightarrow q_t\LPV{\PF{\vect x}{t}{i}}{\PF{\vect x}{t-1}{i},\vec y_t}}\\
\PF{W}{t}{i} &= \PF{\tilde W}{t}{i} / \sum_{i = 1}^N\PF{\tilde W}{t}{i}
\end{align*}
$$

<!--html_preserve-->
</li></ol></small>
<!--/html_preserve-->


## Weight degeneracy

<div class="left">

Weights, $\PF{W}{t}{i}$s, can degenerate.

<div class="fragment">

Use auxiliary particle filter [@Pitt99] and resample with weights that are 
ideally

$$
\PF{\beta}{t}{i}\propto P\LPV{\vec y_t}{\PF{\vect x}{t-1}{i}}\PF{W}{t-1}{i}
$$
</div>

<p class="fragment">
Setting $\PF{\beta}{t}{i} = \PF{W}{t-1}{i}$ yields the sequential importance 
resampling algorithm.</p>

<small class = "fragment">Different resampling approaches can be used. See 
@Douc05.</small>

</div>

## Auxiliary particle filter

<!--html_preserve-->
<small><ol type = "1">
<li> 
<!--/html_preserve-->

Sample $j_1,\dots,j_N$ indices using $\{\PF{\beta}{t}{i}\}_{i=1,\dots,n}$.

<!--html_preserve-->
</li><li class="fragment">Sample
<!--/html_preserve-->

$$
\PF{\vect x}{t}{i} \sim 
  \overrightarrow q_t\LPV{\cdot}{\PF{\vect x}{t-1}{j_i},\vec y_t}
$$
<!--html_preserve-->
</li><li class = "fragment">Compute and normalize weights
<!--/html_preserve-->

$$
\begin{align*}
\PF{\tilde W}{t}{i} &= 
  \frac{\PF{W}{t-1}{j_i}}{\PF{\beta}{t}{j_i}}
  \frac{
  g_t\LPV{\vect y_t}{\vect x_t}f
  \LPV{\PF{\vect x}{t}{i}}{\PF{\vect x}{t-1}{j_i}}
  }{\overrightarrow q_t\LPV{\PF{\vect x}{t}{i}}{\PF{\vect x}{t-1}{j_i},\vec y_t}}\\ 
\PF{W}{t}{i} &= \PF{\tilde W}{t}{i} / \sum_{i = 1}^N\PF{\tilde W}{t}{i}
\end{align*}
$$

<!--html_preserve-->
</li></ol></small>
<!--/html_preserve-->


<!--html_preserve-->
</section>
<!-- need extra end tag before next section -->
</section>
<!--/html_preserve-->






<!--html_preserve-->
<section>
<section>
<h1>Smoothing</h1>
<!--/html_preserve-->
<div class = "left">
Need $P\LPV{\vect X_t}{\vect y_{1:d}}$ and 
$P\LPV{\vect X_{t-1:t}}{\vect y_{1:d}}$.

Use generalized two filter smoothing formula from @Briers09 and smoother from 
@Fearnhead10.


</div>
<!-- there is a end tag from the previous slide -->
<!-- </section> -->

## Smoother from Briers et al. (2009)
<div class = "left">

Use the identity 

$$
\begin{align*}
P\LPV{\vect x_t}{\vect y_{1:d}} &= \frac{
  P\LPV{\vect x_t}{\vect y_{1:t -1}}
  P\LPV{\vect y_{t:d}}{\vect x_t}}{
  P\LPV{\vect y_{t:d}}{\vect y_{1:t-1}}} \\
&\propto \int f\LPV{\vect x_t}{\vect x_{t-1}}
  P\LPV{\vect x_{t-1}}{\vect y_{1:t -1}}\diff \vect x_{t-1}
  P\LPV{\vect y_{t:d}}{\vect x_t}
\end{align*}
$$

<div class = "fragment">
Need to approximate 
$$
P\LPV{\vect y_{t:d}}{\vect x_t} = \
  \int \prod_{k = t + 1}^d f\LPV{\vect x_k}{\vect x_{k-1}}
  \prod_{k =t}^d g\LPV{\vect y_k}{\vect x_k}\diff \vect x_{t + 1:d}
$$
</div></div>

## Backward filter
<div class="left">

Let

$$
\gamma_1\LP{\vect x}, \quad 
  \gamma_t(\vect x) = \int f\LPV{\vect x}{\vect x_{t-1}}
  \gamma_{t-1}\LP{\vect x_{t-1}}\diff\vect x_{t-1}
$$
<div class="fragment">
Then we introduce artificial probability densities 

$$
\tilde P\LPV{\vect x_{t:d}}{\vect y_{1:d}} =
  \frac{
    \gamma_t\LP{\vect x_t}
    \prod_{k = t + 1}^d f\LPV{\vect x_k}{\vect x_{k-1}}
    \prod_{k =t}^d g\LPV{\vect y_k}{\vect x_k}
  }{\tilde P\LP{\vect y_{t:d}}}
$$
</div>

<p class = "fragment">We can make a backward filter to sample 
$\{\PB{\vect x}{t}{i}, \PB{W}{t}{i} \}_{i = 1, \dots, N}$ from 
$\tilde P\LPV{\vect x_{t}}{\vect y_{1:d}}$.</p>

</div>

## Combining filters

<div class = "left">

We can show that 

$$
\begin{align*}
P\LPV{\vect y_{t:d}}{\vect x_t} &= 
  \tilde P\LP{\vect y_{t:d}} 
  \frac{\tilde P\LPV{\vect x_t}{\vect y_{t:d}}}{
  \gamma_t\LP{\vect x_t}} \\
&\propto \frac{\tilde P\LPV{\vect x_t}{\vect y_{t:d}}}{
  \gamma_t\LP{\vect x_t}}
  \approx \sum_{i = 1}^N \frac{\PB{W}{t}{i}}{\gamma_t(\vect x_t)}
  \delta_{\PB{\vect x}{t}{i}}(\vect x_t)
\end{align*}
$$
</div>

## Combining filters
<div class = "left">
Now, we have the approximations we need 

$$
\begin{align*}
P\LPV{\vect x_t}{\vect y_{1:d}} &\propto 
  \int f\LPV{\vect x_t}{\vect x_{t-1}}
  P\LPV{\vect x_{t-1}}{\vect y_{1:t -1}}\diff \vect x_{t-1}
  P\LPV{\vect y_{t:d}}{\vect x_t} \\
&\approx 
  \sum_{j=1}^N \PS{W}{t}{j}\delta_{\PB{\vect x}{t}{j}}\LP{\vect x_t} \\
\PS{W}{t}{j} &\propto 
  \sum_{i = 1}^N
  f\LPV{\PB{\vect x}{t}{j}}{\PF{\vect x}{t-1}{i}}\PF{W}{t-1}{i}
  \frac{\PB{W}{t}{j}}{\gamma_t\LP{\PB{\vect x}{t}{j}}}  
\end{align*}
$$

</div>



## Comments

<div class = "left">

Smoothing step is $\bigO{N^2}$ but independent of $n_t = \lvert R_t \rvert$.

<p class="fragment">
Can be reduced with approximate methods in @Klaas06. Yields $\bigO{N\log N}$ run times. 
Alternatively, use rejection sampling if 
$f\LPV{\vect x_t}{\vect x_{t-1}} / \gamma_t\LP{\vect x_t}$ is bounded.</p>

<p class="fragment">
Cannot handle singular components.</p>
</div>

## Smoother from Fearnhead et al. (2010)
<div class = "left">
Use the identity 

$$
\begin{align*}
P\LPV{\vect x_t}{\vect y_{1:d}} 
&= \frac{
  P\LPV{\vect x_t}{\vect y_{1:t -1}}
  P\LPV{\vect y_{t:d}}{\vect x_t}}{
  P\LPV{\vect y_{t:d}}{\vect y_{1:t-1}}}  \\
&= \frac{
  P\LPV{\vect x_t}{\vect y_{1:t -1}}
  g_t\LPV{\vect y_t}{\vect x_t}
  P\LPV{\vect y_{t+1:d}}{\vect x_t}}{
  P\LPV{\vect y_{t:d}}{\vect y_{1:t-1}}} \\
&\propto
  \int f\LPV{\vect x_t}{\vect x_{t-1}}
  P\LPV{\vect x_{t-1}}{\vect y_{1:t -1}}\diff \vect x_{t-1}
  g_t\LPV{\vect y_t}{\vect x_t} \cdot \\
&\qquad\int P\LPV{\vect y_{t+1}}{\vect x_{t+1}}
  f\LPV{\vect x_{t + 1}}{\vect x_t}\diff \vect x_{t+1}
\end{align*}
$$

</div>

## Smoother from Fearnhead et al. (2010)
<!--html_preserve-->
<small><div class = "left">
<!--/html_preserve-->

1. Sample $(i_k,j_k)_{k=1,\dots,N}$ with weights $\PF{\beta}{t-1}{i}$ and
$\PB{\beta}{t+1}{k}$.
2. Sample $\PS{\vect x}{t}{k} \sim \overleftrightarrow q_t\LPV{\cdot}{\PF{\vect x}{t-1}{i_k}, \PB{\vect x}{t+1}{j_k}, \vect y_t}$. 
3. Compute and normalize weights

$$
\begin{align*}
\PS{\tilde W}{t}{k} & =\frac{
  f\LPV{\color{vermilion}\PS{\vect x}{t}{k}}{\color{blugre}\PF{\vect x}{t-1}{i_k}}
  \color{vermilion}g_t\LPV{\vect y}{\PS{\vect x}{t}{k}}
  \color{black}f\LPV{\color{cora}\PB{\vect x}{t+1}{j_k}}{
    \color{vermilion}\PS{\vect x}{t}{k}}
}{
  \overleftrightarrow q_t\LPV{\PS{\vect x}{t}{k}}{\PF{\vect x}{t-1}{i_k}, \PB{\vect x}{t+1}{j_k}, \vect y_t}
}\frac{
  \color{cora}{\PB{W}{t+1}{j_k}}
  \color{blugre}{\PF{W}{t-1}{i_k}}
}{
  \color{cora}\gamma_{t+1}\LP{\PB{\vect x}{t+1}{j_k}}
  \color{black}\PB{\beta}{t+1}{j_k}\PF{\beta}{t-1}{i_k}
} \\
\PS{W}{t}{k} &= \PS{\tilde W}{t}{k} / \sum_{i = 1}^N\PS{\tilde W}{t}{k}
\end{align*}
$$


<div class="fragment">
Compare with 

$$
\begin{align*}
P\LPV{\vect x_t}{\vect y_{1:d}} 
&= 
  \color{blugre}\int
  \color{black}f\LPV{\color{vermilion}\vect x_t}{\color{blugre}\vect x_{t-1}}
  \color{blugre}P\LPV{\vect x_{t-1}}{\vect y_{1:t -1}}\diff \vect x_{t-1}
  \color{vermilion}g_t\LPV{\vect y_t}{\vect x_t} \cdot \\
&\qquad\color{cora}\int P\LPV{\vect y_{t+1}}{\vect x_{t+1}}
  \color{black}f\LPV{\color{cora}\vect x_{t + 1}}{\color{vermilion}\vect x_t}
  \color{cora}\diff \vect x_{t+1}
\end{align*}
$$


</div></div></small>


## Comments

<div class = "left">

Smoothing step is $\bigO{N}$ but $\bigO{n_tp}$ where $n_t = \lvert R_t \rvert$
and $p$ is the dimension of $\vect X_t$.

<p class="fragment">
Can generalize to handle singular components.</p>

</div>

<!--html_preserve-->
</section>
<!-- need extra end tag before next section -->
</section>
<!--/html_preserve-->





<!--html_preserve-->
<section>
<section>
<h1>Implementation</h1><div class = "left">
<!--/html_preserve-->

Implemented in the `dynamichazard` package. 

<p class = "fragment">
Most parts are done in parallel with OpenMP or the `thread` library in C++.</p>

<!--html_preserve-->
<div class = "fragment no-top-marg-next"> 
<!--/html_preserve-->
Sampling is not done in parallel. Does not matter if $n_t = \lvert R_t \rvert$
is large. 

<small>
Can be done in parallel e.g., as in @Zhou15 or using the software from @Ecuyer02.</small>
</div></div>

<!-- there is a end tag from the previous slide -->
<!-- </section> -->

## M-step
<div class = "left">
Maximizing with respect to $\vect \beta$ is expensive for moderate amount of 
fixed effects

<!--html_preserve--><small><!--/html_preserve-->

$$
\begin{align*}
& E\LPV{
    \sum_{t = 1}^d\sum_{i \in R_t}
      y_{it}\log\LP{h^{-1}\LP{\eta_{it}}} + 
      \LP{1 - y_{it}}\log\LP{1 - h^{-1}\LP{\eta_{it}}}
  }{\vect \theta^{(k)}} \\
& \eta_{it}\LP{\vect x_t} = 
    \vect\beta^\top\vect z_{it} + \vect x_t^\top\vect u_{it}
\end{align*}
$$

<!--html_preserve--></small><!--/html_preserve-->

<p class = "fragment">
E.g., $(n_t, N, d) = (1000, 250, 400)$ then it amounts to a GLM with 
`r 1000L * 250L * 400L` observations.
</p>

</diV>

## M-step

<div class = "left">

Take only one iteratively re-weighted least squares iteration.

<div class = "fragment no-top-marg-next">
Do as @Wood15 to compute a QR decomposition in parallel with low memory 
requirements.

<small>Implemented in the `bam` function in the `mgcv` package.</small>
</div>

<p class = "fragment">May be able to reduce computation time further by using that
linear predictors only differ by $\vect x_t^\top\vect u_{it}$.</p>
</div>


## Proposal distribution
<div class = "left">
Need up to three proposal distribution: 

$$
\begin{align*}
\PF{\vect x}{t}{i} &\sim 
  \overrightarrow q_t\LPV{\cdot}{\PF{\vect x}{t-1}{i},\vect y_t} \\
\PB{\vect x}{t}{i} &\sim 
  \overleftarrow q_t\LPV{\cdot}{\PB{\vect x}{t+1}{i},\vect y_t} \\
\PS{\vect x}{t}{k} &\sim 
  \overleftrightarrow q_t\LPV{\cdot}{\PF{\vect x}{t-1}{i_k}, \PB{\vect x}{t+1}{j_k}, \vect y_t}
\end{align*}
$$

Focus on $\overrightarrow q_t\LPV{\cdot}{\PF{\vect x}{t-1}{i},\vect y_t}$.

</div>


## Proposal distribution
<div class = "left">
<!--html_preserve-->
<div class = "no-top-marg-next"> 
<!--/html_preserve-->
Use $f$ as proposal distribution. 

<small>**Bootstrap filter** from @Gordon93.</small></div>

<div class = "fragment">
Estimate the mode 

$$
\vect \mu_t^{(i)} = \argmax_{\vect x_t}
  g_t\LPV{\vect y_t}{\vect x_t}f\LPV{\vect x_t}{\PF{\vect x}{t-1}{i}}
$$

<!--html_preserve-->
<div class = "no-top-marg-next"> 
<!--/html_preserve-->
and make a Taylor expansion around $\vect \mu_t^{(i)}$.

<small>Then use a multivariate normal distribution or multivariate 
t-distribution as suggested in @Pitt99 and used in @Fearnhead10.</small></div>

</div>

<p class = "fragment">
Is $\bigO{Nn_tp^2}$ where $p$ is the dimension of $\vect x_t$.</p>

</diV>

## Proposal distribution
<div class="left">
Instead make the expansion once using

$$
\bar{\vect x}_{t -1} = \sum_{i = 1}^N \PF{W}{t - 1}{i}\PF{\vect x}{t - 1}{i}
$$

Reduces cost to $\bigO{Nn_tp + n_tp^2}$.
</div>

<!--html_preserve-->
</section>
<!-- need extra end tag before next section -->
</section>
<!--/html_preserve-->




<!--html_preserve-->
<section>
<section>
<h1>Example</h1>
<!--/html_preserve-->

```{r show_frm}
frm_fixed
Fmat <- diag(c(0.948, 0.9793))
Q. <- matrix(c(0.05979 , 0.011014, 
               0.011014, 0.002504), byrow = TRUE, 2)
```

```{r set_fix, echo = FALSE}
# too long so did not show this
fix_e <- c(
  -10.15376, 0.08497, -1.51364, 1.14573, -0.13661, 44.42375, -1.32413, 
  0.03975, -0.37105, 0.50640, -4.27648, 1.82763, -0.78028, 0.59698, 
  1.31762, -1.58914, -2.27547)
```

$$
\begin{align*}
h\LP{E\LPV{y_{it}}{\vect z_t, \vect x_t, \vect u_{it}}} &= 
  \vect\beta^\top\vect z_{it}  + \vect x_t^\top\vect u_{it} \\
\vect x_t &\sim \mathcal N\LP{\mat F\vect x_{t-1}, \mat \Sigma}
\end{align*}
$$

<!-- there is a end tag from the previous slide -->
<!-- </section> -->

## Bootstrap filter

```{r boot, cache = 1}
library(dynamichazard)
set.seed(58201195)
system.time(
  f1 <- PF_EM(
    fixed = frm_fixed, random = ~ 1 + wz(rel_size), 
    data = distress, model = "logit", type = "VAR", 
    Fmat = Fmat, Q = Q., fixed_effects = fix_e,
    control = PF_control(
      N_fw_n_bw = 1000L, N_smooth = 2500L, N_first = 3000L, 
      N_smooth_final = 500L, n_threads = 4L, n_max = 1L, nu = 8L,
      method = "bootstrap_filter", smoother = "Fearnhead_O_N"),
    by = 1L, max_T = max(distress$tstop), id = distress$gvkey,
    Q_0 = diag(1, 2)))

logLik(f1)       # from particle filter
logLik(base_fit) # glm model w/o random effects
```

## Bootstrap filter: Effective sample size

```{r ess_boot}
par(mar = c(5, 4, 1, 1))
plot(head(f1$effective_sample_size$smoothed_clouds, -1), type = "h", 
     ylab = "Effective sample size", xlab = "Time")
```

## Bootstrap filter: Predicted state variables

```{r plot_boot, fig.width = 9}
par(mfcol = c(1, 2), mar = c(5, 4, 1, 1))
plot(f1, cov_index = 1, qlvls = c(.05, .95), qtype = "lines")
plot(f1, cov_index = 2, qlvls = c(.05, .95), qtype = "lines")
```

## Auxiliary particle filter with different proposal

<!-- 
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache(
      "aux_fearn", path = paste0("cache", .Platform$file.sep))
-->

```{r aux_fearn, cache = 1}
set.seed(58201195)
system.time(
  f2 <- PF_EM(
    fixed = frm_fixed, random = ~ 1 + wz(rel_size), 
    data = distress, model = "logit", type = "VAR", 
    Fmat = Fmat, Q = Q., fixed_effects = fix_e,
    control = PF_control(
      N_fw_n_bw = 1000L, N_smooth = 2500L, N_first = 3000L, 
      N_smooth_final = 500L, n_threads = 4L, n_max = 1L, nu = 8L,
      method = "AUX_normal_approx_w_cloud_mean", smoother = "Fearnhead_O_N"),
    by = 1L, max_T = max(distress$tstop), id = distress$gvkey,
    Q_0 = diag(1, 2)))

logLik(f2)
```

## Auxiliary particle filter with different proposal

```{r ess_fearn}
par(mar = c(5, 4, 1, 1))
plot(head(f2$effective_sample_size$smoothed_clouds - 
            f1$effective_sample_size$smoothed_clouds, -1), type = "h", 
     ylab = "Diff effective sample size", xlab = "Time")
```

## Auxiliary particle filter with different proposal

```{r plot_fearn, fig.width = 9}
par(mfcol = c(1, 2), mar = c(5, 4, 1, 1))
plot(f2, cov_index = 1, qlvls = c(.05, .95), qtype = "lines")
plot(f2, cov_index = 2, qlvls = c(.05, .95), qtype = "lines")
```

## Different smoother

```{r brier, cache = 1}
set.seed(58201195)
system.time(
  f3 <- PF_EM(
    fixed = frm_fixed, random = ~ 1 + wz(rel_size), 
    data = distress, model = "logit", type = "VAR", 
    Fmat = Fmat, Q = Q., fixed_effects = fix_e,
    control = PF_control(
      N_fw_n_bw = 500L, N_smooth = 1L, N_first = 3000L, n_threads = 4L, 
      n_max = 1L, nu = 8L,
      method = "AUX_normal_approx_w_cloud_mean", smoother = "Brier_O_N_square"),
    by = 1L, max_T = max(distress$tstop), id = distress$gvkey,
    Q_0 = diag(1, 2)))

logLik(f3)
```

<div class = "left">
The `PF_forward_filter` function can be used to get more precise log-likelihood
estimate.</div>

## Different smoother

```{r ess_brier}
par(mar = c(5, 4, 1, 1))
plot(head(f3$effective_sample_size$smoothed_clouds - 
            f2$effective_sample_size$smoothed_clouds, -1), type = "h", 
     ylab = "Diff effective sample size", xlab = "Time")
```


## Auxiliary particle filter with different proposal

<!--
  knitr::opts_knit$set(output.dir = ".")
  knitr::load_cache(
      "pf_taylor_aux_fearn", path = paste0("cache", .Platform$file.sep))
-->

```{r pf_taylor_aux_fearn, cache = 1}
set.seed(58201195)
system.time(
  f4 <- PF_EM(
    fixed = frm_fixed, random = ~ 1 + wz(rel_size), 
    data = distress, model = "logit", type = "VAR", 
    Fmat = Fmat, Q = Q., fixed_effects = fix_e,
    control = PF_control(
      N_fw_n_bw = 1000L, N_smooth = 2500L, N_first = 3000L, 
      N_smooth_final = 500L, n_threads = 4L, n_max = 1L, nu = 8L,
      method = "AUX_normal_approx_w_particles", smoother = "Fearnhead_O_N"),
    by = 1L, max_T = max(distress$tstop), id = distress$gvkey,
    Q_0 = diag(1, 2)))

logLik(f4)
```

## Auxiliary particle filter with different proposal

```{r ess_pf_taylor_aux_fearn}
par(mar = c(5, 4, 1, 1))
plot(head(f4$effective_sample_size$smoothed_clouds - 
            f2$effective_sample_size$smoothed_clouds, -1), type = "h", 
     ylab = "Diff effective sample size", xlab = "Time")
```

## Auxiliary particle filter with different proposal

```{r diff_ess_pf_taylor_aux_fearn}
sapply(f4$effective_sample_size, mean) - sapply(f2$effective_sample_size, mean)
```



<!--html_preserve-->
</section>
<!-- need extra end tag before next section -->
</section>
<!--/html_preserve-->




<section>
<section>
<h1>Next steps</h1>
<div class = "left">
<!-- there is a end tag from the previous slide -->
<!-- </section> -->

Different $y_{it}$ and distributions $g_t\LPV{\vect y_t}{\vect x_t}$.

<div class = "fragment no-top-marg-next">
Other types of state models.

<small>$\text{VAR}(q)$ with $q > 1$, $\text{VARMA}(p,q)$, restricted models,
etc.</small></div>

<div class = "fragment no-top-marg-next">
Compute observed information matrix.

<small>E.g., see @Cappe05 [chapter 11] and @Poyiadjis11.</small></div>

<div class = "fragment no-top-marg-next">
Change particle filter and smoothers.


<small>Different proposal distributions (e.g., unscented transformation and 
adaptive importance sampling), use resample-moves, use block sampling, and 
more.</small></div>

<p class = "fragment">"Nicer" API.</p>

<p class = "fragment">Better implementation of the smoother from @Briers09.</p>
<div>

<!--html_preserve-->
</section>
<!-- need extra end tag before next section -->
</section>
<!--/html_preserve-->


<!--html_preserve-->
<section>
<section class="end">
<h1>Thank you!</h1>

<small><p>Slides are on <a href="https://github.com/boennecd/Talks">github.com/boennecd/Talks</a> and
<a href="http://rpubs.com/boennecd/KU-PF-18">rpubs.com/boennecd/KU-PF-18</a>.</p>

<p>More examples at <a href="https://github.com/boennecd/dynamichazard/tree/master/examples">
github.com/boennecd/dynamichazard/tree/master/examples</a>.</p>

<!--/html_preserve-->

@Doucet11 and @Givens12 [chapter 6] has been used in preparation of the slides.

<!--html_preserve-->
</small></section>
</section>
<!--/html_preserve-->



<!--html_preserve-->
<section class="titleslide slide level1">
<h1>References</h1>
<!--/html_preserve-->

