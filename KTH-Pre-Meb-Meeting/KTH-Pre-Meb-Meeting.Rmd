---
title: "Mixed Effect Models for Pedigree Data"
bibliography: ref.bib
biblio-style: apa
output: 
  revealjs::revealjs_presentation:
    css: styles.css
    theme: black
    center: false
    transition: slide
    highlight: monochrome
    self_contained: true
    reveal_options:
      slideNumber: true
    includes:
      in_header: header.html
      after_body: doc_suffix.html
---

## dummy slide

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.height = 5, cache.path = "cache/")
.par_use <- list(cex = 1.33, cex.lab = 1.2)
options(digits = 3, 
        knitr.kable.NA = '')
source(file.path("R", "pedigree-util.R"))
```

<!--html_preserve-->
<script>
(function() {
  document.getElementById("dummy-slide").remove(); 
  
  var front_div = document.getElementsByTagName("section")[0];
  front_div.classList.add("front");
  front_div.classList.add("center");
  
  // add second header
  var second_head = document.createElement("p");
  var node = document.createTextNode("Application of Randomized Quasi-Monte Carlo");
  second_head.appendChild(node);
  second_head.style.margin = "0";
  front_div.appendChild(second_head);
  
  // add author 
  var credit_div = document.createElement('div');
  credit_div.innerHTML += "<div class='w-small'><p>Benjamin Christoffersen</p><p class='smallish'>KI, Department of Medical Epidemiology and Biostatistics, <a href='mailto:benjamin.christoffersen@ki.se'>benjamin.christoffersen@ki.se</a></p><p class='smallish'>KTH, Division of Robotics, Perception and Learning, <a href='mailto:benchr@kth.se'>benchr@kth.se</a></p></div>";
  credit_div.classList.add("authors");
  front_div.appendChild(credit_div);
})();
</script>
<!--end dummy slide-->

</section>

<section>
<section class="large-first center slide level2">
<h1>Motivation</h1>
<!--/html_preserve-->

<div style="display: none;">
$$
\renewcommand\vec{\boldsymbol}
\def\bigO#1{\mathcal{O}(#1)}
\def\Cond#1#2{\left(#1\,\middle|\, #2\right)}
\def\mat#1{\boldsymbol{#1}}
\def\der{{\mathop{}\!\mathrm{d}}}
\def\argmax{\text{arg}\,\text{max}}
\def\Prob{\text{P}}
\def\Expec{\text{E}}
\def\logit{\text{logit}}
\def\diag{\text{diag}}
$$
</div>

<!--html_preserve-->
</section>
<section class="center-horiz">
<h2>Example</h2>
<!--/html_preserve-->

```{r setup_ex_dat, echo = FALSE}
library(kinship2, quietly = TRUE)
ex_dat <- pedigree(
  id = 1:10, 
  dadid = c(NA, NA, 1L, NA, NA, 1L, 3L, 3L, 5L, 5L), 
  momid = c(NA, NA, 2L, NA, NA, 2L, 4L, 4L, 6L, 6L), 
  sex = c(1:2, 1:2, 1:2, 1L, 1L, 1L, 1L))
plot(ex_dat)
```

<div class = "w-small">
Suppose that the above is family $i$ for which we 
observe $Y_{i1},\dots Y_{i10} \in \{0, 1\}$.
<p class = "smallish">
Circles are females and squares are male.</p>
</div>

## Research Questions
Want to estimate unobserved genetic effects, environmental effects, 
paternal effects, etc. for binary outcomes.

<p class = "fragment">
Can be done by extending GLM's with a probit link.</p>

## Logistic Regression
Have $i = 1,\dots,m$ families (clusters) with $j = 1,\dots,n_i$ members.

<div class = "fragment">
Logistic regression can be seen as 

$$
Y_{ij} = \begin{cases}
  1 & \vec x_{ij}^\top\vec\beta + \epsilon_{ij} > 0 \\
  0 & \text{otherwise}
\end{cases}
$$

where $\epsilon_{it}$ follows a logistic distribution. Implies

$$
\text{logit}(P(Y_{ij} = 1)) = \vec x_{ij}^\top\vec\beta.
$$
</div>

## Probit Model
If we instead assume that $\epsilon_{ij} \sim N(0, 1)$ then 

$$
\Phi(P(Y_{ij} = 1)) = \vec x_{ij}^\top\vec\beta.
$$

<p class = "fragment">
Generalizing this is computationally attractive.</p>

## Extending the Probit Model

Want to add genetic effects, $g_{ij}$. Assume
that the genetic effects are additive on the latent scale:

$$
\begin{align*}
Y_{ij} &= \begin{cases}
  1 & \vec x_{ij}^\top\vec\beta + \epsilon_{ij} + g_{ij} > 0 \\
  0 & \text{otherwise}
\end{cases} \\
\epsilon_{ij} &\sim N(0, 1) \\
(g_{i1}, \dots, g_{in_i})^\top & \sim N^{(n_i)}(\vec 0; \sigma_g^2 \mat C_{ig}).
\end{align*}
$$

<p class = "fragment">
Need to select the matrix $\mat C_{ig}$.</p>

## Selecting the Scale Matrix
<div class = "w-small">
We select $\mat C_{ig}$ such that $c_{igkj}$ is two times the kinship matrix 
entry for individual $k$ and $j$ in family $i$. 
<p class = "smallish">
</p>
</div>

<!--html_preserve-->
</section>
<section class="center-horiz">
<h2>Example</h2>
<!--/html_preserve-->

```{r show_ped, echo = FALSE}
plot(ex_dat)
```

Subject 7 and 8 are siblings and have $2^{-1} = 50$% while 7 and 9 are cousins 
and only have $2^{-3} = 12.5$%.

## Adding Other Effects
Add environmental effects, $e_{ij}$, to get a classical ACE model:

$$
\begin{align*}
Y_{ij} &= \begin{cases}
  1 & \vec x_{ij}^\top\vec\beta + \epsilon_{ij} + g_{ij} + e_{ij} > 0 \\
  0 & \text{otherwise}
\end{cases} \\
\epsilon_{ij} &\sim N(0, 1) \\
(g_{i1}, \dots, g_{in_i})^\top & 
  \sim N^{(n_i)}(\vec 0; \sigma_g^2 \mat C_{ig}) \\
(e_{i1}, \dots, e_{in_i})^\top & 
  \sim N^{(n_i)}(\vec 0; \sigma_e^2 \mat C_{ie}). \\
\end{align*}
$$

<!--html_preserve-->
</section>
<section class="center-horiz">
<h2>Example (continued)</h2>
<!--/html_preserve-->

```{r rep_show_ped, echo = FALSE}
plot(ex_dat)
```

(1,2,3,6) $\mat C_{ie}$ entries are equal to one and 
so are (3, 4, 7, 8) and (5, 6, 9, 10) entries.

## Inference
<div class = "w-small">
Interested in $\sigma_g^2/(1 + \sigma_g^2+ \sigma_e^2)$
<p class = "smallish">
the proportion of the variance explained by additive genetic effects,</p>
</div>

<div class = "fragment w-small">
and $\sigma_e^2/(1 + \sigma_g^2+ \sigma_e^2)$
<p class = "smallish">
the proportion of the variance explained by environmental effects.</p>
</div>

## Re-write
The model can be written as:

$$
\begin{align*}
Y_{ij} &= \begin{cases}
  1 & \vec x_{ij}^\top\vec\beta + \epsilon_{ij} > 0 \\
  0 & \text{otherwise}
\end{cases} \\
(\epsilon_{i1}, \dots, \epsilon_{in_i})^\top & 
  \sim N^{(n_i)}(\vec 0; \mat I_{n_i} + \sigma_g^2 \mat C_{ig} 
    + \sigma_e^2 \mat C_{ie}).
\end{align*}
$$

## Re-write (continued)

Or for $K$ different effects:

$$
\begin{align*}
Y_{ij} &= \begin{cases}
  1 & \vec x_{ij}^\top\vec\beta + \epsilon_{ij} > 0 \\
  0 & \text{otherwise}
\end{cases} \\
(\epsilon_{i1}, \dots, \epsilon_{in_i})^\top & 
  \sim N^{(n_i)}(\vec 0; \mat I_{n_i} + 
  \mat\Sigma(\vec\sigma)) \\
\mat\Sigma(\vec\sigma) &= \sum_{k = 1}^K\sigma_k^2 \mat C_{ik} 
\end{align*}
$$

## Estimation 
The log marginal likelihood is intractable

<div class = "fragment w-small">
... but can expressed as a multivariate normal distribution CDFs.
<p class = "smallish">
Many approximations exists! Used by @Pawitan04, @Lindstroem06, 
@Svensson09, @yip2018, and @Bai19. </p>
</div>

## Previous Work
<div class = "w-small">
Previous work only use a log marginal likelihood approximation.
<p class = "smallish">
Have to use derivative-free optimization.</p>
</div>

<div class = "fragment w-small">
To simplify the computation, they

 - discretize covariates,
 - omit observations,
 - ignore a lot of information from the pedigree, and
 - use a composite likelihood like procedure
 
<p class = "smallish">
where individuals with cousin both on their mother and father side is 
repeated twice.</p>
</div>

## New Method and Implementation
Method with gradient approximations. 

<p class = "fragment">
Our Implementation supports computation in parallel.</p>

<p class = "fragment">
A faster approximation.</p>

<p class = "fragment">
Works for an arbitrary number of effects, $K$.</p>

## MCMC
<div class = "w-small">
MCMC is also an option.
<p class = "smallish">
Seems popular with animal data. See e.g. MCMCglmm [@Hadfield10] and 
BLUPF90 [@BLUPF90].</p> 
</div>

<p class = "fragment">
Very slow!</p> 

<!--html_preserve-->
</section>
<!-- need extra end tag before next section -->
</section>



<section>
<section class="large-first center slide level2">
<h1>Marginal Likelihood</h1>
<!--/html_preserve-->

## Maximum Likelihood

Find the MLE:

$$
\argmax_{\vec\beta,\vec\sigma} \sum_{i = 1}^m\log 
  L_i(\vec\beta,\vec\sigma)
$$


where $L_i$ is the marginal likelihood of family (cluster) $i$.

## The Marginal Likelihood
<div class = "w-small">

$$
\begin{align*}
L_i(\vec\beta,\vec\sigma) &= 
  \Phi^{(n_i)}(\mat U_i\mat X_i\vec\beta;\vec 0, \mat I + 
  \mat U_i\mat\Sigma_i(\vec\sigma) \mat U_i) \\
u_{ij} &= 2y_{ij} - 1 \\
\mat U_i &= \text{diag}(\vec u_i) \\
\end{align*}
$$

<p class = "smallish">
where $\Phi^{(K)}(\vec x; \vec\omega, \mat \Omega)$ is the $K$ dimensional 
multivariate normal distribution's CDF 
with mean $\vec\omega$ and covariance matrix 
$\mat\Omega$ evaluated at $\vec x$.</p>
</div>

<p class = "fragment">
Point: need CDF approximation.</p>

## Importance Sampling
@Genz92 and @Genz02 develop an efficient importance sampling procedure
for the CDF with:

a. A heuristic variable re-ordering. 
b. Randomized Korobov rules [randomized quasi-Monte Carlo; see @Niederreiter1972; @Keast73; @Cranley76].

## New Code 
<div class = "w-small">
Rewritten most of the Fortran code by @Genz02 in C++. 
<p class ="smallish">
Used in the mvtnorm package [@Genz20].</p>
</div>

<p class = "fragment">
Added fast $\Phi$ and $\Phi^{-1}$ approximations.</p>

<div class = "w-small fragment">
Added support for computation in parallel. 
<p class = "smallish">
Seems compute-bound.</p>
</div>

<div class = "w-small fragment">
Added gradient approximations.
<p class = "smallish">
Like @Hajivassiliou96 but with all of the above, re-ordering, and
randomized Korobov rules.</p>
</div>

<!--html_preserve-->
</section>
<!-- need extra end tag before next section -->
</section>



<section>
<section class="large-first center slide level2">
<h1>Simulation and Applied Examples</h1>
<!--/html_preserve-->

## Simulation Study
<div class = "w-small">
250 families, with 10 members in each family, and the same pedigree. We add an 
additive genetic effect.
<p class = "smallish">
Same pedigree as earlier.</p>
</div>

<div class = "fragment">
$$\begin{align*}
Y_{ij} &= \begin{cases}
  1 & \beta_0 + \beta_1X_{ij} + \beta_2 B_{ij} + \epsilon_{ij} > 0 \\
  0 & \text{otherwise}
\end{cases} \\
(\epsilon_{i1}, \dots, \epsilon_{i10})^\top &\sim 
  N^{(10)}(\vec 0, \mat I_{10} + \sigma_g^2\mat C_{ig}) 
\end{align*}$$

<p>
$X_{ij}\sim N(0, 1)$, $B_{ij}\sim \text{Bin}(0.5, 1)$ are observed.</p>
</div>

<div class = "w-small fragment">
Compare with MCMCglmm.
<p class = "smallish">
Caveat: we use four threads with our method.</p>
</div>


## Simulation Study Results

```{r load_libraries, echo = FALSE, message=FALSE}
library(MCMCglmm, quietly = TRUE)
library(kinship2, quietly = TRUE)
library(igraph, quietly = TRUE)
```

```{r assign_sim_dat, echo = FALSE}
# create the family we will use
fam <- data.frame(id = 1:10, sex = rep(1:2, 5L),
                  father = c(NA, NA, 1L, NA, 1L, NA, 3L, 3L, 5L, 5L), 
                  mother = c(NA, NA, 2L, NA, 2L, NA, 4L, 4L, 6L, 6L))

# plot the pedigree
ped <- with(fam, pedigree(id = id, dadid = father, momid = mother, sex = sex))

# simulates a data set. 
# 
# Args:
#   n_fams: number of families.
#   beta: the fixed effect coefficients.
#   sig_sq: the scale parameter.
sim_dat <- function(n_fams = 250L, beta = c(-3, 1, 2), sig_sq = 3){
  # setup before the simulations
  Cmat <- 2 * kinship(ped)
  n_obs <- NROW(fam)
  Sig <- diag(n_obs) + sig_sq * Cmat
  Sig_chol <- chol(Sig)
  
  # simulate the data
  out <- replicate(
    n_fams, {
      # simulate covariates
      X <- cbind(`(Intercept)` = 1, Continuous = rnorm(n_obs), 
                 Binary = runif(n_obs) > .5)
      
      # assign the linear predictor + noise
      eta <- drop(X %*% beta) + drop(rnorm(n_obs) %*% Sig_chol)
      
      # return the list in the format needed for the package
      list(y = as.numeric(eta > 0), X = X, scale_mats = list(Cmat))
    }, simplify = FALSE)
  
  # add attributes with the true values and return 
  attributes(out) <- list(beta = beta, sig_sq = sig_sq)
  out
}

# concatenates a data set from the sim_dat function.
concatenate_data <- function(x){
  # create the concatenated data frame
  X_all <- do.call(rbind, lapply(x, `[[`, "X"))
  y_all <- do.call(c    , lapply(x, `[[`, "y"))
  y_all <- as.numeric(y_all)
  
  n_families <- length(x)
  n_members <- NROW(fam)
  inc <- rep(1:n_families - 1L, each = n_members)
  inc <- inc * n_members
  
  id <- seq_len(n_members) + inc
  father_id <- fam$father + inc
  mom_id <- fam$mother + inc
  sex <- rep(fam$sex, n_families)
  
  dat_all <- data.frame(ID = id, Father_ID = father_id, Mother_ID = mom_id,
                        Outcome = y_all, family_id = as.integer(factor(inc)), 
                        sex = sex)
  dat_all <- cbind(dat_all, X_all)  
}
```

```{r do_sims, echo = FALSE, message=FALSE}
# the seeds we will use
seeds <- c(60941821L, 30845227L, 17633234L, 79310131L, 85229418L, 21612295L, 73949913L, 57500133L, 57252222L, 63875873L, 55778211L, 21326154L, 36283814L, 7245204L, 58241367L, 38132857L, 2571948L, 35133815L, 16221382L, 24733735L, 28461866L, 92624152L, 42058797L, 93446166L, 12604787L, 32695400L, 88433623L, 30421988L, 89657111L, 74309716L, 76310780L, 87031329L, 36451989L, 18774630L, 76585289L, 31898455L, 55733878L, 99681114L, 37725150L, 99188448L, 66989159L, 20673587L, 47985954L, 42571905L, 53089211L, 18457743L, 96049437L, 70222325L, 86393368L, 45380572L, 81116968L, 48291155L, 89755299L, 69891073L, 1846862L, 15263013L, 37537710L, 24144323L, 5592547L, 21386862L, 23254835L, 53175345L, 23651644L, 82951608L, 49669937L, 53555447L, 49184987L, 8794223L, 34847097L, 26709043L, 30600548L, 8492848L, 71358252L, 23513193L, 75184122L, 72107226L, 1188113L, 27375563L, 56816302L, 26365677L, 69687049L, 53553756L, 41517720L, 76120783L, 32705509L, 49863328L, 14950823L, 64794764L, 8228366L, 17704066L, 37762560L, 96608760L, 97055078L, 51441364L, 8297093L, 69737435L, 64236096L, 8438069L, 28846936L, 74723637L)

# perform the simulation study
get_file_name <- function(s, prefix)
  file.path("cache", "direct-genetic-study", paste0(prefix, "-", s, ".RDS"))

res <- lapply(seeds, function(s){
  set.seed(s)
  f     <- get_file_name(s, "pedmod")
  f_dat <- get_file_name(s, "data")
  
  # run the study if the file does not exists
  if(!file.exists(f) || !file.exists(f_dat)){
    # simulate the data set
    dat <- sim_dat()  
    saveRDS(list(dat, dat_all = concatenate_data(dat)), 
            f_dat)
    
    # estimate the model
    library(pedmod)
    est_time <- system.time({
      ll_terms <- get_pedigree_ll_terms(dat, max_threads = 4L)
      
      start <- pedmod_start(ll_terms, dat, maxvls = 1000L, minvls = 200L, 
                            n_threads = 4L)
      
      opt_out_quick <- pedmod_opt(
        ptr = ll_terms, par = start$par, maxvls = 5000L, abs_eps = 0, 
        rel_eps = 1e-2, minvls = 500L, use_aprx = TRUE, n_threads = 4L)
      
      opt_out <- pedmod_opt(
        ptr = ll_terms, par = opt_out_quick$par, abs_eps = 0, use_aprx = TRUE, 
        n_threads = 4L, 
        maxvls = 25000L, rel_eps = 1e-3, minvls = 5000L)
    })
    
    opt_out$time <- est_time
    saveRDS(opt_out, f)
  }
  
  # load the results
  out <- readRDS(f)
  
  # report and return
  message(paste0(capture.output(out$par), collapse = "\n"))
  message(sprintf("Time: %14.2f", out$time["elapsed"]))
  
  out
})
```

```{r comp_bias, echo = FALSE}
# compute the bias and average computation times along with standard errors
est <- sapply(res, `[[`, "par")
est[4, ] <- exp(est[4, ])
rownames(est)[4] <- "Sigma"
tis <- sapply(res, `[[`, "time")["elapsed", ]

tmp <- sim_dat(1)
truth <- c(attr(tmp, "beta"), attr(tmp, "sig_sq"))

# re-scale the fixed effects and scale parameters.
# 
# Args:
#   x: estimated parameters. An p x n matrix should be used if there are n 
#      different estimates.
#   n_scales: number of scale parameters on the standard deviation scale.
standardize_estimates <- function(x, n_scales){
  if(is.matrix(x))
    return(apply(x, 2L, standardize_estimates, n_scales = n_scales))
  scales <- tail(x, n_scales)
  c(head(x, -n_scales) / sqrt(1 + sum(scales)), scales / (1 + sum(scales)))
}

err <- standardize_estimates(est, 1L) - standardize_estimates(truth, 1L)

# compute the mean and standard error
comp_bias <- function(x, MARGIN = 1L)
  list(bias = apply(x, MARGIN, mean), 
       SE   = apply(x, MARGIN, sd) / sqrt(dim(x)[if(MARGIN == 1L) 2L else 1L]))

# save it in one object
add_genetic_res <- list(bias = comp_bias(rbind(err, Time = tis)), 
                        data = list(data = est, time = tis))
```

```{r assign_fit_mcmc, echo = FALSE}
# estimates the model with MCMCglmm.
# 
# Args:
#   s: the seed to use.
fit_mcmc <- function(s, verbose = FALSE){
  # load in the data from the data folder
  f <- get_file_name(s, "MCMCglmm")
  
  if(!file.exists(f)){
    library(MCMCglmm)
    # set the prior. Note that R's variance is fixed . We need to account for 
    # this later. I fixed it to a small value as multiple sources state that the 
    # MCMCglmm package is sensitive to over-/underflow issues the probit link 
    # function (some state linear predictors should be in +/-7), E.g. see
    #   https://stat.ethz.ch/pipermail/r-sig-mixed-models/2010q3/004006.html
    # prior <- list(R = list(V = 0.1, fix = 1), G = list(G1 = list(
    #   # inverse gamma (shape: nu/2, scale: nu * V / 2) 
    #   # see https://stat.ethz.ch/pipermail/r-sig-mixed-models/2011q2/016198.html
    #   #V = 1, nu = 2)),
    #   # F-distribution with nu and alpha.V degrees of freedom
    #   V = 1, nu = 1, alpha.mu=0,  alpha.V = 10)), 
    #   B = list(mu = rep(0, 3), V = diag(5^2, 3)))
    
    # we use family = "threshold" such that we get the standard 
    # parameterisation. See 
    #   https://stat.ethz.ch/pipermail/r-sig-mixed-models/2017q4/026115.html
    #   https://stat.ethz.ch/pipermail/r-sig-mixed-models/2015q1/023176.html
    prior <- list(R = list(V = 1, fix = 1), G = list(G1 = list(
      # F-distribution with 1 and nu degrees of freedom
      V = 1, nu = 10, alpha.mu = 0,  alpha.V = 1)), 
      B = list(mu = rep(0, 3), V = diag(10^2, 3)))
    
    # prepare the data
    dat <- readRDS(get_file_name(s, "data"))
    dat_all <- dat$dat_all
    mcmc_dat <- dat_all[, setdiff(colnames(dat_all), 
                                  c("Father_ID", "Mother_ID", "family_id"))]
    colnames(mcmc_dat)[colnames(mcmc_dat) == "ID"] <- "animal"
    pedigree_data <- setNames(dat_all[, c("ID", "Father_ID", "Mother_ID")], 
                              c("animal", "sire", "dam"))
    
    # run MCMC
    set.seed(s)
    mcmc_time <- system.time(
      mcmc_fit <- MCMCglmm(
        Outcome ~ Continuous + Binary, random = ~ animal, family = "threshold",
        prior = prior, pedigree = pedigree_data, data = mcmc_dat, nitt = 1e6, 
        burnin = 1e5, thin = 100, slice = TRUE, verbose = verbose))
    
    mcmc_fit$time <- mcmc_time
    saveRDS(mcmc_fit, f)
  }
  
  # load, report, and return 
  out <- readRDS(f)
  message(paste0(capture.output(summary(out)), collapse = "\n"))
  out
}
```

```{r get_mcmc_res, message=FALSE, echo = FALSE}
# get the results
MCMCglmm_res <- lapply(head(seeds, 10L), fit_mcmc)
pedmod_res <- lapply(head(seeds, 10L), function(s)
  readRDS(get_file_name(s, "pedmod")))

est_mcmc <- sapply(MCMCglmm_res, function(x){
  beta <- x$Sol
  scale <- x$VCV[, "animal"]
  samps <- standardize_estimates(rbind(t(beta), scale), n_scales = 1L)
  
  mode <- c(posterior.mode(x$Sol), Scale = posterior.mode(x$VCV[, "animal"]))
  
  cbind(
    mean = rowMeans(samps),
    median = apply(samps, 1L, median), 
    mode = standardize_estimates(mode, 1L))
}, simplify = "array")
est_pedmod <- sapply(pedmod_res, function(x)
  c(x$par[1:3], Scale = exp(x$par[4])))

# get the computation time
tis_mcmc   <- sapply(MCMCglmm_res, `[[`, "time")["elapsed", ]
tis_pedmod <- sapply(pedmod_res  , `[[`, "time")["elapsed", ]

# compute the means and compare with pedmod
tmp <- sim_dat(1)
truth <- c(attr(tmp, "beta"), attr(tmp, "sig_sq"))
truth <- standardize_estimates(truth, 1L)
bias_MCMCglmm_mean <- comp_bias(
  rbind(est_mcmc[, "mean", ] - truth, 
        Time = tis_mcmc))
bias_MCMCglmm_mode <- comp_bias(
  rbind(est_mcmc[, "mode", ] - truth, 
        Time = NA_real_))
bias_pedmod        <- comp_bias(
  rbind(standardize_estimates(est_pedmod, 1L) - truth, 
        Time = tis_pedmod))

# compute stats for effective sample size
MCMCglmm_ess <- sapply(MCMCglmm_res, function(x)
  c(effectiveSize(x$Sol), Var = unname(effectiveSize(x$VCV[, "animal"]))), 
  simplify = "array")
MCMCglmm_ess_mean <- apply(MCMCglmm_ess, 1L, mean)
MCMCglmm_ess_sd <- apply(MCMCglmm_ess, 1L, sd)

# create the list with the results to save
mcmc_comp_res <- list(
  bias = list(MCMCglmm_mean = bias_MCMCglmm_mean, 
              bias_MCMCglmm_mode = bias_MCMCglmm_mode, 
              pedmod = bias_pedmod), 
  data = list(MCMCglmm_res = est_mcmc, 
              pedmod_res = est_pedmod,
              MCMCglmm_ess_mean = MCMCglmm_ess_mean, 
              MCMCglmm_ess_sd = MCMCglmm_ess_sd))

# show the table with many samples
tab <- do.call(rbind, add_genetic_res$bias)
rownames(tab) <- c("Bias", "SE")
colnames(tab)[colnames(tab) == "(Intercept)"] <- "Intercept"
mult <- 100
tab[, colnames(tab) != "Time"] <- tab[, colnames(tab) != "Time"] * 100
colnames(tab) <- sapply(colnames(tab), function(x)
  switch (
    x,
    Intercept = "$\\beta_0$", 
    Continuous = "$\\beta_1$", 
    Binary = "$\\beta_2$", 
    Sigma = "$\\sigma_g$", 
    Time = "Time", 
    stop()))

knitr::kable(tab)
```

<p class = "smallish">
Bias estimates using `r length(seeds)` data sets. The bias estimates are 
multiplied by `r mult`. The last column shows the 
average estimation time in seconds. The SE row shows the standard errors.</p>

## Results: Comparison with MCMC

```{r show_mcmc_res, echo = FALSE}
tab <- lapply(mcmc_comp_res$bias, function(x) do.call(rbind, x))
nams <- names(tab)
nams <- sapply(nams, function(x) 
  switch (x, 
          MCMCglmm_mean = "MCMC (mean)", 
          bias_MCMCglmm_mode = "MCMC (mode)", 
          pedmod = "New method", 
          stop()))
tab <- do.call(rbind, tab)
rownames(tab) <- unlist(lapply(nams, function(x) c(x, "SE")))
colnames(tab)[colnames(tab) == "(Intercept)"] <- "Intercept"

tab[, colnames(tab) != "Time"] <- tab[, colnames(tab) != "Time"] * 100
colnames(tab) <- sapply(colnames(tab), function(x)
  switch (
    x,
    Intercept = "$\\beta_0$", 
    Continuous = "$\\beta_1$", 
    Binary = "$\\beta_2$", 
    scale =,
    Sigma = "$\\sigma_g$", 
    Time = "Time", 
    stop()))
knitr::kable(tab)
```

<p class = "smallish">
Bias estimates using `r dim(mcmc_comp_res$data$MCMCglmm_res)[3]` data sets. 
The bias estimates are multiplied by `r mult`.
The last column shows average the estimation time in seconds. 
The SE rows show the standard errors. Both posterior means and modes are shown.</p>

## Simplifying the Real Application

Want to avoid the composite likelihood like procedure and use as much 
information from the pedigree as possible. 

<p class = "fragment">
Use the Swedish Multi-Generation Registry [@Ekbom10] 
like @Mahjani20 to study OCD.</p>

<div class = "fragment w-small">
Most families are small but one is very large with 618.162 members with 
167.279 members with observed outcomes.
<p class = "smallish">
Want to avoid 167.279 dimensional integration!</p>
</div>

<!--html_preserve-->
</section>
<section class="center-horiz">
<h2>Dependencies as a Graph</h2>
<!--/html_preserve-->

```{r plot_as_graph, echo = FALSE}
par(mar = c(1, 1, 1, 1), mfcol = c(1, 2))
plot(ped)
graph_dat <- create_igraph_input(ped)
plot(graph.data.frame(graph_dat, directed = FALSE), 
      vertex.size = 10, vertex.color = "gray")
```

<!--html_preserve-->
</section>
<section class="center-horiz">
<h2>Dependencies as a Graph (continued)</h2>
<!--/html_preserve-->

```{r larger_ped, echo = FALSE}
dat <- data.frame(
  id = 1:52,
  mom = c(NA, NA, 2L, 2L, 2L, NA, NA, 7L, 7L, 7L, 3L, 3L, 3L, 3L, NA, NA, 16L, 17L, 17L, NA, NA, 21L, 22L, NA, NA, 25L, 25L, 9L, 9L, 9L, 9L, NA, NA, 33L, 33L, 33L, 34L, 34L, NA, NA, 40L, 40L, 40L, 42L, 42L, NA, NA, 47L, 47L, 47L, 36L, 36L),
  dad = c(NA, NA, 1L, 1L, 1L, NA, NA, 6L, 6L, 6L, 8L, 8L, 8L, 8L, NA, NA, 15L, 4L, 4L, NA, NA, 20L, 5L, NA, NA, 24L, 24L, 26L, 26L, 26L, 26L, NA, NA, 32L, 32L, 32L, 27L, 27L, NA, NA, 39L, 39L, 39L, 35L, 35L, NA, NA, 46L, 46L, 46L, 49L, 49L),
  sex = c(1L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, 1L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 1L, 1L, 2L, 2L))
dat[22, "mom"] <- 47L
dat[22, "dad"] <- 46L
dat[17, "mom"] <- dat[17, "dad"] <- NA_integer_
dat <- dat[-c(15:16, 20:21), ]
ped_larger <- with(dat, pedigree(id = id, dadid = dad, momid = mom, sex = sex))
par(mar = c(1, 1, 1, 1))
plot(ped_larger)
```

<!--html_preserve-->
</section>
<section class="center-horiz">
<h2>Dependencies as a Graph (continued)</h2>
<!--/html_preserve-->

```{r graph_larger_ped, echo = FALSE}
graph_dat <- create_igraph_input(ped_larger)
par(mar = c(1, 1, 1, 1))
plot(graph.data.frame(graph_dat, directed = FALSE), 
      vertex.size = 10, vertex.color = "gray")
```

## Reducing the Computational Cost
Removing an edge introduces independence between two individuals. 

<div class = "fragment w-small">
Removing edges can creates two non-connected families (subgraphs) with 
$\tilde n_k$ and $\tilde n_l$ individuals (vertices) 
<p class = "smallish">
with $\tilde n_k + \tilde n_l = n_i$.</p>
</p>
</div>

<div class = "fragment w-small">
Yields two integrals to be approximated of size $\tilde n_k$ and $\tilde n_l$
<p class = "smallish">
rather than one of size $n$.</p>
</div>  

## Reducing the Computational Cost (continued)
Idea: create $p$ families each of roughly size $n_i / p$ such the number of 
removed dependencies is minimized.

<div class = "fragment w-small">
Essentially the $p$-way partition problem.
<p class = "smallish">
Many approximations exists if the partitions need not to be connected 
[@Simon97; @Karypis98].</p>
</div>

<div class = "fragment w-small">
Currently use a very crude heuristic. 
<p class = "smallish">
Reduces the number of cousin pairs by 7.1 pct.</p>
</div>

<!--html_preserve-->
</section>
<section class="center-horiz">
<h2>Profile Likelihood</h2>
<!--/html_preserve-->

<img src="fig/Profile likelihood curve-page-001.jpg"/>

<!-- 12111.568 / 60 / 60  -->

<div class = "w-small">
The above profile likelihood was computed in 3 hours and 22 minutes on 
Vector.
<p class = "smallish">
The MCMC analysis in @Mahjani20 took weeks!</p>
</div>

<!--html_preserve-->
</section>
<!-- need extra end tag before next section -->
</section>



<section>
<section class="large-first center slide level2">
<h1>Extensions</h1>
<!--/html_preserve-->

## Improvements
<div class = "w-small">
Get better starting values.
<p class = "smallish">
Make an efficient Gaussian variational approximation or Laplace 
approximation to get starting values.</p>
</div>

<p class = "fragment">
Use stochastic gradient techniques.</p>

## Survival Data
$Y_{ij}^* \in (0,\infty)$ with independent right censoring time 
$S_{ij} \in (0,\infty)$. 

<p class = "fragment">
Only observe $Y_{ij} = \min (Y_{ij}^*, S_{ij})$.
</p>

<div class = "fragment">
Use a mixed generalized survival model:

$$
\begin{align*}
P(Y_{ij}^* > y \mid \epsilon_{ij} = e) &= \Phi(
  -\vec g(y)^\top\vec\omega - \vec\beta^\top\vec x_{ij} - e) \\
(\epsilon_{i1}, \dots, \epsilon_{in_i}) &\sim
  N^{(n_i)}(\vec 0, \mat\Sigma_i(\vec\theta))
\end{align*}
$$
</div>

## Survival Data
A simple case is $\vec g(y) = \log y$ in which case

$$
\begin{align*}
\omega\log Y_{ij}^* &= -\vec\beta^\top\vec x_{ij} + \epsilon_{ij} \\
\vec\epsilon_i &\sim
  N^{(n_i)}(\vec 0, \mat I_{n_i} + \mat\Sigma_i(\vec\sigma))
\end{align*}
$$

<div class = "fragment w-small">

Or equivalently

$$
\begin{align*}
\log Y_{ij}^* &= -\omega^{-1}\vec\beta^\top\vec x_{ij} + \epsilon_{ij} \\
\vec\epsilon_i &\sim
  N^{(n_i)}(\vec 0, \omega^{-2}(\mat I_{n_i} + \mat\Sigma_i(\vec\sigma)))
\end{align*}
$$

<p class = "smallish">
Used in the bivariate case by @Szulkin17.</p>
</div>

## Survival Data (continued)
Generally, 

$$
\begin{align*}
\vec g(Y_{ij}^*)^\top\vec\omega &= -\vec\beta^\top\vec x_{ij} + \epsilon_{ij} \\
\vec\epsilon_i &\sim
  N^{(n_i)}(\vec 0, \mat I_{n_i} + \mat\Sigma_i(\vec\sigma))
\end{align*}
$$

<p class = "fragment">
The CDF dimension is equal to the number of censored individuals.</p>

## Generic GLMMs
Similar log marginal likelihoods appears for some mixed models with 
multinomial and ordinal data.

<p class = "fragment">
The CDF is attractive when the number of random effects per cluster 
is not much smaller than the number of observations in a cluster.</p>

## Imputation Method
Use Gaussian copula for the joint distribution of mixed data types. 

<div class = "w-small fragment">
Improve upon the approximate EM-algorithm by @zhao19. 
<p class = "smallish">
See [github.com/boennecd/mdgc](https://github.com/boennecd/mdgc).</p>
</div>

<div class = "fragment w-small">
One can extend the approach to multinomial data.
<p class="smallish">
See @Christoffersen21.</p>
</div>


<!--html_preserve-->
</section>
<!-- need extra end tag before next section -->
</section>


<section>
<section class="large-first center slide level2">
<h1>Conclusions</h1>
<!--/html_preserve-->

## Conclusions
Extended the estimation method suggested by @Pawitan04. 

<div class = "fragment w-small">
Showed that the new C++ implementation is fast 
<p class = "smallish">
for large data sets with moderate to high dimensional integrals per 
cluster.</p>
</div>

<p class = "fragment">
Discussed extensions to other types of outcomes.</p>

<!--html_preserve-->
</section>
<!-- need extra end tag before next section -->
</section>



<section>
<section class="center final">
<h1>Thank You!</h1>

<div class="w-small">
<p class="smallish">The presentation is at  
<a href="https://rpubs.com/boennecd/KTH-Pre-Meb-Meeting">rpubs.com/boennecd/KTH-Pre-Meb-Meeting</a>.</p>
<p class="smallish">The markdown is at  
<a href="https://github.com/boennecd/Talks">github.com/boennecd/Talks</a>.</p>
<p class="smallish">The R package is at
<a href="https://github.com/boennecd/pedmod">github.com/boennecd/pedmod</a>.</p>
<p class="smallish">References are on the next slide.</p>
</div>

</section>
<!-- need extra end tag before next section -->
</section>


<section>
<h1>References</h1>

<!--/html_preserve-->