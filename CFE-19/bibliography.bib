@article{Dempster77,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2984875},
 abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
 author = {A. P. Dempster and N. M. Laird and D. B. Rubin},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {1--38},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Maximum Likelihood from Incomplete Data via the EM Algorithm},
 volume = {39},
 year = {1977}
}

@Article{Vaupel1979,
author="Vaupel, James W.
and Manton, Kenneth G.
and Stallard, Eric",
title="The impact of heterogeneity in individual frailty on the dynamics of mortality",
journal="Demography",
year="1979",
month="Aug",
day="01",
volume="16",
number="3",
pages="439--454",
abstract="Life table methods are developed for populations whose members differ in their endowment for longevity. Unlike standard methods, which ignore such heterogeneity, these methods use different calculations to construct cohort, period, and individual life tables. The results imply that standard methods overestimate current life expectancy and potential gains in life expectancy from health and safety interventions, while underestimating rates of individual aging, past progress in reducing mortality, and mortality differentials between pairs of populations. Calculations based on Swedish mortality data suggest that these errors may be important, especially in old age.",
issn="1533-7790",
doi="10.2307/2061224",
url="https://doi.org/10.2307/2061224"
}


@article{Meng93,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2337198},
 abstract = {Two major reasons for the popularity of the EM algorithm are that its maximum step involves only complete-data maximum likelihood estimation, which is often computationally simple, and that its convergence is stable, with each iteration increasing the likelihood. When the associated complete-data maximum likelihood estimation itself is complicated, EM is less attractive because the M-step is computationally unattractive. In many cases, however, complete-data maximum likelihood estimation is relatively simple when conditional on some function of the parameters being estimated. We introduce a class of generalized EM algorithms, which we call the ECM algorithm, for Expectation/Conditional Maximization (CM), that takes advantage of the simplicity of complete-data conditional maximum likelihood estimation by replacing a complicated M-step of EM with several computationally simpler CM-steps. We show that the ECM algorithm shares all the appealing convergence properties of EM, such as always increasing the likelihood, and present several illustrative examples.},
 author = {Xiao-Li Meng and Donald B. Rubin},
 journal = {Biometrika},
 number = {2},
 pages = {267--278},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Maximum Likelihood Estimation via the ECM Algorithm: A General Framework},
 volume = {80},
 year = {1993}
}



@ARTICLE{Gordon93, 
author={N. J. Gordon and D. J. Salmond and A. F. M. Smith}, 
journal={IEE Proceedings F - Radar and Signal Processing}, 
title={Novel approach to nonlinear/non-Gaussian Bayesian state estimation}, 
year={1993}, 
volume={140}, 
number={2}, 
pages={107-113}, 
keywords={Bayes methods;filtering and prediction theory;Kalman filters;state estimation;tracking;State estimation;state transition model;state vector density;nonlinear Bayesian state estimation;nonGaussian Bayesian state estimation;algorithm;bootstrap filter;recursive Bayesian filters;random samples;Gaussian noise;measurement model;simulation;bearings only tracking problem;extended Kalman filter;Bayes procedures;Filtering;Kalman filtering;Tracking;Prediction methods}, 
doi={10.1049/ip-f-2.1993.0015}, 
ISSN={0956-375X}, 
month={April}}

@article{Pitt99,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2670179},
 abstract = {This article analyses the recently suggested particle approach to filtering time series. We suggest that the algorithm is not robust to outliers for two reasons: the design of the simulators and the use of the discrete support to represent the sequentially updating prior distribution. Here we tackle the first of these problems.},
 author = {Michael K. Pitt and Neil Shephard},
 journal = {Journal of the American Statistical Association},
 number = {446},
 pages = {590--599},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Filtering via Simulation: Auxiliary Particle Filters},
 volume = {94},
 year = {1999}
}

@article{Shumway01,
 ISSN = {00219398, 15375374},
 URL = {http://www.jstor.org/stable/10.1086/209665},
 abstract = {I argue that hazard models are more appropriate than single‐period models for forecasting bankruptcy. Single‐period models are inconsistent, while hazard models produce consistent estimates. I describe a simple technique for estimating a discrete‐time hazard model. I find that about half of the accounting ratios that have been used in previous models are not statistically significant. Moreover, market size, past stock returns, and idiosyncratic returns variability are all strongly related to bankruptcy. I propose a model that uses both accounting ratios and market‐driven variables to produce out‐of‐sample forecasts that are more accurate than those of alternative models.},
 author = {Tyler Shumway},
 journal = {The Journal of Business},
 number = {1},
 pages = {101-124},
 publisher = {The University of Chicago Press},
 title = {Forecasting Bankruptcy More Accurately: A Simple Hazard Model},
 volume = {74},
 year = {2001}
}


@article{Friedman01,
 ISSN = {00905364},
 URL = {http://www.jstor.org/stable/2699986},
 abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
 author = {Jerome H. Friedman},
 journal = {The Annals of Statistics},
 number = {5},
 pages = {1189-1232},
 publisher = {Institute of Mathematical Statistics},
 title = {Greedy Function Approximation: A Gradient Boosting Machine},
 volume = {29},
 year = {2001}
}


@article{Friedman02,
title = "Stochastic gradient boosting",
journal = "Computational Statistics & Data Analysis",
volume = "38",
number = "4",
pages = "367 - 378",
year = "2002",
note = "Nonlinear Methods and Data Mining",
issn = "0167-9473",
doi = "https://doi.org/10.1016/S0167-9473(01)00065-2",
url = "http://www.sciencedirect.com/science/article/pii/S0167947301000652",
author = "Jerome H. Friedman"
}

@INPROCEEDINGS{Gray03,
author = {Gray, A., and Moore, A.}, 
year = {2003}, 
title = {Rapid evaluation of multiple density models},
booktitle = {Artificial Intelligence and Statistics}
}


@article{Chava04,
author = {Chava, Sudheer and Jarrow, Robert A.},
title = { Bankruptcy Prediction with Industry Effects *},
journal = {Review of Finance},
volume = {8},
number = {4},
pages = {537-569},
year = {2004},
doi = {10.1093/rof/8.4.537},
URL = { + http://dx.doi.org/10.1093/rof/8.4.537},
eprint = {/oup/backfile/content_public/journal/rof/8/4/10.1093_rof_8.4.537/3/8-4-537.pdf}
}

@article{Bates04,
title = "Linear mixed models and penalized least squares",
journal = "Journal of Multivariate Analysis",
volume = "91",
number = "1",
pages = "1 - 17",
year = "2004",
note = "Special Issue on Semiparametric and Nonparametric Mixed Models",
issn = "0047-259X",
doi = "https://doi.org/10.1016/j.jmva.2004.04.013",
url = "http://www.sciencedirect.com/science/article/pii/S0047259X04000867",
author = "Douglas M Bates and Saikat DebRoy",
keywords = "REML, Gradient, Hessian, EM algorithm, ECME algorithm, Maximum likelihood, Profile likelihood, Multilevel models"
}

@article{Vassalou04,
 ISSN = {00221082, 15406261},
 URL = {http://www.jstor.org/stable/3694915},
 abstract = {This is the first study that uses Merton's (1974) option pricing model to compute default measures for individual firms and assess the effect of default risk on equity returns. The size effect is a default effect, and this is also largely true for the book-to-market (BM) effect. Both exist only in segments of the market with high default risk. Default risk is systematic risk. The Fama-French (FF) factors SMB and HML contain some default-related information, but this is not the main reason that the FF model can explain the cross section of equity returns.},
 author = {Maria Vassalou and Yuhang Xing},
 journal = {The Journal of Finance},
 number = {2},
 pages = {831--868},
 publisher = {[American Finance Association, Wiley]},
 title = {Default Risk in Equity Returns},
 volume = {59},
 year = {2004}
}

@article{Lin05,
author = {Ming T Lin and Junni L Zhang and Qiansheng Cheng and Rong Chen},
title = {Independent Particle Filters},
journal = {Journal of the American Statistical Association},
volume = {100},
number = {472},
pages = {1412-1421},
year  = {2005},
publisher = {Taylor & Francis},
doi = {10.1198/016214505000000349},

URL = { 
        https://doi.org/10.1198/016214505000000349
    
},
eprint = { 
        https://doi.org/10.1198/016214505000000349
    
}

}

@book{cappe05,
 author = {Capp{\'e}, Olivier and Moulines, Eric and Ryden, Tobias},
 title = {Inference in Hidden Markov Models},
 year = {2005},
 isbn = {978-0-387-40264-2, 978-1-4419-2319-6},
 publisher = {Springer-Verlag New York},
} 

@Article{Beaver05,
author="Beaver, William H.
and McNichols, Maureen F.
and Rhie, Jung-Wu",
title="Have Financial Statements Become Less Informative? Evidence from the Ability of Financial Ratios to Predict Bankruptcy",
journal="Review of Accounting Studies",
year="2005",
month="Mar",
day="01",
volume="10",
number="1",
pages="93--122",
abstract="Using a hazard model, we examine secular changes in the ability of financial statement data to predict bankruptcy from 1962 to 2002. We identify three trends in financial reporting that could influence predictive ability with respect to bankruptcy: FASB standards, the perceived increase in discretionary financial reporting behavior, and the increase in unrecognized assets and obligations. A parsimonious three-variable model provides significant explanatory power throughout the time period, with only a slight deterioration in predictive power from the first to the second time period. The striking feature of the results is the robustness of the predictive models over a forty-year period.",
issn="1573-7136",
doi="10.1007/s11142-004-6341-9",
url="https://doi.org/10.1007/s11142-004-6341-9"
}

@INPROCEEDINGS{Cappe05, 
author={O. {Cappe} and E. {Moulines}}, 
booktitle={IEEE/SP 13th Workshop on Statistical Signal Processing, 2005}, 
title={Recursive computation of the score and observed information matrix in hidden markov models}, 
year={2005}, 
volume={}, 
number={}, 
pages={703-708}, 
keywords={approximation theory;hidden Markov models;matrix algebra;Monte Carlo methods;recursive functions;signal processing;smoothing methods;hidden Markov model;HMM;statistical model;Fisher identity;Louis identity;recursive equation;information matrix;sensitivity equation;recursive smoother;approximation;sequential Monte Carlo method;Hidden Markov models;Equations;Signal processing algorithms;State-space methods;Bioinformatics;Econometrics;Solid modeling;Smoothing methods;Digital communication;Speech recognition}, 
doi={10.1109/SSP.2005.1628685}, 
ISSN={2373-0803}, 
month={July},}

@TECHREPORT{Basel06,
  title={International Convergence of Capital Measurement and Capital Standards: a revised framework, (comprehensive version)},
  author={{Basel Committee on Banking Supervision}}, 
  year={2006},
  month={June}, 
  INSTITUTION={Bank for International Settlements}
}

@inproceedings{Klaas06,
 author = {Klaas, Mike and Briers, Mark and de Freitas, Nando and Doucet, Arnaud and Maskell, Simon and Lang, Dustin},
 title = {Fast Particle Smoothing: If I Had a Million Particles},
 booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
 series = {ICML '06},
 year = {2006},
 isbn = {1-59593-383-2},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {481--488},
 numpages = {8},
 doi = {10.1145/1143844.1143905},
 acmid = {1143905},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@article{Duffie07,
title = "Multi-period corporate default prediction with stochastic covariates",
journal = "Journal of Financial Economics",
volume = "83",
number = "3",
pages = "635 - 665",
year = "2007",
issn = "0304-405X",
doi = "https://doi.org/10.1016/j.jfineco.2005.10.011",
url = "http://www.sciencedirect.com/science/article/pii/S0304405X06002029",
author = "Darrell Duffie and Leandro Saita and Ke Wang",
keywords = "Default, Bankruptcy, Duration analysis, Doubly stochastic, Distance to default"
}




@article{Buhlmann07,
 ISSN = {08834237},
 URL = {http://www.jstor.org/stable/27645854},
 abstract = {We present a statistical perspective on boosting. Special emphasis is given to estimating potentially complex parametric or nonparametric models, including generalized linear and additive models as well as regression models for survival analysis. Concepts of degrees of freedom and corresponding Akaike or Bayesian information criteria, particularly useful for regularization and variable selection in high-dimensional covariate spaces, are discussed as well. The practical aspects of boosting procedures for fitting statistical models are illustrated by means of the dedicated open-source software package mboost. This package implements functions which can be used for model fitting, prediction and variable selection. It is flexible, allowing for the implementation of new boosting algorithms optimizing user-specified loss functions.},
 author = {Peter Bühlmann and Torsten Hothorn},
 journal = {Statistical Science},
 number = {4},
 pages = {477-505},
 publisher = {Institute of Mathematical Statistics},
 title = {Boosting Algorithms: Regularization, Prediction and Model Fitting},
 volume = {22},
 year = {2007}
}


@article {Daniel07,
author = {Berg, Daniel},
title = {Bankruptcy prediction by generalized additive models},
journal = {Applied Stochastic Models in Business and Industry},
volume = {23},
number = {2},
publisher = {John Wiley & Sons, Ltd.},
issn = {1526-4025},
url = {http://dx.doi.org/10.1002/asmb.658},
doi = {10.1002/asmb.658},
pages = {129--143},
keywords = {bankruptcy prediction, generalized additive models, default horizon, performance depreciation, multi-year model},
year = {2007},
}


@article {Campbell08,
author = {Campbell, John Y. And Hilscher, Jens and Szilagyi, Jan},
title = {In Search of Distress Risk},
journal = {The Journal of Finance},
volume = {63},
number = {6},
publisher = {Blackwell Publishing Inc},
issn = {1540-6261},
url = {http://dx.doi.org/10.1111/j.1540-6261.2008.01416.x},
doi = {10.1111/j.1540-6261.2008.01416.x},
pages = {2899--2939},
year = {2008},
}


@article{Alfaro08,
title = "Bankruptcy forecasting: An empirical comparison of AdaBoost and neural networks",
journal = "Decision Support Systems",
volume = "45",
number = "1",
pages = "110 - 122",
year = "2008",
note = "Data Warehousing and OLAP",
issn = "0167-9236",
doi = "https://doi.org/10.1016/j.dss.2007.12.002",
url = "http://www.sciencedirect.com/science/article/pii/S016792360700214X",
author = "Esteban Alfaro and Noelia García and Matías Gámez and David Elizondo",
keywords = "Corporate Failure Prediction, Neural Network, AdaBoost"
}


@article{Shumway08,
author = {Bharath, Sreedhar T. and Shumway, Tyler},
title = {Forecasting Default with the Merton Distance to Default Model},
journal = {The Review of Financial Studies},
volume = {21},
number = {3},
pages = {1339-1369},
year = {2008},
doi = {10.1093/rfs/hhn044},
URL = {http://dx.doi.org/10.1093/rfs/hhn044},
eprint = {/oup/backfile/content_public/journal/rfs/21/3/10.1093_rfs_hhn044/1/hhn044.pdf}
}

@article{Bharath08,
    author = {Bharath, Sreedhar T. and Shumway, Tyler},
    title = "{Forecasting Default with the Merton Distance to Default Model}",
    journal = {The Review of Financial Studies},
    volume = {21},
    number = {3},
    pages = {1339-1369},
    year = {2008},
    month = {05},
    abstract = "{We examine the accuracy and contribution of the Merton distance to default (DD) model, which is based on Merton's (1974) bond pricing model. We compare the model to a “naïve” alternative, which uses the functional form suggested by the Merton model but does not solve the model for an implied probability of default. We find that the naïve predictor performs slightly better in hazard models and in out-of-sample forecasts than both the Merton DD model and a reduced-form model that uses the same inputs. Several other forecasting variables are also important predictors, and fitted values from an expanded hazard model outperform Merton DD default probabilities out of sample. Implied default probabilities from credit default swaps and corporate bond yield spreads are only weakly correlated with Merton DD probabilities after adjusting for agency ratings and bond characteristics. We conclude that while the Merton DD model does not produce a sufficient statistic for the probability of default, its functional form is useful for forecasting defaults.}",
    issn = {0893-9454},
    doi = {10.1093/rfs/hhn044},
    url = {https://dx.doi.org/10.1093/rfs/hhn044},
    eprint = {http://oup.prod.sis.lan/rfs/article-pdf/21/3/1339/24429422/hhn044.pdf},
}

@article {Duffie09,
author = {Duffie, Darrell and Eckner, Andreas and Horel, Guillaume and Saita, Leandro},
title = {Frailty Correlated Default},
journal = {The Journal of Finance},
volume = {64},
number = {5},
publisher = {Blackwell Publishing Inc},
issn = {1540-6261},
url = {http://dx.doi.org/10.1111/j.1540-6261.2009.01495.x},
doi = {10.1111/j.1540-6261.2009.01495.x},
pages = {2089--2123},
year = {2009},
}

@Article{Briers09,
author="Briers, Mark
and Doucet, Arnaud
and Maskell, Simon",
title="Smoothing algorithms for state--space models",
journal="Annals of the Institute of Statistical Mathematics",
year="2009",
month="Jun",
day="09",
volume="62",
number="1",
pages="61",
abstract="Two-filter smoothing is a principled approach for performing optimal smoothing in non-linear non-Gaussian state--space models where the smoothing distributions are computed through the combination of `forward' and `backward' time filters. The `forward' filter is the standard Bayesian filter but the `backward' filter, generally referred to as the backward information filter, is not a probability measure on the space of the hidden Markov process. In cases where the backward information filter can be computed in closed form, this technical point is not important. However, for general state--space models where there is no closed form expression, this prohibits the use of flexible numerical techniques such as Sequential Monte Carlo (SMC) to approximate the two-filter smoothing formula. We propose here a generalised two-filter smoothing formula which only requires approximating probability distributions and applies to any state--space model, removing the need to make restrictive assumptions used in previous approaches to this problem. SMC algorithms are developed to implement this generalised recursion and we illustrate their performance on various problems.",
issn="1572-9052",
doi="10.1007/s10463-009-0236-2",
url="https://doi.org/10.1007/s10463-009-0236-2"
}

@article{Fearnhead10,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/25734097},
 abstract = {In this paper we propose a new particle smoother that has a computational complexity of O(N), where N is the number of particles. This compares favourably with the O(N²) computational cost of most smoothers. The new method also overcomes some degeneracy problems in existing algorithms. Through simulation studies we show that substantial gains in efficiency are obtained for practical amounts of computational cost. It is shown both through these simulation studies, and by the analysis of an athletics dataset, that our new method also substantially outperforms the simple filter-smoother, the only other smoother with computational cost that is O(N).},
 author = {Paul Fearnhead and David Wyncoll and Jonathan Tawn},
 journal = {Biometrika},
 number = {2},
 pages = {447--464},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {A sequential smoothing algorithm with linear computational cost},
 volume = {97},
 year = {2010}
}


@article{Dakovic10,
author = { Rada   Dakovic  and  Claudia   Czado  and  Daniel   Berg },
title = {Bankruptcy prediction in Norway: a comparison study},
journal = {Applied Economics Letters},
volume = {17},
number = {17},
pages = {1739-1746},
year  = {2010},
publisher = {Routledge},
doi = {10.1080/13504850903299594},

URL = { 
        https://doi.org/10.1080/13504850903299594
    
},
eprint = { 
        https://doi.org/10.1080/13504850903299594
    
}
}

@article{Lando10,
title = "Correlation in corporate defaults: Contagion or conditional independence?",
journal = "Journal of Financial Intermediation",
volume = "19",
number = "3",
pages = "355 - 372",
year = "2010",
note = "Risk Transfer Mechanisms and Financial Stability",
issn = "1042-9573",
doi = "https://doi.org/10.1016/j.jfi.2010.03.002",
url = "http://www.sciencedirect.com/science/article/pii/S1042957310000070",
author = "David Lando and Mads Stenbo Nielsen",
keywords = "Default correlation, Intensity estimation, Hawkes process",
abstract = "We revisit a method used by Das et al. (2007) (DDKS) who jointly test and reject a specification of firm default intensities and the doubly stochastic assumption in intensity models of default. The method relies on a time change result for counting processes. With an almost identical set of default histories recorded by Moody’s in the period from 1982 to 2006, but using a different specification of the default intensity, we cannot reject the tests based on time change used in DDKS. We then note that the method proposed by DDKS is mainly a misspecification test in that it has very limited power in detecting violations of the doubly stochastic assumption. For example, it will not detect contagion which spreads through the explanatory variables “covariates” that determine the default intensities of individual firms. Therefore, we perform a different test using a Hawkes process alternative to see if firm-specific variables are affected by occurrences of defaults, but find no evidence of default contagion."
}



@ARTICLE{Zhou10,
   author = {{Zhou}, S.},
    title = "{Thresholded Lasso for high dimensional variable selection and statistical estimation}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1002.1583},
 keywords = {Mathematics - Statistics},
     year = 2010,
    month = feb,
   adsurl = {http://adsabs.harvard.edu/abs/2010arXiv1002.1583Z},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@book{buhlmann11,
  title={Statistics for high-dimensional data: methods, theory and applications},
  author={B{\"u}hlmann, Peter and Van De Geer, Sara},
  year={2011},
  publisher={Springer Science \& Business Media}
}

@article{Poyiadjis11,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/29777165},
 author = {George Poyiadjis and Arnaud Doucet and Sumeetpal S. Singh},
 journal = {Biometrika},
 number = {1},
 pages = {65--80},
 publisher = {Biometrika Trust},
 title = {Particle approximations of the score and observed information matrix in state space models with application to parameter estimation},
 volume = {98},
 year = {2011}
}


@article{Koopman11,
title = "Modeling frailty-correlated defaults using many macroeconomic covariates",
journal = "Journal of Econometrics",
volume = "162",
number = "2",
pages = "312 - 325",
year = "2011",
issn = "0304-4076",
doi = "https://doi.org/10.1016/j.jeconom.2011.02.003",
url = "http://www.sciencedirect.com/science/article/pii/S0304407611000303",
author = "Siem Jan Koopman and André Lucas and Bernd Schwaab",
keywords = "Systematic default risk, Frailty-correlated defaults, State space methods, Credit risk management"
}

@article{Chava11,
author = {Chava, Sudheer and Stefanescu, Catalina and Turnbull, Stuart},
title = {Modeling the Loss Distribution},
journal = {Management Science},
volume = {57},
number = {7},
pages = {1267-1287},
year = {2011},
doi = {10.1287/mnsc.1110.1345},

URL = { 
        https://doi.org/10.1287/mnsc.1110.1345
    
},
eprint = { 
        https://doi.org/10.1287/mnsc.1110.1345
    
}
,
    abstract = { In this paper, we focus on modeling and predicting the loss distribution for credit risky assets such as bonds and loans. We model the probability of default and the recovery rate given default based on shared covariates. We develop a new class of default models that explicitly accounts for sector specific and regime dependent unobservable heterogeneity in firm characteristics. Based on the analysis of a large default and recovery data set over the horizon 1980–2008, we document that the specification of the default model has a major impact on the predicted loss distribution, whereas the specification of the recovery model is less important. In particular, we find evidence that industry factors and regime dynamics affect the performance of default models, implying that the appropriate choice of default models for loss prediction will depend on the credit cycle and on portfolio characteristics. Finally, we show that default probabilities and recovery rates predicted out of sample are negatively correlated and that the magnitude of the correlation varies with seniority class, industry, and credit cycle. This paper was accepted by Wei Xiong, finance. }
}

@article{Poyiadjis11,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/29777165},
 abstract = {Particle methods are popular computational tools for Bayesian inference in nonlinear non-Gaussian state space models. For this class of models, we present two particle algorithms to compute the score vector and observed information matrix recursively. The first algorithm is implemented with computational complexity ð’ª(N) and the second with complexity ð’ª(NÂ²), where N is the number of particles. Although cheaper, the performance of the ð’ª(N) method degrades quickly, as it relies on the approximation of a sequence of probability distributions whose dimension increases linearly with time. In particular, even under strong mixing assumptions, the variance of the estimates computed with the ð’ª(N) method increases at least quadratically in time. The more expensive ð’ª(NÂ²) method relies on a nonstandard particle implementation and does not suffer from this rapid degradation. It is shown how both methods can be used to perform batch and recursive parameter estimation.},
 author = {George Poyiadjis and Arnaud Doucet and Sumeetpal S. Singh},
 journal = {Biometrika},
 number = {1},
 pages = {65--80},
 publisher = {Biometrika Trust},
 title = {Particle approximations of the score and observed information matrix in state space models with application to parameter estimation},
 volume = {98},
 year = {2011}
}

@article{Duan12,
title = "Multiperiod corporate default prediction—A forward intensity approach",
journal = "Journal of Econometrics",
volume = "170",
number = "1",
pages = "191 - 209",
year = "2012",
issn = "0304-4076",
doi = "https://doi.org/10.1016/j.jeconom.2012.05.002",
url = "http://www.sciencedirect.com/science/article/pii/S0304407612001145",
author = "Jin-Chuan Duan and Jie Sun and Tao Wang",
keywords = "Default, Bankruptcy, Forward intensity, Maximum pseudo-likelihood, Forward default probability, Cumulative default probability, Accuracy ratio"
}

@article{Hwang12,
title = "A varying-coefficient default model",
journal = "International Journal of Forecasting",
volume = "28",
number = "3",
pages = "675 - 688",
year = "2012",
issn = "0169-2070",
doi = "https://doi.org/10.1016/j.ijforecast.2011.11.006",
url = "http://www.sciencedirect.com/science/article/pii/S0169207012000052",
author = "Ruey-Ching Hwang",
keywords = "Discrete-time hazard model, Local likelihood, Expanding rolling window approach, Predicted number of defaults, Predictive interval, Varying-coefficient model"
}

@article{David13,
author = {Lando, David and Medhat, Mamdouh and Nielsen, Mads Stenbo and Nielsen, Søren Feodor},
title = {Additive Intensity Regression Models in Corporate Default Analysis},
journal = {Journal of Financial Econometrics},
volume = {11},
number = {3},
pages = {443-485},
year = {2013},
doi = {10.1093/jjfinec/nbs018},
URL = {http://dx.doi.org/10.1093/jjfinec/nbs018},
eprint = {/oup/backfile/content_public/journal/jfec/11/3/10.1093/jjfinec/nbs018/2/nbs018.pdf}
}

@article{Chen14,
title = "Default prediction with dynamic sectoral and macroeconomic frailties",
journal = "Journal of Banking & Finance",
volume = "40",
pages = "211 - 226",
year = "2014",
issn = "0378-4266",
doi = "https://doi.org/10.1016/j.jbankfin.2013.11.036",
url = "http://www.sciencedirect.com/science/article/pii/S0378426613004573",
author = "Peimin Chen and Chunchi Wu",
keywords = "Default risk, Hazard rate function, Frailty, Distance to default, Tail loss, Monte Carlo expectations maximization (EM), Gibbs sampler"
}

@article{Qi14,
title = "Unobserved systematic risk factor and default prediction",
journal = "Journal of Banking & Finance",
volume = "49",
pages = "216 - 227",
year = "2014",
issn = "0378-4266",
doi = "https://doi.org/10.1016/j.jbankfin.2014.09.009",
url = "http://www.sciencedirect.com/science/article/pii/S0378426614003094",
author = "Min Qi and Xiaofei Zhang and Xinlei Zhao",
keywords = "Observed systematic risk factors, Unobserved systematic risk factor, Corporate default prediction, Rank order, Predictive accuracy"
}


@article{Bates15,
   author = {Douglas Bates and Martin Mächler and Ben Bolker and Steve Walker},
   title = {Fitting Linear Mixed-Effects Models Using lme4},
   journal = {Journal of Statistical Software, Articles},
   volume = {67},
   number = {1},
   year = {2015},
   keywords = {sparse matrix methods; linear mixed models; penalized least squares; Cholesky decomposition},
   abstract = {Maximum likelihood or restricted maximum likelihood (REML) estimates of the parameters in linear mixed-effects models can be determined using the lmer function in the lme4 package for R. As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed- and random-effects terms. The formula and data together determine a numerical representation of the model from which the profiled deviance or the profiled REML criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the profiled deviance or REML criterion, and the structure of classes or types that represents such a model. Sufficient detail is included to allow specialization of these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.},
   issn = {1548-7660},
   pages = {1--48},
   doi = {10.18637/jss.v067.i01},
   url = {https://www.jstatsoft.org/v067/i01}
}


@article{Zieba16,
title = "Ensemble boosted trees with synthetic features generation in application to bankruptcy prediction",
journal = "Expert Systems with Applications",
volume = "58",
pages = "93 - 101",
year = "2016",
issn = "0957-4174",
doi = "https://doi.org/10.1016/j.eswa.2016.04.001",
url = "http://www.sciencedirect.com/science/article/pii/S0957417416301592",
author = "Maciej Zięba and Sebastian K. Tomczak and Jakub M. Tomczak",
keywords = "Bankruptcy prediction, Extreme gradient boosting, Synthetic features generation, Imbalanced data"
}

@article{Filipe16,
title = "Forecasting distress in European SME portfolios",
journal = "Journal of Banking & Finance",
volume = "64",
pages = "112 - 135",
year = "2016",
issn = "0378-4266",
doi = "https://doi.org/10.1016/j.jbankfin.2015.12.007",
url = "http://www.sciencedirect.com/science/article/pii/S0378426615003611",
author = "Sara Ferreira Filipe and Theoharry Grammatikos and Dimitra Michala",
keywords = "Credit risk, Distress, Forecasting, SMEs, Logit",
abstract = "In this paper, we examine idiosyncratic and systematic distress predictors for small and medium sized enterprises (SMEs) in Europe over the period 2000–2009. We find that SMEs across European regions are vulnerable to common idiosyncratic factors but systematic factors vary. Moreover, systematic factors move average distress rates and small SMEs are more vulnerable to these factors compared to large SMEs. By including many very small companies in the sample, our models offer unique insights into the European small business sector. By exploring distress in a multi-country setting, the models uncover regional vulnerabilities. Finally, by incorporating systematic dependencies, the models capture distress co-movements."
}


@article {Jones17,
author = {Jones, Stewart and Johnstone, David and Wilson, Roy},
title = {Predicting Corporate Bankruptcy: An Evaluation of Alternative Statistical Frameworks},
journal = {Journal of Business Finance & Accounting},
volume = {44},
number = {1-2},
issn = {1468-5957},
url = {http://dx.doi.org/10.1111/jbfa.12218},
doi = {10.1111/jbfa.12218},
pages = {3--34},
keywords = {corporate bankruptcy prediction, binary classifiers, statistical learning},
year = {2017},
}


@article{Nickerson17,
title = "Debt correlations in the wake of the financial crisis: What are appropriate default correlations for structured products?",
journal = "Journal of Financial Economics",
volume = "125",
number = "3",
pages = "454 - 474",
year = "2017",
issn = "0304-405X",
doi = "https://doi.org/10.1016/j.jfineco.2017.06.011",
url = "http://www.sciencedirect.com/science/article/pii/S0304405X17301289",
author = "Jordan Nickerson and John M. Griffin",
keywords = "Credit ratings, Financial crises, Structured finance, Default correlations"
}


@Book{Wood17,
    title = {Generalized Additive Models: An Introduction with R},
    year = {2017},
    author = {S.N Wood},
    edition = {2},
    publisher = {Chapman and Hall/CRC},
}

@article{Jensen17,
  title={Cyclicality and Firm-size in Private Firm Defaults},
  author={Jensen, Thais and Lando, David and Medhat, Mamdouh},
  year={2017},
  Journal={International Journal of Central Banking},
  volume={13}, 
  number={4}, 
  pages={97--145}
}

@Book{Wood17,
    title = {Generalized Additive Models: An Introduction with R},
    year = {2017},
    author = {S.N Wood},
    edition = {2},
    publisher = {Chapman and Hall/CRC},
  }
  
@article{Jensen17,
title = "Cyclicality and Firm Size in Private Firm Defaults",
abstract = "The Basel II/III and CRD IV Accords reduce capital charges on bank loans to smaller firms by assuming that the default probabilities of smaller firms are less sensitive to macroeconomic cycles. We test this assumption in a default intensity framework using a large sample of bank loans to private Danish firms. We find that controlling only for size, the default probabilities of small firms are, in fact, less cyclical than the default probabilities of large firms. However, accounting for firm characteristics other than size, we find that the default probabilities of small firms are equally cyclical or even more cyclical than the default probabilities of large firms. These results hold using a multiplicative Cox model as well as an additive Aalen model with time-varying coefficients.",
author = "Jensen, {Thais L{\ae}rkholm} and David Lando and Mamdouh Medhat",
year = "2017",
language = "English",
volume = "13",
pages = "97--145",
journal = "International Journal of Central Banking",
issn = "1815-4654",
publisher = "Association of the International Journal of Central Banking",
number = "4",
}


@article{kwon18,
title = "Industry specific defaults",
journal = "Journal of Empirical Finance",
volume = "45",
pages = "45 - 58",
year = "2018",
issn = "0927-5398",
doi = "https://doi.org/10.1016/j.jempfin.2017.10.002",
url = "http://www.sciencedirect.com/science/article/pii/S0927539817300920",
author = "Tae Yeon Kwon and Yoonjung Lee",
keywords = "Intensity credit risk model, Within industry default correlation, Between industries default correlation, Frailty, MCEM"
}

@article{Azizpour18,
title = "Exploring the sources of default clustering",
journal = "Journal of Financial Economics",
volume = "129",
number = "1",
pages = "154 - 183",
year = "2018",
issn = "0304-405X",
doi = "https://doi.org/10.1016/j.jfineco.2018.04.008",
url = "http://www.sciencedirect.com/science/article/pii/S0304405X1830103X",
author = "S Azizpour and K. Giesecke and G. Schwenkler",
keywords = "Default clustering, Contagion, Frailty, Correlated default risk"
}

@unpublished{Christoffersen18,
title = {Can Machine Learning Models Capture Correlations in Corporate Distresses?},
author = {Christoffersen, Benjamin and Matin, Rastin and M{\o}lgaard, Pia},
year = {2018},
note = {Available at SSRN: \url{https://ssrn.com/abstract=3273985}}
}

@Manual{R2018,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2018},
    url = {https://www.R-project.org/},
 }

@Manual{Christoffersen18B,
    title = {dynamichazard: Dynamic Hazard Models using State Space Models},
    author = {Benjamin Christoffersen},
    note = {R package version 0.6.3},
    year = {2018},
    url = {https://github.com/boennecd/dynamichazard},
 }

@unpublished{Christoffersen19,
title = {Modeling Frailty Correlated Defaults with Multivariate Latent Factors},
author = {Christoffersen, Benjamin and Matin, Rastin},
year = {2019},
note = {Available at SSRN: \url{https://ssrn.com/abstract=3339981}}
}